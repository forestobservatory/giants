{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"giants \u00b6 Basic machine learning optimization support, developed to identify big trees. Documentation : the.forestobservatory.com/giants Source code : forestobservatory/giants Introduction \u00b6 giants is a simple package that provides python support for tuning sklearn models via hyperparameter searches. There are a series of pre-defined configurations and hyperparameter grids defined for a series of models, which should be fairly easy to extend as needed. It was originally developed for the Big Trees project but we found it useful enough to clean it up and publish it as a standalone package for easy re-use. Install \u00b6 pip install giants Developed by \u00b6","title":"Home"},{"location":"#giants","text":"Basic machine learning optimization support, developed to identify big trees. Documentation : the.forestobservatory.com/giants Source code : forestobservatory/giants","title":"giants"},{"location":"#introduction","text":"giants is a simple package that provides python support for tuning sklearn models via hyperparameter searches. There are a series of pre-defined configurations and hyperparameter grids defined for a series of models, which should be fairly easy to extend as needed. It was originally developed for the Big Trees project but we found it useful enough to clean it up and publish it as a standalone package for easy re-use.","title":"Introduction"},{"location":"#install","text":"pip install giants","title":"Install"},{"location":"#developed-by","text":"","title":"Developed by"},{"location":"user-guide/","text":"Model tuning \u00b6 The giants package is a set of routines to make model tuning with sklearn easy. The primary pattern is to load your dataset, create a model tuner, run the hyperparameter search, apply the predictions to test data, and evaluate performance. Classification example \u00b6 import giants from sklearn import datasets , metrics , model_selection # load test classification data and create train/test splits x , y = datasets . load_iris ( return_X_y = True ) xtrain , xtest , ytrain , ytest = model_selection . train_test_split ( x , y , test_size = 0.25 ) # create the model tuner object tuner = giants . model . Tuner ( xtrain , ytrain ) # run a random forest hyperparameter search tuner . RandomForestClassifier () # evaluate model performance ypred = tuner . best_estimator . predict ( xtest ) print ( metrics . classification_report ( ytest , ypred )) The tuner.best_estimator object stores the sklearn model that minimized the scoring function provided. Each model has a default scoring function it uses, but the user can define this: tuner . RandomForestClassifier ( scorer = 'balanced_accuracy' ) You can list the available scorers: giants . model . list_scorers () Pass your own hyperparameter grid to train models using pair-wise combinations of each hyperparameter. param_grid : { \"n_estimators\" : ( 10 , 50 ), \"max_depth\" : ( 2 , 10 , None )} tuner . RandomForestClassifier ( param_grid = param_grid ) Configuration \u00b6 The hyperparameter grids, scoring functions, and other default configuration values are stored in these objects: giants . model . ModelDefaults giants . model . ModelEstimators giants . model . ParamGrids The values are stored as attributes of these classes: print ( giants . model . ModelDefaults . NumberOfSplits ) # 4 The code that constructs these configuration classes is in giants.config . Supported models \u00b6 The following models are currently supported. This list could be easily extended to cover a broader range of existing sklearn models. AdaBoostClassifier AdaBoostRegressor DecisionTreeClassifier GradientBoostingClassifier GradientBoostingRegressor LinearRegression LinearSVC LinearSVR LogisticRegression RandomForestClassifier RandomForestRegressor SVC SVR","title":"User Guide"},{"location":"user-guide/#model-tuning","text":"The giants package is a set of routines to make model tuning with sklearn easy. The primary pattern is to load your dataset, create a model tuner, run the hyperparameter search, apply the predictions to test data, and evaluate performance.","title":"Model tuning"},{"location":"user-guide/#classification-example","text":"import giants from sklearn import datasets , metrics , model_selection # load test classification data and create train/test splits x , y = datasets . load_iris ( return_X_y = True ) xtrain , xtest , ytrain , ytest = model_selection . train_test_split ( x , y , test_size = 0.25 ) # create the model tuner object tuner = giants . model . Tuner ( xtrain , ytrain ) # run a random forest hyperparameter search tuner . RandomForestClassifier () # evaluate model performance ypred = tuner . best_estimator . predict ( xtest ) print ( metrics . classification_report ( ytest , ypred )) The tuner.best_estimator object stores the sklearn model that minimized the scoring function provided. Each model has a default scoring function it uses, but the user can define this: tuner . RandomForestClassifier ( scorer = 'balanced_accuracy' ) You can list the available scorers: giants . model . list_scorers () Pass your own hyperparameter grid to train models using pair-wise combinations of each hyperparameter. param_grid : { \"n_estimators\" : ( 10 , 50 ), \"max_depth\" : ( 2 , 10 , None )} tuner . RandomForestClassifier ( param_grid = param_grid )","title":"Classification example"},{"location":"user-guide/#configuration","text":"The hyperparameter grids, scoring functions, and other default configuration values are stored in these objects: giants . model . ModelDefaults giants . model . ModelEstimators giants . model . ParamGrids The values are stored as attributes of these classes: print ( giants . model . ModelDefaults . NumberOfSplits ) # 4 The code that constructs these configuration classes is in giants.config .","title":"Configuration"},{"location":"user-guide/#supported-models","text":"The following models are currently supported. This list could be easily extended to cover a broader range of existing sklearn models. AdaBoostClassifier AdaBoostRegressor DecisionTreeClassifier GradientBoostingClassifier GradientBoostingRegressor LinearRegression LinearSVC LinearSVR LogisticRegression RandomForestClassifier RandomForestRegressor SVC SVR","title":"Supported models"},{"location":"module/config/","text":"Default configuration options for model hyperparameter searches. ModelConfig \u00b6 Stores default model tuning parameters CVClassification ( _BaseKFold ) \u00b6 Stratified K-Folds cross-validator. Provides train/test indices to split data in train/test sets. This cross-validation object is a variation of KFold that returns stratified folds. The folds are made by preserving the percentage of samples for each class. Read more in the :ref: User Guide <stratified_k_fold> . Parameters \u00b6 n_splits : int, default=5 Number of folds. Must be at least 2. .. versionchanged:: 0.22 ``n_splits`` default value changed from 3 to 5. shuffle : bool, default=False Whether to shuffle each class's samples before splitting into batches. Note that the samples within each split will not be shuffled. random_state : int, RandomState instance or None, default=None When shuffle is True, random_state affects the ordering of the indices, which controls the randomness of each fold for each class. Otherwise, leave random_state as None . Pass an int for reproducible output across multiple function calls. See :term: Glossary <random_state> . Examples \u00b6 import numpy as np from sklearn.model_selection import StratifiedKFold X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]]) y = np.array([0, 0, 1, 1]) skf = StratifiedKFold(n_splits=2) skf.get_n_splits(X, y) 2 print(skf) StratifiedKFold(n_splits=2, random_state=None, shuffle=False) for train_index, test_index in skf.split(X, y): ... print(\"TRAIN:\", train_index, \"TEST:\", test_index) ... X_train, X_test = X[train_index], X[test_index] ... y_train, y_test = y[train_index], y[test_index] TRAIN: [1 3] TEST: [0 2] TRAIN: [0 2] TEST: [1 3] Notes \u00b6 The implementation is designed to: Generate test sets such that all contain the same distribution of classes, or as close as possible. Be invariant to class label: relabelling y = [\"Happy\", \"Sad\"] to y = [1, 0] should not change the indices generated. Preserve order dependencies in the dataset ordering, when shuffle=False : all samples from class k in some test set were contiguous in y, or separated in y by samples from classes other than k. Generate test sets where the smallest and largest differ by at most one sample. .. versionchanged:: 0.22 The previous implementation did not follow the last constraint. See Also \u00b6 RepeatedStratifiedKFold : Repeats Stratified K-Fold n times. split ( self , X , y , groups = None ) \u00b6 Generate indices to split data into training and test set. Parameters \u00b6 X : array-like of shape (n_samples, n_features) Training data, where n_samples is the number of samples and n_features is the number of features. Note that providing ``y`` is sufficient to generate the splits and hence ``np.zeros(n_samples)`` may be used as a placeholder for ``X`` instead of actual training data. y : array-like of shape (n_samples,) The target variable for supervised learning problems. Stratification is done based on the y labels. groups : object Always ignored, exists for compatibility. Yields \u00b6 train : ndarray The training set indices for that split. test : ndarray The testing set indices for that split. Notes \u00b6 Randomized CV splitters may return different results for each call of split. You can make the results identical by setting random_state to an integer. Source code in giants/config.py def split ( self , X , y , groups = None ): \"\"\"Generate indices to split data into training and test set. Parameters ---------- X : array-like of shape (n_samples, n_features) Training data, where `n_samples` is the number of samples and `n_features` is the number of features. Note that providing ``y`` is sufficient to generate the splits and hence ``np.zeros(n_samples)`` may be used as a placeholder for ``X`` instead of actual training data. y : array-like of shape (n_samples,) The target variable for supervised learning problems. Stratification is done based on the y labels. groups : object Always ignored, exists for compatibility. Yields ------ train : ndarray The training set indices for that split. test : ndarray The testing set indices for that split. Notes ----- Randomized CV splitters may return different results for each call of split. You can make the results identical by setting `random_state` to an integer. \"\"\" y = check_array ( y , ensure_2d = False , dtype = None ) return super () . split ( X , y , groups ) CVRegression ( _BaseKFold ) \u00b6 K-Folds cross-validator Provides train/test indices to split data in train/test sets. Split dataset into k consecutive folds (without shuffling by default). Each fold is then used once as a validation while the k - 1 remaining folds form the training set. Read more in the :ref: User Guide <k_fold> . Parameters \u00b6 n_splits : int, default=5 Number of folds. Must be at least 2. .. versionchanged:: 0.22 ``n_splits`` default value changed from 3 to 5. shuffle : bool, default=False Whether to shuffle the data before splitting into batches. Note that the samples within each split will not be shuffled. random_state : int, RandomState instance or None, default=None When shuffle is True, random_state affects the ordering of the indices, which controls the randomness of each fold. Otherwise, this parameter has no effect. Pass an int for reproducible output across multiple function calls. See :term: Glossary <random_state> . Examples \u00b6 import numpy as np from sklearn.model_selection import KFold X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]]) y = np.array([1, 2, 3, 4]) kf = KFold(n_splits=2) kf.get_n_splits(X) 2 print(kf) KFold(n_splits=2, random_state=None, shuffle=False) for train_index, test_index in kf.split(X): ... print(\"TRAIN:\", train_index, \"TEST:\", test_index) ... X_train, X_test = X[train_index], X[test_index] ... y_train, y_test = y[train_index], y[test_index] TRAIN: [2 3] TEST: [0 1] TRAIN: [0 1] TEST: [2 3] Notes \u00b6 The first n_samples % n_splits folds have size n_samples // n_splits + 1 , other folds have size n_samples // n_splits , where n_samples is the number of samples. Randomized CV splitters may return different results for each call of split. You can make the results identical by setting random_state to an integer. See Also \u00b6 StratifiedKFold : Takes group information into account to avoid building folds with imbalanced class distributions (for binary or multiclass classification tasks). GroupKFold : K-fold iterator variant with non-overlapping groups. RepeatedKFold : Repeats K-Fold n times. Optimizer ( BaseSearchCV ) \u00b6 Exhaustive search over specified parameter values for an estimator. Important members are fit, predict. GridSearchCV implements a \"fit\" and a \"score\" method. It also implements \"score_samples\", \"predict\", \"predict_proba\", \"decision_function\", \"transform\" and \"inverse_transform\" if they are implemented in the estimator used. The parameters of the estimator used to apply these methods are optimized by cross-validated grid-search over a parameter grid. Read more in the :ref: User Guide <grid_search> . Parameters \u00b6 estimator : estimator object This is assumed to implement the scikit-learn estimator interface. Either estimator needs to provide a score function, or scoring must be passed. param_grid : dict or list of dictionaries Dictionary with parameters names ( str ) as keys and lists of parameter settings to try as values, or a list of such dictionaries, in which case the grids spanned by each dictionary in the list are explored. This enables searching over any sequence of parameter settings. scoring : str, callable, list, tuple or dict, default=None Strategy to evaluate the performance of the cross-validated model on the test set. If `scoring` represents a single score, one can use: - a single string (see :ref:`scoring_parameter`); - a callable (see :ref:`scoring`) that returns a single value. If `scoring` represents multiple scores, one can use: - a list or tuple of unique strings; - a callable returning a dictionary where the keys are the metric names and the values are the metric scores; - a dictionary with metric names as keys and callables a values. See :ref:`multimetric_grid_search` for an example. n_jobs : int, default=None Number of jobs to run in parallel. None means 1 unless in a :obj: joblib.parallel_backend context. -1 means using all processors. See :term: Glossary <n_jobs> for more details. .. versionchanged:: v0.20 `n_jobs` default changed from 1 to None refit : bool, str, or callable, default=True Refit an estimator using the best found parameters on the whole dataset. For multiple metric evaluation, this needs to be a `str` denoting the scorer that would be used to find the best parameters for refitting the estimator at the end. Where there are considerations other than maximum score in choosing a best estimator, ``refit`` can be set to a function which returns the selected ``best_index_`` given ``cv_results_``. In that case, the ``best_estimator_`` and ``best_params_`` will be set according to the returned ``best_index_`` while the ``best_score_`` attribute will not be available. The refitted estimator is made available at the ``best_estimator_`` attribute and permits using ``predict`` directly on this ``GridSearchCV`` instance. Also for multiple metric evaluation, the attributes ``best_index_``, ``best_score_`` and ``best_params_`` will only be available if ``refit`` is set and all of them will be determined w.r.t this specific scorer. See ``scoring`` parameter to know more about multiple metric evaluation. .. versionchanged:: 0.20 Support for callable added. cv : int, cross-validation generator or an iterable, default=None Determines the cross-validation splitting strategy. Possible inputs for cv are: - None, to use the default 5-fold cross validation, - integer, to specify the number of folds in a `(Stratified)KFold`, - :term:`CV splitter`, - An iterable yielding (train, test) splits as arrays of indices. For integer/None inputs, if the estimator is a classifier and ``y`` is either binary or multiclass, :class:`StratifiedKFold` is used. In all other cases, :class:`KFold` is used. These splitters are instantiated with `shuffle=False` so the splits will be the same across calls. Refer :ref:`User Guide <cross_validation>` for the various cross-validation strategies that can be used here. .. versionchanged:: 0.22 ``cv`` default value if None changed from 3-fold to 5-fold. verbose : int Controls the verbosity: the higher, the more messages. - >1 : the computation time for each fold and parameter candidate is displayed; - >2 : the score is also displayed; - >3 : the fold and candidate parameter indexes are also displayed together with the starting time of the computation. pre_dispatch : int, or str, default='2*n_jobs' Controls the number of jobs that get dispatched during parallel execution. Reducing this number can be useful to avoid an explosion of memory consumption when more jobs get dispatched than CPUs can process. This parameter can be: - None, in which case all the jobs are immediately created and spawned. Use this for lightweight and fast-running jobs, to avoid delays due to on-demand spawning of the jobs - An int, giving the exact number of total jobs that are spawned - A str, giving an expression as a function of n_jobs, as in '2*n_jobs' error_score : 'raise' or numeric, default=np.nan Value to assign to the score if an error occurs in estimator fitting. If set to 'raise', the error is raised. If a numeric value is given, FitFailedWarning is raised. This parameter does not affect the refit step, which will always raise the error. return_train_score : bool, default=False If False , the cv_results_ attribute will not include training scores. Computing training scores is used to get insights on how different parameter settings impact the overfitting/underfitting trade-off. However computing the scores on the training set can be computationally expensive and is not strictly required to select the parameters that yield the best generalization performance. .. versionadded:: 0.19 .. versionchanged:: 0.21 Default value was changed from ``True`` to ``False`` Attributes \u00b6 cv_results_ : dict of numpy (masked) ndarrays A dict with keys as column headers and values as columns, that can be imported into a pandas DataFrame . For instance the below given table +------------+-----------+------------+-----------------+---+---------+ |param_kernel|param_gamma|param_degree|split0_test_score|...|rank_t...| +============+===========+============+=================+===+=========+ | 'poly' | -- | 2 | 0.80 |...| 2 | +------------+-----------+------------+-----------------+---+---------+ | 'poly' | -- | 3 | 0.70 |...| 4 | +------------+-----------+------------+-----------------+---+---------+ | 'rbf' | 0.1 | -- | 0.80 |...| 3 | +------------+-----------+------------+-----------------+---+---------+ | 'rbf' | 0.2 | -- | 0.93 |...| 1 | +------------+-----------+------------+-----------------+---+---------+ will be represented by a ``cv_results_`` dict of:: { 'param_kernel': masked_array(data = ['poly', 'poly', 'rbf', 'rbf'], mask = [False False False False]...) 'param_gamma': masked_array(data = [-- -- 0.1 0.2], mask = [ True True False False]...), 'param_degree': masked_array(data = [2.0 3.0 -- --], mask = [False False True True]...), 'split0_test_score' : [0.80, 0.70, 0.80, 0.93], 'split1_test_score' : [0.82, 0.50, 0.70, 0.78], 'mean_test_score' : [0.81, 0.60, 0.75, 0.85], 'std_test_score' : [0.01, 0.10, 0.05, 0.08], 'rank_test_score' : [2, 4, 3, 1], 'split0_train_score' : [0.80, 0.92, 0.70, 0.93], 'split1_train_score' : [0.82, 0.55, 0.70, 0.87], 'mean_train_score' : [0.81, 0.74, 0.70, 0.90], 'std_train_score' : [0.01, 0.19, 0.00, 0.03], 'mean_fit_time' : [0.73, 0.63, 0.43, 0.49], 'std_fit_time' : [0.01, 0.02, 0.01, 0.01], 'mean_score_time' : [0.01, 0.06, 0.04, 0.04], 'std_score_time' : [0.00, 0.00, 0.00, 0.01], 'params' : [{'kernel': 'poly', 'degree': 2}, ...], } NOTE The key ``'params'`` is used to store a list of parameter settings dicts for all the parameter candidates. The ``mean_fit_time``, ``std_fit_time``, ``mean_score_time`` and ``std_score_time`` are all in seconds. For multi-metric evaluation, the scores for all the scorers are available in the ``cv_results_`` dict at the keys ending with that scorer's name (``'_<scorer_name>'``) instead of ``'_score'`` shown above. ('split0_test_precision', 'mean_train_precision' etc.) best_estimator_ : estimator Estimator that was chosen by the search, i.e. estimator which gave highest score (or smallest loss if specified) on the left out data. Not available if refit=False . See ``refit`` parameter for more information on allowed values. best_score_ : float Mean cross-validated score of the best_estimator For multi-metric evaluation, this is present only if ``refit`` is specified. This attribute is not available if ``refit`` is a function. best_params_ : dict Parameter setting that gave the best results on the hold out data. For multi-metric evaluation, this is present only if ``refit`` is specified. best_index_ : int The index (of the cv_results_ arrays) which corresponds to the best candidate parameter setting. The dict at ``search.cv_results_['params'][search.best_index_]`` gives the parameter setting for the best model, that gives the highest mean score (``search.best_score_``). For multi-metric evaluation, this is present only if ``refit`` is specified. scorer_ : function or a dict Scorer function used on the held out data to choose the best parameters for the model. For multi-metric evaluation, this attribute holds the validated ``scoring`` dict which maps the scorer key to the scorer callable. n_splits_ : int The number of cross-validation splits (folds/iterations). refit_time_ : float Seconds used for refitting the best model on the whole dataset. This is present only if ``refit`` is not False. .. versionadded:: 0.20 multimetric_ : bool Whether or not the scorers compute several metrics. classes_ : ndarray of shape (n_classes,) The classes labels. This is present only if refit is specified and the underlying estimator is a classifier. n_features_in_ : int Number of features seen during :term: fit . Only defined if best_estimator_ is defined (see the documentation for the refit parameter for more details) and that best_estimator_ exposes n_features_in_ when fit. .. versionadded:: 0.24 feature_names_in_ : ndarray of shape ( n_features_in_ ,) Names of features seen during :term: fit . Only defined if best_estimator_ is defined (see the documentation for the refit parameter for more details) and that best_estimator_ exposes feature_names_in_ when fit. .. versionadded:: 1.0 Notes \u00b6 The parameters selected are those that maximize the score of the left out data, unless an explicit score is passed in which case it is used instead. If n_jobs was set to a value higher than one, the data is copied for each point in the grid (and not n_jobs times). This is done for efficiency reasons if individual jobs take very little time, but may raise errors if the dataset is large and not enough memory is available. A workaround in this case is to set pre_dispatch . Then, the memory is copied only pre_dispatch many times. A reasonable value for pre_dispatch is 2 * n_jobs . See Also \u00b6 ParameterGrid : Generates all the combinations of a hyperparameter grid. train_test_split : Utility function to split the data into a development set usable for fitting a GridSearchCV instance and an evaluation set for its final evaluation. sklearn.metrics.make_scorer : Make a scorer from a performance metric or loss function. Examples \u00b6 from sklearn import svm, datasets from sklearn.model_selection import GridSearchCV iris = datasets.load_iris() parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]} svc = svm.SVC() clf = GridSearchCV(svc, parameters) clf.fit(iris.data, iris.target) GridSearchCV(estimator=SVC(), param_grid={'C': [1, 10], 'kernel': ('linear', 'rbf')}) sorted(clf.cv_results_.keys()) ['mean_fit_time', 'mean_score_time', 'mean_test_score',... 'param_C', 'param_kernel', 'params',... 'rank_test_score', 'split0_test_score',... 'split2_test_score', ... 'std_fit_time', 'std_score_time', 'std_test_score'] ModelEstimatorConfig \u00b6 Stores sklearn model estimators AdaBoostClassifier ( ClassifierMixin , BaseWeightBoosting ) \u00b6 An AdaBoost classifier. An AdaBoost [1] classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases. This class implements the algorithm known as AdaBoost-SAMME [2]. Read more in the :ref: User Guide <adaboost> . .. versionadded:: 0.14 Parameters \u00b6 base_estimator : object, default=None The base estimator from which the boosted ensemble is built. Support for sample weighting is required, as well as proper classes_ and n_classes_ attributes. If None , then the base estimator is :class: ~sklearn.tree.DecisionTreeClassifier initialized with max_depth=1 . n_estimators : int, default=50 The maximum number of estimators at which boosting is terminated. In case of perfect fit, the learning procedure is stopped early. learning_rate : float, default=1.0 Weight applied to each classifier at each boosting iteration. A higher learning rate increases the contribution of each classifier. There is a trade-off between the learning_rate and n_estimators parameters. algorithm : {'SAMME', 'SAMME.R'}, default='SAMME.R' If 'SAMME.R' then use the SAMME.R real boosting algorithm. base_estimator must support calculation of class probabilities. If 'SAMME' then use the SAMME discrete boosting algorithm. The SAMME.R algorithm typically converges faster than SAMME, achieving a lower test error with fewer boosting iterations. random_state : int, RandomState instance or None, default=None Controls the random seed given at each base_estimator at each boosting iteration. Thus, it is only used when base_estimator exposes a random_state . Pass an int for reproducible output across multiple function calls. See :term: Glossary <random_state> . Attributes \u00b6 base_estimator_ : estimator The base estimator from which the ensemble is grown. estimators_ : list of classifiers The collection of fitted sub-estimators. classes_ : ndarray of shape (n_classes,) The classes labels. n_classes_ : int The number of classes. estimator_weights_ : ndarray of floats Weights for each estimator in the boosted ensemble. estimator_errors_ : ndarray of floats Classification error for each estimator in the boosted ensemble. feature_importances_ : ndarray of shape (n_features,) The impurity-based feature importances if supported by the base_estimator (when based on decision trees). Warning: impurity-based feature importances can be misleading for high cardinality features (many unique values). See :func:`sklearn.inspection.permutation_importance` as an alternative. n_features_in_ : int Number of features seen during :term: fit . .. versionadded:: 0.24 feature_names_in_ : ndarray of shape ( n_features_in_ ,) Names of features seen during :term: fit . Defined only when X has feature names that are all strings. .. versionadded:: 1.0 See Also \u00b6 AdaBoostRegressor : An AdaBoost regressor that begins by fitting a regressor on the original dataset and then fits additional copies of the regressor on the same dataset but where the weights of instances are adjusted according to the error of the current prediction. GradientBoostingClassifier : GB builds an additive model in a forward stage-wise fashion. Regression trees are fit on the negative gradient of the binomial or multinomial deviance loss function. Binary classification is a special case where only a single regression tree is induced. sklearn.tree.DecisionTreeClassifier : A non-parametric supervised learning method used for classification. Creates a model that predicts the value of a target variable by learning simple decision rules inferred from the data features. References \u00b6 .. [1] Y. Freund, R. Schapire, \"A Decision-Theoretic Generalization of on-Line Learning and an Application to Boosting\", 1995. .. [2] J. Zhu, H. Zou, S. Rosset, T. Hastie, \"Multi-class AdaBoost\", 2009. Examples \u00b6 from sklearn.ensemble import AdaBoostClassifier from sklearn.datasets import make_classification X, y = make_classification(n_samples=1000, n_features=4, ... n_informative=2, n_redundant=0, ... random_state=0, shuffle=False) clf = AdaBoostClassifier(n_estimators=100, random_state=0) clf.fit(X, y) AdaBoostClassifier(n_estimators=100, random_state=0) clf.predict([[0, 0, 0, 0]]) array([1]) clf.score(X, y) 0.983... decision_function ( self , X ) \u00b6 Compute the decision function of X . Parameters \u00b6 X : {array-like, sparse matrix} of shape (n_samples, n_features) The training input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. COO, DOK, and LIL are converted to CSR. Returns \u00b6 score : ndarray of shape of (n_samples, k) The decision function of the input samples. The order of outputs is the same of that of the :term: classes_ attribute. Binary classification is a special cases with k == 1 , otherwise k==n_classes . For binary classification, values closer to -1 or 1 mean more like the first or second class in classes_ , respectively. Source code in giants/config.py def decision_function ( self , X ): \"\"\"Compute the decision function of ``X``. Parameters ---------- X : {array-like, sparse matrix} of shape (n_samples, n_features) The training input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. COO, DOK, and LIL are converted to CSR. Returns ------- score : ndarray of shape of (n_samples, k) The decision function of the input samples. The order of outputs is the same of that of the :term:`classes_` attribute. Binary classification is a special cases with ``k == 1``, otherwise ``k==n_classes``. For binary classification, values closer to -1 or 1 mean more like the first or second class in ``classes_``, respectively. \"\"\" check_is_fitted ( self ) X = self . _check_X ( X ) n_classes = self . n_classes_ classes = self . classes_ [:, np . newaxis ] if self . algorithm == \"SAMME.R\" : # The weights are all 1. for SAMME.R pred = sum ( _samme_proba ( estimator , n_classes , X ) for estimator in self . estimators_ ) else : # self.algorithm == \"SAMME\" pred = sum ( ( estimator . predict ( X ) == classes ) . T * w for estimator , w in zip ( self . estimators_ , self . estimator_weights_ ) ) pred /= self . estimator_weights_ . sum () if n_classes == 2 : pred [:, 0 ] *= - 1 return pred . sum ( axis = 1 ) return pred fit ( self , X , y , sample_weight = None ) \u00b6 Build a boosted classifier from the training set (X, y). Parameters \u00b6 X : {array-like, sparse matrix} of shape (n_samples, n_features) The training input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. COO, DOK, and LIL are converted to CSR. y : array-like of shape (n_samples,) The target values (class labels). sample_weight : array-like of shape (n_samples,), default=None Sample weights. If None, the sample weights are initialized to 1 / n_samples . Returns \u00b6 self : object Fitted estimator. Source code in giants/config.py def fit ( self , X , y , sample_weight = None ): \"\"\"Build a boosted classifier from the training set (X, y). Parameters ---------- X : {array-like, sparse matrix} of shape (n_samples, n_features) The training input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. COO, DOK, and LIL are converted to CSR. y : array-like of shape (n_samples,) The target values (class labels). sample_weight : array-like of shape (n_samples,), default=None Sample weights. If None, the sample weights are initialized to ``1 / n_samples``. Returns ------- self : object Fitted estimator. \"\"\" # Check that algorithm is supported if self . algorithm not in ( \"SAMME\" , \"SAMME.R\" ): raise ValueError ( \"algorithm %s is not supported\" % self . algorithm ) # Fit return super () . fit ( X , y , sample_weight ) predict ( self , X ) \u00b6 Predict classes for X. The predicted class of an input sample is computed as the weighted mean prediction of the classifiers in the ensemble. Parameters \u00b6 X : {array-like, sparse matrix} of shape (n_samples, n_features) The training input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. COO, DOK, and LIL are converted to CSR. Returns \u00b6 y : ndarray of shape (n_samples,) The predicted classes. Source code in giants/config.py def predict ( self , X ): \"\"\"Predict classes for X. The predicted class of an input sample is computed as the weighted mean prediction of the classifiers in the ensemble. Parameters ---------- X : {array-like, sparse matrix} of shape (n_samples, n_features) The training input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. COO, DOK, and LIL are converted to CSR. Returns ------- y : ndarray of shape (n_samples,) The predicted classes. \"\"\" pred = self . decision_function ( X ) if self . n_classes_ == 2 : return self . classes_ . take ( pred > 0 , axis = 0 ) return self . classes_ . take ( np . argmax ( pred , axis = 1 ), axis = 0 ) predict_log_proba ( self , X ) \u00b6 Predict class log-probabilities for X. The predicted class log-probabilities of an input sample is computed as the weighted mean predicted class log-probabilities of the classifiers in the ensemble. Parameters \u00b6 X : {array-like, sparse matrix} of shape (n_samples, n_features) The training input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. COO, DOK, and LIL are converted to CSR. Returns \u00b6 p : ndarray of shape (n_samples, n_classes) The class probabilities of the input samples. The order of outputs is the same of that of the :term: classes_ attribute. Source code in giants/config.py def predict_log_proba ( self , X ): \"\"\"Predict class log-probabilities for X. The predicted class log-probabilities of an input sample is computed as the weighted mean predicted class log-probabilities of the classifiers in the ensemble. Parameters ---------- X : {array-like, sparse matrix} of shape (n_samples, n_features) The training input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. COO, DOK, and LIL are converted to CSR. Returns ------- p : ndarray of shape (n_samples, n_classes) The class probabilities of the input samples. The order of outputs is the same of that of the :term:`classes_` attribute. \"\"\" return np . log ( self . predict_proba ( X )) predict_proba ( self , X ) \u00b6 Predict class probabilities for X. The predicted class probabilities of an input sample is computed as the weighted mean predicted class probabilities of the classifiers in the ensemble. Parameters \u00b6 X : {array-like, sparse matrix} of shape (n_samples, n_features) The training input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. COO, DOK, and LIL are converted to CSR. Returns \u00b6 p : ndarray of shape (n_samples, n_classes) The class probabilities of the input samples. The order of outputs is the same of that of the :term: classes_ attribute. Source code in giants/config.py def predict_proba ( self , X ): \"\"\"Predict class probabilities for X. The predicted class probabilities of an input sample is computed as the weighted mean predicted class probabilities of the classifiers in the ensemble. Parameters ---------- X : {array-like, sparse matrix} of shape (n_samples, n_features) The training input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. COO, DOK, and LIL are converted to CSR. Returns ------- p : ndarray of shape (n_samples, n_classes) The class probabilities of the input samples. The order of outputs is the same of that of the :term:`classes_` attribute. \"\"\" check_is_fitted ( self ) n_classes = self . n_classes_ if n_classes == 1 : return np . ones (( _num_samples ( X ), 1 )) decision = self . decision_function ( X ) return self . _compute_proba_from_decision ( decision , n_classes ) staged_decision_function ( self , X ) \u00b6 Compute decision function of X for each boosting iteration. This method allows monitoring (i.e. determine error on testing set) after each boosting iteration. Parameters \u00b6 X : {array-like, sparse matrix} of shape (n_samples, n_features) The training input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. COO, DOK, and LIL are converted to CSR. Yields \u00b6 score : generator of ndarray of shape (n_samples, k) The decision function of the input samples. The order of outputs is the same of that of the :term: classes_ attribute. Binary classification is a special cases with k == 1 , otherwise k==n_classes . For binary classification, values closer to -1 or 1 mean more like the first or second class in classes_ , respectively. Source code in giants/config.py def staged_decision_function ( self , X ): \"\"\"Compute decision function of ``X`` for each boosting iteration. This method allows monitoring (i.e. determine error on testing set) after each boosting iteration. Parameters ---------- X : {array-like, sparse matrix} of shape (n_samples, n_features) The training input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. COO, DOK, and LIL are converted to CSR. Yields ------ score : generator of ndarray of shape (n_samples, k) The decision function of the input samples. The order of outputs is the same of that of the :term:`classes_` attribute. Binary classification is a special cases with ``k == 1``, otherwise ``k==n_classes``. For binary classification, values closer to -1 or 1 mean more like the first or second class in ``classes_``, respectively. \"\"\" check_is_fitted ( self ) X = self . _check_X ( X ) n_classes = self . n_classes_ classes = self . classes_ [:, np . newaxis ] pred = None norm = 0.0 for weight , estimator in zip ( self . estimator_weights_ , self . estimators_ ): norm += weight if self . algorithm == \"SAMME.R\" : # The weights are all 1. for SAMME.R current_pred = _samme_proba ( estimator , n_classes , X ) else : # elif self.algorithm == \"SAMME\": current_pred = estimator . predict ( X ) current_pred = ( current_pred == classes ) . T * weight if pred is None : pred = current_pred else : pred += current_pred if n_classes == 2 : tmp_pred = np . copy ( pred ) tmp_pred [:, 0 ] *= - 1 yield ( tmp_pred / norm ) . sum ( axis = 1 ) else : yield pred / norm staged_predict ( self , X ) \u00b6 Return staged predictions for X. The predicted class of an input sample is computed as the weighted mean prediction of the classifiers in the ensemble. This generator method yields the ensemble prediction after each iteration of boosting and therefore allows monitoring, such as to determine the prediction on a test set after each boost. Parameters \u00b6 X : array-like of shape (n_samples, n_features) The input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. COO, DOK, and LIL are converted to CSR. Yields \u00b6 y : generator of ndarray of shape (n_samples,) The predicted classes. Source code in giants/config.py def staged_predict ( self , X ): \"\"\"Return staged predictions for X. The predicted class of an input sample is computed as the weighted mean prediction of the classifiers in the ensemble. This generator method yields the ensemble prediction after each iteration of boosting and therefore allows monitoring, such as to determine the prediction on a test set after each boost. Parameters ---------- X : array-like of shape (n_samples, n_features) The input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. COO, DOK, and LIL are converted to CSR. Yields ------ y : generator of ndarray of shape (n_samples,) The predicted classes. \"\"\" X = self . _check_X ( X ) n_classes = self . n_classes_ classes = self . classes_ if n_classes == 2 : for pred in self . staged_decision_function ( X ): yield np . array ( classes . take ( pred > 0 , axis = 0 )) else : for pred in self . staged_decision_function ( X ): yield np . array ( classes . take ( np . argmax ( pred , axis = 1 ), axis = 0 )) staged_predict_proba ( self , X ) \u00b6 Predict class probabilities for X. The predicted class probabilities of an input sample is computed as the weighted mean predicted class probabilities of the classifiers in the ensemble. This generator method yields the ensemble predicted class probabilities after each iteration of boosting and therefore allows monitoring, such as to determine the predicted class probabilities on a test set after each boost. Parameters \u00b6 X : {array-like, sparse matrix} of shape (n_samples, n_features) The training input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. COO, DOK, and LIL are converted to CSR. Yields \u00b6 p : generator of ndarray of shape (n_samples,) The class probabilities of the input samples. The order of outputs is the same of that of the :term: classes_ attribute. Source code in giants/config.py def staged_predict_proba ( self , X ): \"\"\"Predict class probabilities for X. The predicted class probabilities of an input sample is computed as the weighted mean predicted class probabilities of the classifiers in the ensemble. This generator method yields the ensemble predicted class probabilities after each iteration of boosting and therefore allows monitoring, such as to determine the predicted class probabilities on a test set after each boost. Parameters ---------- X : {array-like, sparse matrix} of shape (n_samples, n_features) The training input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. COO, DOK, and LIL are converted to CSR. Yields ------ p : generator of ndarray of shape (n_samples,) The class probabilities of the input samples. The order of outputs is the same of that of the :term:`classes_` attribute. \"\"\" n_classes = self . n_classes_ for decision in self . staged_decision_function ( X ): yield self . _compute_proba_from_decision ( decision , n_classes ) AdaBoostRegressor ( RegressorMixin , BaseWeightBoosting ) \u00b6 An AdaBoost regressor. An AdaBoost [1] regressor is a meta-estimator that begins by fitting a regressor on the original dataset and then fits additional copies of the regressor on the same dataset but where the weights of instances are adjusted according to the error of the current prediction. As such, subsequent regressors focus more on difficult cases. This class implements the algorithm known as AdaBoost.R2 [2]. Read more in the :ref: User Guide <adaboost> . .. versionadded:: 0.14 Parameters \u00b6 base_estimator : object, default=None The base estimator from which the boosted ensemble is built. If None , then the base estimator is :class: ~sklearn.tree.DecisionTreeRegressor initialized with max_depth=3 . n_estimators : int, default=50 The maximum number of estimators at which boosting is terminated. In case of perfect fit, the learning procedure is stopped early. learning_rate : float, default=1.0 Weight applied to each regressor at each boosting iteration. A higher learning rate increases the contribution of each regressor. There is a trade-off between the learning_rate and n_estimators parameters. loss : {'linear', 'square', 'exponential'}, default='linear' The loss function to use when updating the weights after each boosting iteration. random_state : int, RandomState instance or None, default=None Controls the random seed given at each base_estimator at each boosting iteration. Thus, it is only used when base_estimator exposes a random_state . In addition, it controls the bootstrap of the weights used to train the base_estimator at each boosting iteration. Pass an int for reproducible output across multiple function calls. See :term: Glossary <random_state> . Attributes \u00b6 base_estimator_ : estimator The base estimator from which the ensemble is grown. estimators_ : list of regressors The collection of fitted sub-estimators. estimator_weights_ : ndarray of floats Weights for each estimator in the boosted ensemble. estimator_errors_ : ndarray of floats Regression error for each estimator in the boosted ensemble. feature_importances_ : ndarray of shape (n_features,) The impurity-based feature importances if supported by the base_estimator (when based on decision trees). Warning: impurity-based feature importances can be misleading for high cardinality features (many unique values). See :func:`sklearn.inspection.permutation_importance` as an alternative. n_features_in_ : int Number of features seen during :term: fit . .. versionadded:: 0.24 feature_names_in_ : ndarray of shape ( n_features_in_ ,) Names of features seen during :term: fit . Defined only when X has feature names that are all strings. .. versionadded:: 1.0 See Also \u00b6 AdaBoostClassifier : An AdaBoost classifier. GradientBoostingRegressor : Gradient Boosting Classification Tree. sklearn.tree.DecisionTreeRegressor : A decision tree regressor. References \u00b6 .. [1] Y. Freund, R. Schapire, \"A Decision-Theoretic Generalization of on-Line Learning and an Application to Boosting\", 1995. .. [2] H. Drucker, \"Improving Regressors using Boosting Techniques\", 1997. Examples \u00b6 from sklearn.ensemble import AdaBoostRegressor from sklearn.datasets import make_regression X, y = make_regression(n_features=4, n_informative=2, ... random_state=0, shuffle=False) regr = AdaBoostRegressor(random_state=0, n_estimators=100) regr.fit(X, y) AdaBoostRegressor(n_estimators=100, random_state=0) regr.predict([[0, 0, 0, 0]]) array([4.7972...]) regr.score(X, y) 0.9771... fit ( self , X , y , sample_weight = None ) \u00b6 Build a boosted regressor from the training set (X, y). Parameters \u00b6 X : {array-like, sparse matrix} of shape (n_samples, n_features) The training input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. COO, DOK, and LIL are converted to CSR. y : array-like of shape (n_samples,) The target values (real numbers). sample_weight : array-like of shape (n_samples,), default=None Sample weights. If None, the sample weights are initialized to 1 / n_samples. Returns \u00b6 self : object Fitted AdaBoostRegressor estimator. Source code in giants/config.py def fit ( self , X , y , sample_weight = None ): \"\"\"Build a boosted regressor from the training set (X, y). Parameters ---------- X : {array-like, sparse matrix} of shape (n_samples, n_features) The training input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. COO, DOK, and LIL are converted to CSR. y : array-like of shape (n_samples,) The target values (real numbers). sample_weight : array-like of shape (n_samples,), default=None Sample weights. If None, the sample weights are initialized to 1 / n_samples. Returns ------- self : object Fitted AdaBoostRegressor estimator. \"\"\" # Check loss if self . loss not in ( \"linear\" , \"square\" , \"exponential\" ): raise ValueError ( \"loss must be 'linear', 'square', or 'exponential'\" ) # Fit return super () . fit ( X , y , sample_weight ) predict ( self , X ) \u00b6 Predict regression value for X. The predicted regression value of an input sample is computed as the weighted median prediction of the regressors in the ensemble. Parameters \u00b6 X : {array-like, sparse matrix} of shape (n_samples, n_features) The training input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. COO, DOK, and LIL are converted to CSR. Returns \u00b6 y : ndarray of shape (n_samples,) The predicted regression values. Source code in giants/config.py def predict ( self , X ): \"\"\"Predict regression value for X. The predicted regression value of an input sample is computed as the weighted median prediction of the regressors in the ensemble. Parameters ---------- X : {array-like, sparse matrix} of shape (n_samples, n_features) The training input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. COO, DOK, and LIL are converted to CSR. Returns ------- y : ndarray of shape (n_samples,) The predicted regression values. \"\"\" check_is_fitted ( self ) X = self . _check_X ( X ) return self . _get_median_predict ( X , len ( self . estimators_ )) staged_predict ( self , X ) \u00b6 Return staged predictions for X. The predicted regression value of an input sample is computed as the weighted median prediction of the regressors in the ensemble. This generator method yields the ensemble prediction after each iteration of boosting and therefore allows monitoring, such as to determine the prediction on a test set after each boost. Parameters \u00b6 X : {array-like, sparse matrix} of shape (n_samples, n_features) The training input samples. Yields \u00b6 y : generator of ndarray of shape (n_samples,) The predicted regression values. Source code in giants/config.py def staged_predict ( self , X ): \"\"\"Return staged predictions for X. The predicted regression value of an input sample is computed as the weighted median prediction of the regressors in the ensemble. This generator method yields the ensemble prediction after each iteration of boosting and therefore allows monitoring, such as to determine the prediction on a test set after each boost. Parameters ---------- X : {array-like, sparse matrix} of shape (n_samples, n_features) The training input samples. Yields ------- y : generator of ndarray of shape (n_samples,) The predicted regression values. \"\"\" check_is_fitted ( self ) X = self . _check_X ( X ) for i , _ in enumerate ( self . estimators_ , 1 ): yield self . _get_median_predict ( X , limit = i ) DecisionTreeClassifier ( ClassifierMixin , BaseDecisionTree ) \u00b6 A decision tree classifier. Read more in the :ref: User Guide <tree> . Parameters \u00b6 criterion : {\"gini\", \"entropy\"}, default=\"gini\" The function to measure the quality of a split. Supported criteria are \"gini\" for the Gini impurity and \"entropy\" for the information gain. splitter : {\"best\", \"random\"}, default=\"best\" The strategy used to choose the split at each node. Supported strategies are \"best\" to choose the best split and \"random\" to choose the best random split. max_depth : int, default=None The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples. min_samples_split : int or float, default=2 The minimum number of samples required to split an internal node: - If int, then consider `min_samples_split` as the minimum number. - If float, then `min_samples_split` is a fraction and `ceil(min_samples_split * n_samples)` are the minimum number of samples for each split. .. versionchanged:: 0.18 Added float values for fractions. min_samples_leaf : int or float, default=1 The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least min_samples_leaf training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression. - If int, then consider `min_samples_leaf` as the minimum number. - If float, then `min_samples_leaf` is a fraction and `ceil(min_samples_leaf * n_samples)` are the minimum number of samples for each node. .. versionchanged:: 0.18 Added float values for fractions. min_weight_fraction_leaf : float, default=0.0 The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided. max_features : int, float or {\"auto\", \"sqrt\", \"log2\"}, default=None The number of features to consider when looking for the best split: - If int, then consider `max_features` features at each split. - If float, then `max_features` is a fraction and `int(max_features * n_features)` features are considered at each split. - If \"auto\", then `max_features=sqrt(n_features)`. - If \"sqrt\", then `max_features=sqrt(n_features)`. - If \"log2\", then `max_features=log2(n_features)`. - If None, then `max_features=n_features`. Note: the search for a split does not stop until at least one valid partition of the node samples is found, even if it requires to effectively inspect more than ``max_features`` features. random_state : int, RandomState instance or None, default=None Controls the randomness of the estimator. The features are always randomly permuted at each split, even if splitter is set to \"best\" . When max_features < n_features , the algorithm will select max_features at random at each split before finding the best split among them. But the best found split may vary across different runs, even if max_features=n_features . That is the case, if the improvement of the criterion is identical for several splits and one split has to be selected at random. To obtain a deterministic behaviour during fitting, random_state has to be fixed to an integer. See :term: Glossary <random_state> for details. max_leaf_nodes : int, default=None Grow a tree with max_leaf_nodes in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes. min_impurity_decrease : float, default=0.0 A node will be split if this split induces a decrease of the impurity greater than or equal to this value. The weighted impurity decrease equation is the following:: N_t / N * (impurity - N_t_R / N_t * right_impurity - N_t_L / N_t * left_impurity) where ``N`` is the total number of samples, ``N_t`` is the number of samples at the current node, ``N_t_L`` is the number of samples in the left child, and ``N_t_R`` is the number of samples in the right child. ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum, if ``sample_weight`` is passed. .. versionadded:: 0.19 class_weight : dict, list of dict or \"balanced\", default=None Weights associated with classes in the form {class_label: weight} . If None, all classes are supposed to have weight one. For multi-output problems, a list of dicts can be provided in the same order as the columns of y. Note that for multioutput (including multilabel) weights should be defined for each class of every column in its own dict. For example, for four-class multilabel classification weights should be [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of [{1:1}, {2:5}, {3:1}, {4:1}]. The \"balanced\" mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as ``n_samples / (n_classes * np.bincount(y))`` For multi-output, the weights of each column of y will be multiplied. Note that these weights will be multiplied with sample_weight (passed through the fit method) if sample_weight is specified. ccp_alpha : non-negative float, default=0.0 Complexity parameter used for Minimal Cost-Complexity Pruning. The subtree with the largest cost complexity that is smaller than ccp_alpha will be chosen. By default, no pruning is performed. See :ref: minimal_cost_complexity_pruning for details. .. versionadded:: 0.22 Attributes \u00b6 classes_ : ndarray of shape (n_classes,) or list of ndarray The classes labels (single output problem), or a list of arrays of class labels (multi-output problem). feature_importances_ : ndarray of shape (n_features,) The impurity-based feature importances. The higher, the more important the feature. The importance of a feature is computed as the (normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance [4]_. Warning: impurity-based feature importances can be misleading for high cardinality features (many unique values). See :func:`sklearn.inspection.permutation_importance` as an alternative. max_features_ : int The inferred value of max_features. n_classes_ : int or list of int The number of classes (for single output problems), or a list containing the number of classes for each output (for multi-output problems). n_features_ : int The number of features when fit is performed. .. deprecated:: 1.0 `n_features_` is deprecated in 1.0 and will be removed in 1.2. Use `n_features_in_` instead. n_features_in_ : int Number of features seen during :term: fit . .. versionadded:: 0.24 feature_names_in_ : ndarray of shape ( n_features_in_ ,) Names of features seen during :term: fit . Defined only when X has feature names that are all strings. .. versionadded:: 1.0 n_outputs_ : int The number of outputs when fit is performed. tree_ : Tree instance The underlying Tree object. Please refer to help(sklearn.tree._tree.Tree) for attributes of Tree object and :ref: sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py for basic usage of these attributes. See Also \u00b6 DecisionTreeRegressor : A decision tree regressor. Notes \u00b6 The default values for the parameters controlling the size of the trees (e.g. max_depth , min_samples_leaf , etc.) lead to fully grown and unpruned trees which can potentially be very large on some data sets. To reduce memory consumption, the complexity and size of the trees should be controlled by setting those parameter values. The :meth: predict method operates using the :func: numpy.argmax function on the outputs of :meth: predict_proba . This means that in case the highest predicted probabilities are tied, the classifier will predict the tied class with the lowest index in :term: classes_ . References \u00b6 .. [1] en.wikipedia.org/wiki/Decision_tree_learning .. [2] L. Breiman, J. Friedman, R. Olshen, and C. Stone, \"Classification and Regression Trees\", Wadsworth, Belmont, CA, 1984. .. [3] T. Hastie, R. Tibshirani and J. Friedman. \"Elements of Statistical Learning\", Springer, 2009. .. [4] L. Breiman, and A. Cutler, \"Random Forests\", www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm Examples \u00b6 from sklearn.datasets import load_iris from sklearn.model_selection import cross_val_score from sklearn.tree import DecisionTreeClassifier clf = DecisionTreeClassifier(random_state=0) iris = load_iris() cross_val_score(clf, iris.data, iris.target, cv=10) ... # doctest: +SKIP ... array([ 1. , 0.93..., 0.86..., 0.93..., 0.93..., 0.93..., 0.93..., 1. , 0.93..., 1. ]) n_features_ : None property readonly \u00b6 DEPRECATED: The attribute n_features_ is deprecated in 1.0 and will be removed in 1.2. Use n_features_in_ instead. fit ( self , X , y , sample_weight = None , check_input = True , X_idx_sorted = 'deprecated' ) \u00b6 Build a decision tree classifier from the training set (X, y). Parameters \u00b6 X : {array-like, sparse matrix} of shape (n_samples, n_features) The training input samples. Internally, it will be converted to dtype=np.float32 and if a sparse matrix is provided to a sparse csc_matrix . y : array-like of shape (n_samples,) or (n_samples, n_outputs) The target values (class labels) as integers or strings. sample_weight : array-like of shape (n_samples,), default=None Sample weights. If None, then samples are equally weighted. Splits that would create child nodes with net zero or negative weight are ignored while searching for a split in each node. Splits are also ignored if they would result in any single class carrying a negative weight in either child node. check_input : bool, default=True Allow to bypass several input checking. Don't use this parameter unless you know what you do. X_idx_sorted : deprecated, default=\"deprecated\" This parameter is deprecated and has no effect. It will be removed in 1.1 (renaming of 0.26). .. deprecated:: 0.24 Returns \u00b6 self : DecisionTreeClassifier Fitted estimator. Source code in giants/config.py def fit ( self , X , y , sample_weight = None , check_input = True , X_idx_sorted = \"deprecated\" ): \"\"\"Build a decision tree classifier from the training set (X, y). Parameters ---------- X : {array-like, sparse matrix} of shape (n_samples, n_features) The training input samples. Internally, it will be converted to ``dtype=np.float32`` and if a sparse matrix is provided to a sparse ``csc_matrix``. y : array-like of shape (n_samples,) or (n_samples, n_outputs) The target values (class labels) as integers or strings. sample_weight : array-like of shape (n_samples,), default=None Sample weights. If None, then samples are equally weighted. Splits that would create child nodes with net zero or negative weight are ignored while searching for a split in each node. Splits are also ignored if they would result in any single class carrying a negative weight in either child node. check_input : bool, default=True Allow to bypass several input checking. Don't use this parameter unless you know what you do. X_idx_sorted : deprecated, default=\"deprecated\" This parameter is deprecated and has no effect. It will be removed in 1.1 (renaming of 0.26). .. deprecated:: 0.24 Returns ------- self : DecisionTreeClassifier Fitted estimator. \"\"\" super () . fit ( X , y , sample_weight = sample_weight , check_input = check_input , X_idx_sorted = X_idx_sorted , ) return self predict_log_proba ( self , X ) \u00b6 Predict class log-probabilities of the input samples X. Parameters \u00b6 X : {array-like, sparse matrix} of shape (n_samples, n_features) The input samples. Internally, it will be converted to dtype=np.float32 and if a sparse matrix is provided to a sparse csr_matrix . Returns \u00b6 proba : ndarray of shape (n_samples, n_classes) or list of n_outputs such arrays if n_outputs > 1 The class log-probabilities of the input samples. The order of the classes corresponds to that in the attribute :term: classes_ . Source code in giants/config.py def predict_log_proba ( self , X ): \"\"\"Predict class log-probabilities of the input samples X. Parameters ---------- X : {array-like, sparse matrix} of shape (n_samples, n_features) The input samples. Internally, it will be converted to ``dtype=np.float32`` and if a sparse matrix is provided to a sparse ``csr_matrix``. Returns ------- proba : ndarray of shape (n_samples, n_classes) or list of n_outputs \\ such arrays if n_outputs > 1 The class log-probabilities of the input samples. The order of the classes corresponds to that in the attribute :term:`classes_`. \"\"\" proba = self . predict_proba ( X ) if self . n_outputs_ == 1 : return np . log ( proba ) else : for k in range ( self . n_outputs_ ): proba [ k ] = np . log ( proba [ k ]) return proba predict_proba ( self , X , check_input = True ) \u00b6 Predict class probabilities of the input samples X. The predicted class probability is the fraction of samples of the same class in a leaf. Parameters \u00b6 X : {array-like, sparse matrix} of shape (n_samples, n_features) The input samples. Internally, it will be converted to dtype=np.float32 and if a sparse matrix is provided to a sparse csr_matrix . check_input : bool, default=True Allow to bypass several input checking. Don't use this parameter unless you know what you do. Returns \u00b6 proba : ndarray of shape (n_samples, n_classes) or list of n_outputs such arrays if n_outputs > 1 The class probabilities of the input samples. The order of the classes corresponds to that in the attribute :term: classes_ . Source code in giants/config.py def predict_proba ( self , X , check_input = True ): \"\"\"Predict class probabilities of the input samples X. The predicted class probability is the fraction of samples of the same class in a leaf. Parameters ---------- X : {array-like, sparse matrix} of shape (n_samples, n_features) The input samples. Internally, it will be converted to ``dtype=np.float32`` and if a sparse matrix is provided to a sparse ``csr_matrix``. check_input : bool, default=True Allow to bypass several input checking. Don't use this parameter unless you know what you do. Returns ------- proba : ndarray of shape (n_samples, n_classes) or list of n_outputs \\ such arrays if n_outputs > 1 The class probabilities of the input samples. The order of the classes corresponds to that in the attribute :term:`classes_`. \"\"\" check_is_fitted ( self ) X = self . _validate_X_predict ( X , check_input ) proba = self . tree_ . predict ( X ) if self . n_outputs_ == 1 : proba = proba [:, : self . n_classes_ ] normalizer = proba . sum ( axis = 1 )[:, np . newaxis ] normalizer [ normalizer == 0.0 ] = 1.0 proba /= normalizer return proba else : all_proba = [] for k in range ( self . n_outputs_ ): proba_k = proba [:, k , : self . n_classes_ [ k ]] normalizer = proba_k . sum ( axis = 1 )[:, np . newaxis ] normalizer [ normalizer == 0.0 ] = 1.0 proba_k /= normalizer all_proba . append ( proba_k ) return all_proba GradientBoostingClassifier ( ClassifierMixin , BaseGradientBoosting ) \u00b6 Gradient Boosting for classification. GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage n_classes_ regression trees are fit on the negative gradient of the binomial or multinomial deviance loss function. Binary classification is a special case where only a single regression tree is induced. Read more in the :ref: User Guide <gradient_boosting> . Parameters \u00b6 loss : {'deviance', 'exponential'}, default='deviance' The loss function to be optimized. 'deviance' refers to deviance (= logistic regression) for classification with probabilistic outputs. For loss 'exponential' gradient boosting recovers the AdaBoost algorithm. learning_rate : float, default=0.1 Learning rate shrinks the contribution of each tree by learning_rate . There is a trade-off between learning_rate and n_estimators. n_estimators : int, default=100 The number of boosting stages to perform. Gradient boosting is fairly robust to over-fitting so a large number usually results in better performance. subsample : float, default=1.0 The fraction of samples to be used for fitting the individual base learners. If smaller than 1.0 this results in Stochastic Gradient Boosting. subsample interacts with the parameter n_estimators . Choosing subsample < 1.0 leads to a reduction of variance and an increase in bias. criterion : {'friedman_mse', 'squared_error', 'mse', 'mae'}, default='friedman_mse' The function to measure the quality of a split. Supported criteria are 'friedman_mse' for the mean squared error with improvement score by Friedman, 'squared_error' for mean squared error, and 'mae' for the mean absolute error. The default value of 'friedman_mse' is generally the best as it can provide a better approximation in some cases. .. versionadded:: 0.18 .. deprecated:: 0.24 `criterion='mae'` is deprecated and will be removed in version 1.1 (renaming of 0.26). Use `criterion='friedman_mse'` or `'squared_error'` instead, as trees should use a squared error criterion in Gradient Boosting. .. deprecated:: 1.0 Criterion 'mse' was deprecated in v1.0 and will be removed in version 1.2. Use `criterion='squared_error'` which is equivalent. min_samples_split : int or float, default=2 The minimum number of samples required to split an internal node: - If int, then consider `min_samples_split` as the minimum number. - If float, then `min_samples_split` is a fraction and `ceil(min_samples_split * n_samples)` are the minimum number of samples for each split. .. versionchanged:: 0.18 Added float values for fractions. min_samples_leaf : int or float, default=1 The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least min_samples_leaf training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression. - If int, then consider `min_samples_leaf` as the minimum number. - If float, then `min_samples_leaf` is a fraction and `ceil(min_samples_leaf * n_samples)` are the minimum number of samples for each node. .. versionchanged:: 0.18 Added float values for fractions. min_weight_fraction_leaf : float, default=0.0 The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided. max_depth : int, default=3 The maximum depth of the individual regression estimators. The maximum depth limits the number of nodes in the tree. Tune this parameter for best performance; the best value depends on the interaction of the input variables. min_impurity_decrease : float, default=0.0 A node will be split if this split induces a decrease of the impurity greater than or equal to this value. The weighted impurity decrease equation is the following:: N_t / N * (impurity - N_t_R / N_t * right_impurity - N_t_L / N_t * left_impurity) where ``N`` is the total number of samples, ``N_t`` is the number of samples at the current node, ``N_t_L`` is the number of samples in the left child, and ``N_t_R`` is the number of samples in the right child. ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum, if ``sample_weight`` is passed. .. versionadded:: 0.19 init : estimator or 'zero', default=None An estimator object that is used to compute the initial predictions. init has to provide :meth: fit and :meth: predict_proba . If 'zero', the initial raw predictions are set to zero. By default, a DummyEstimator predicting the classes priors is used. random_state : int, RandomState instance or None, default=None Controls the random seed given to each Tree estimator at each boosting iteration. In addition, it controls the random permutation of the features at each split (see Notes for more details). It also controls the random splitting of the training data to obtain a validation set if n_iter_no_change is not None. Pass an int for reproducible output across multiple function calls. See :term: Glossary <random_state> . max_features : {'auto', 'sqrt', 'log2'}, int or float, default=None The number of features to consider when looking for the best split: - If int, then consider `max_features` features at each split. - If float, then `max_features` is a fraction and `int(max_features * n_features)` features are considered at each split. - If 'auto', then `max_features=sqrt(n_features)`. - If 'sqrt', then `max_features=sqrt(n_features)`. - If 'log2', then `max_features=log2(n_features)`. - If None, then `max_features=n_features`. Choosing `max_features < n_features` leads to a reduction of variance and an increase in bias. Note: the search for a split does not stop until at least one valid partition of the node samples is found, even if it requires to effectively inspect more than ``max_features`` features. verbose : int, default=0 Enable verbose output. If 1 then it prints progress and performance once in a while (the more trees the lower the frequency). If greater than 1 then it prints progress and performance for every tree. max_leaf_nodes : int, default=None Grow trees with max_leaf_nodes in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes. warm_start : bool, default=False When set to True , reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just erase the previous solution. See :term: the Glossary <warm_start> . validation_fraction : float, default=0.1 The proportion of training data to set aside as validation set for early stopping. Must be between 0 and 1. Only used if n_iter_no_change is set to an integer. .. versionadded:: 0.20 n_iter_no_change : int, default=None n_iter_no_change is used to decide if early stopping will be used to terminate training when validation score is not improving. By default it is set to None to disable early stopping. If set to a number, it will set aside validation_fraction size of the training data as validation and terminate training when validation score is not improving in all of the previous n_iter_no_change numbers of iterations. The split is stratified. .. versionadded:: 0.20 tol : float, default=1e-4 Tolerance for the early stopping. When the loss is not improving by at least tol for n_iter_no_change iterations (if set to a number), the training stops. .. versionadded:: 0.20 ccp_alpha : non-negative float, default=0.0 Complexity parameter used for Minimal Cost-Complexity Pruning. The subtree with the largest cost complexity that is smaller than ccp_alpha will be chosen. By default, no pruning is performed. See :ref: minimal_cost_complexity_pruning for details. .. versionadded:: 0.22 Attributes \u00b6 n_estimators_ : int The number of estimators as selected by early stopping (if n_iter_no_change is specified). Otherwise it is set to n_estimators . .. versionadded:: 0.20 feature_importances_ : ndarray of shape (n_features,) The impurity-based feature importances. The higher, the more important the feature. The importance of a feature is computed as the (normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance. Warning: impurity-based feature importances can be misleading for high cardinality features (many unique values). See :func:`sklearn.inspection.permutation_importance` as an alternative. oob_improvement_ : ndarray of shape (n_estimators,) The improvement in loss (= deviance) on the out-of-bag samples relative to the previous iteration. oob_improvement_[0] is the improvement in loss of the first stage over the init estimator. Only available if subsample < 1.0 train_score_ : ndarray of shape (n_estimators,) The i-th score train_score_[i] is the deviance (= loss) of the model at iteration i on the in-bag sample. If subsample == 1 this is the deviance on the training data. loss_ : LossFunction The concrete LossFunction object. init_ : estimator The estimator that provides the initial predictions. Set via the init argument or loss.init_estimator . estimators_ : ndarray of DecisionTreeRegressor of shape (n_estimators, loss_.K ) The collection of fitted sub-estimators. loss_.K is 1 for binary classification, otherwise n_classes. classes_ : ndarray of shape (n_classes,) The classes labels. n_features_ : int The number of data features. .. deprecated:: 1.0 Attribute `n_features_` was deprecated in version 1.0 and will be removed in 1.2. Use `n_features_in_` instead. n_features_in_ : int Number of features seen during :term: fit . .. versionadded:: 0.24 feature_names_in_ : ndarray of shape ( n_features_in_ ,) Names of features seen during :term: fit . Defined only when X has feature names that are all strings. .. versionadded:: 1.0 n_classes_ : int The number of classes. max_features_ : int The inferred value of max_features. See Also \u00b6 HistGradientBoostingClassifier : Histogram-based Gradient Boosting Classification Tree. sklearn.tree.DecisionTreeClassifier : A decision tree classifier. RandomForestClassifier : A meta-estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. AdaBoostClassifier : A meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases. Notes \u00b6 The features are always randomly permuted at each split. Therefore, the best found split may vary, even with the same training data and max_features=n_features , if the improvement of the criterion is identical for several splits enumerated during the search of the best split. To obtain a deterministic behaviour during fitting, random_state has to be fixed. References \u00b6 J. Friedman, Greedy Function Approximation: A Gradient Boosting Machine, The Annals of Statistics, Vol. 29, No. 5, 2001. J. Friedman, Stochastic Gradient Boosting, 1999 T. Hastie, R. Tibshirani and J. Friedman. Elements of Statistical Learning Ed. 2, Springer, 2009. Examples \u00b6 The following example shows how to fit a gradient boosting classifier with 100 decision stumps as weak learners. from sklearn.datasets import make_hastie_10_2 from sklearn.ensemble import GradientBoostingClassifier X, y = make_hastie_10_2(random_state=0) X_train, X_test = X[:2000], X[2000:] y_train, y_test = y[:2000], y[2000:] clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, ... max_depth=1, random_state=0).fit(X_train, y_train) clf.score(X_test, y_test) 0.913... decision_function ( self , X ) \u00b6 Compute the decision function of X . Parameters \u00b6 X : {array-like, sparse matrix} of shape (n_samples, n_features) The input samples. Internally, it will be converted to dtype=np.float32 and if a sparse matrix is provided to a sparse csr_matrix . Returns \u00b6 score : ndarray of shape (n_samples, n_classes) or (n_samples,) The decision function of the input samples, which corresponds to the raw values predicted from the trees of the ensemble . The order of the classes corresponds to that in the attribute :term: classes_ . Regression and binary classification produce an array of shape (n_samples,). Source code in giants/config.py def decision_function ( self , X ): \"\"\"Compute the decision function of ``X``. Parameters ---------- X : {array-like, sparse matrix} of shape (n_samples, n_features) The input samples. Internally, it will be converted to ``dtype=np.float32`` and if a sparse matrix is provided to a sparse ``csr_matrix``. Returns ------- score : ndarray of shape (n_samples, n_classes) or (n_samples,) The decision function of the input samples, which corresponds to the raw values predicted from the trees of the ensemble . The order of the classes corresponds to that in the attribute :term:`classes_`. Regression and binary classification produce an array of shape (n_samples,). \"\"\" X = self . _validate_data ( X , dtype = DTYPE , order = \"C\" , accept_sparse = \"csr\" , reset = False ) raw_predictions = self . _raw_predict ( X ) if raw_predictions . shape [ 1 ] == 1 : return raw_predictions . ravel () return raw_predictions predict ( self , X ) \u00b6 Predict class for X. Parameters \u00b6 X : {array-like, sparse matrix} of shape (n_samples, n_features) The input samples. Internally, it will be converted to dtype=np.float32 and if a sparse matrix is provided to a sparse csr_matrix . Returns \u00b6 y : ndarray of shape (n_samples,) The predicted values. Source code in giants/config.py def predict ( self , X ): \"\"\"Predict class for X. Parameters ---------- X : {array-like, sparse matrix} of shape (n_samples, n_features) The input samples. Internally, it will be converted to ``dtype=np.float32`` and if a sparse matrix is provided to a sparse ``csr_matrix``. Returns ------- y : ndarray of shape (n_samples,) The predicted values. \"\"\" raw_predictions = self . decision_function ( X ) encoded_labels = self . loss_ . _raw_prediction_to_decision ( raw_predictions ) return self . classes_ . take ( encoded_labels , axis = 0 ) predict_log_proba ( self , X ) \u00b6 Predict class log-probabilities for X. Parameters \u00b6 X : {array-like, sparse matrix} of shape (n_samples, n_features) The input samples. Internally, it will be converted to dtype=np.float32 and if a sparse matrix is provided to a sparse csr_matrix . Returns \u00b6 p : ndarray of shape (n_samples, n_classes) The class log-probabilities of the input samples. The order of the classes corresponds to that in the attribute :term: classes_ . Raises \u00b6 AttributeError If the loss does not support probabilities. Source code in giants/config.py def predict_log_proba ( self , X ): \"\"\"Predict class log-probabilities for X. Parameters ---------- X : {array-like, sparse matrix} of shape (n_samples, n_features) The input samples. Internally, it will be converted to ``dtype=np.float32`` and if a sparse matrix is provided to a sparse ``csr_matrix``. Returns ------- p : ndarray of shape (n_samples, n_classes) The class log-probabilities of the input samples. The order of the classes corresponds to that in the attribute :term:`classes_`. Raises ------ AttributeError If the ``loss`` does not support probabilities. \"\"\" proba = self . predict_proba ( X ) return np . log ( proba ) predict_proba ( self , X ) \u00b6 Predict class probabilities for X. Parameters \u00b6 X : {array-like, sparse matrix} of shape (n_samples, n_features) The input samples. Internally, it will be converted to dtype=np.float32 and if a sparse matrix is provided to a sparse csr_matrix . Returns \u00b6 p : ndarray of shape (n_samples, n_classes) The class probabilities of the input samples. The order of the classes corresponds to that in the attribute :term: classes_ . Raises \u00b6 AttributeError If the loss does not support probabilities. Source code in giants/config.py def predict_proba ( self , X ): \"\"\"Predict class probabilities for X. Parameters ---------- X : {array-like, sparse matrix} of shape (n_samples, n_features) The input samples. Internally, it will be converted to ``dtype=np.float32`` and if a sparse matrix is provided to a sparse ``csr_matrix``. Returns ------- p : ndarray of shape (n_samples, n_classes) The class probabilities of the input samples. The order of the classes corresponds to that in the attribute :term:`classes_`. Raises ------ AttributeError If the ``loss`` does not support probabilities. \"\"\" raw_predictions = self . decision_function ( X ) try : return self . loss_ . _raw_prediction_to_proba ( raw_predictions ) except NotFittedError : raise except AttributeError as e : raise AttributeError ( \"loss= %r does not support predict_proba\" % self . loss ) from e staged_decision_function ( self , X ) \u00b6 Compute decision function of X for each iteration. This method allows monitoring (i.e. determine error on testing set) after each stage. Parameters \u00b6 X : {array-like, sparse matrix} of shape (n_samples, n_features) The input samples. Internally, it will be converted to dtype=np.float32 and if a sparse matrix is provided to a sparse csr_matrix . Yields \u00b6 score : generator of ndarray of shape (n_samples, k) The decision function of the input samples, which corresponds to the raw values predicted from the trees of the ensemble . The classes corresponds to that in the attribute :term: classes_ . Regression and binary classification are special cases with k == 1 , otherwise k==n_classes . Source code in giants/config.py def staged_decision_function ( self , X ): \"\"\"Compute decision function of ``X`` for each iteration. This method allows monitoring (i.e. determine error on testing set) after each stage. Parameters ---------- X : {array-like, sparse matrix} of shape (n_samples, n_features) The input samples. Internally, it will be converted to ``dtype=np.float32`` and if a sparse matrix is provided to a sparse ``csr_matrix``. Yields ------ score : generator of ndarray of shape (n_samples, k) The decision function of the input samples, which corresponds to the raw values predicted from the trees of the ensemble . The classes corresponds to that in the attribute :term:`classes_`. Regression and binary classification are special cases with ``k == 1``, otherwise ``k==n_classes``. \"\"\" yield from self . _staged_raw_predict ( X ) staged_predict ( self , X ) \u00b6 Predict class at each stage for X. This method allows monitoring (i.e. determine error on testing set) after each stage. Parameters \u00b6 X : {array-like, sparse matrix} of shape (n_samples, n_features) The input samples. Internally, it will be converted to dtype=np.float32 and if a sparse matrix is provided to a sparse csr_matrix . Yields \u00b6 y : generator of ndarray of shape (n_samples,) The predicted value of the input samples. Source code in giants/config.py def staged_predict ( self , X ): \"\"\"Predict class at each stage for X. This method allows monitoring (i.e. determine error on testing set) after each stage. Parameters ---------- X : {array-like, sparse matrix} of shape (n_samples, n_features) The input samples. Internally, it will be converted to ``dtype=np.float32`` and if a sparse matrix is provided to a sparse ``csr_matrix``. Yields ------- y : generator of ndarray of shape (n_samples,) The predicted value of the input samples. \"\"\" for raw_predictions in self . _staged_raw_predict ( X ): encoded_labels = self . loss_ . _raw_prediction_to_decision ( raw_predictions ) yield self . classes_ . take ( encoded_labels , axis = 0 ) staged_predict_proba ( self , X ) \u00b6 Predict class probabilities at each stage for X. This method allows monitoring (i.e. determine error on testing set) after each stage. Parameters \u00b6 X : {array-like, sparse matrix} of shape (n_samples, n_features) The input samples. Internally, it will be converted to dtype=np.float32 and if a sparse matrix is provided to a sparse csr_matrix . Yields \u00b6 y : generator of ndarray of shape (n_samples,) The predicted value of the input samples. Source code in giants/config.py def staged_predict_proba ( self , X ): \"\"\"Predict class probabilities at each stage for X. This method allows monitoring (i.e. determine error on testing set) after each stage. Parameters ---------- X : {array-like, sparse matrix} of shape (n_samples, n_features) The input samples. Internally, it will be converted to ``dtype=np.float32`` and if a sparse matrix is provided to a sparse ``csr_matrix``. Yields ------ y : generator of ndarray of shape (n_samples,) The predicted value of the input samples. \"\"\" try : for raw_predictions in self . _staged_raw_predict ( X ): yield self . loss_ . _raw_prediction_to_proba ( raw_predictions ) except NotFittedError : raise except AttributeError as e : raise AttributeError ( \"loss= %r does not support predict_proba\" % self . loss ) from e GradientBoostingRegressor ( RegressorMixin , BaseGradientBoosting ) \u00b6 Gradient Boosting for regression. GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage a regression tree is fit on the negative gradient of the given loss function. Read more in the :ref: User Guide <gradient_boosting> . Parameters \u00b6 loss : {'squared_error', 'absolute_error', 'huber', 'quantile'}, default='squared_error' Loss function to be optimized. 'squared_error' refers to the squared error for regression. 'absolute_error' refers to the absolute error of regression and is a robust loss function. 'huber' is a combination of the two. 'quantile' allows quantile regression (use alpha to specify the quantile). .. deprecated:: 1.0 The loss 'ls' was deprecated in v1.0 and will be removed in version 1.2. Use `loss='squared_error'` which is equivalent. .. deprecated:: 1.0 The loss 'lad' was deprecated in v1.0 and will be removed in version 1.2. Use `loss='absolute_error'` which is equivalent. learning_rate : float, default=0.1 Learning rate shrinks the contribution of each tree by learning_rate . There is a trade-off between learning_rate and n_estimators. n_estimators : int, default=100 The number of boosting stages to perform. Gradient boosting is fairly robust to over-fitting so a large number usually results in better performance. subsample : float, default=1.0 The fraction of samples to be used for fitting the individual base learners. If smaller than 1.0 this results in Stochastic Gradient Boosting. subsample interacts with the parameter n_estimators . Choosing subsample < 1.0 leads to a reduction of variance and an increase in bias. criterion : {'friedman_mse', 'squared_error', 'mse', 'mae'}, default='friedman_mse' The function to measure the quality of a split. Supported criteria are \"friedman_mse\" for the mean squared error with improvement score by Friedman, \"squared_error\" for mean squared error, and \"mae\" for the mean absolute error. The default value of \"friedman_mse\" is generally the best as it can provide a better approximation in some cases. .. versionadded:: 0.18 .. deprecated:: 0.24 `criterion='mae'` is deprecated and will be removed in version 1.1 (renaming of 0.26). The correct way of minimizing the absolute error is to use `loss='absolute_error'` instead. .. deprecated:: 1.0 Criterion 'mse' was deprecated in v1.0 and will be removed in version 1.2. Use `criterion='squared_error'` which is equivalent. min_samples_split : int or float, default=2 The minimum number of samples required to split an internal node: - If int, then consider `min_samples_split` as the minimum number. - If float, then `min_samples_split` is a fraction and `ceil(min_samples_split * n_samples)` are the minimum number of samples for each split. .. versionchanged:: 0.18 Added float values for fractions. min_samples_leaf : int or float, default=1 The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least min_samples_leaf training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression. - If int, then consider `min_samples_leaf` as the minimum number. - If float, then `min_samples_leaf` is a fraction and `ceil(min_samples_leaf * n_samples)` are the minimum number of samples for each node. .. versionchanged:: 0.18 Added float values for fractions. min_weight_fraction_leaf : float, default=0.0 The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided. max_depth : int, default=3 Maximum depth of the individual regression estimators. The maximum depth limits the number of nodes in the tree. Tune this parameter for best performance; the best value depends on the interaction of the input variables. min_impurity_decrease : float, default=0.0 A node will be split if this split induces a decrease of the impurity greater than or equal to this value. The weighted impurity decrease equation is the following:: N_t / N * (impurity - N_t_R / N_t * right_impurity - N_t_L / N_t * left_impurity) where ``N`` is the total number of samples, ``N_t`` is the number of samples at the current node, ``N_t_L`` is the number of samples in the left child, and ``N_t_R`` is the number of samples in the right child. ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum, if ``sample_weight`` is passed. .. versionadded:: 0.19 init : estimator or 'zero', default=None An estimator object that is used to compute the initial predictions. init has to provide :term: fit and :term: predict . If 'zero', the initial raw predictions are set to zero. By default a DummyEstimator is used, predicting either the average target value (for loss='squared_error'), or a quantile for the other losses. random_state : int, RandomState instance or None, default=None Controls the random seed given to each Tree estimator at each boosting iteration. In addition, it controls the random permutation of the features at each split (see Notes for more details). It also controls the random splitting of the training data to obtain a validation set if n_iter_no_change is not None. Pass an int for reproducible output across multiple function calls. See :term: Glossary <random_state> . max_features : {'auto', 'sqrt', 'log2'}, int or float, default=None The number of features to consider when looking for the best split: - If int, then consider `max_features` features at each split. - If float, then `max_features` is a fraction and `int(max_features * n_features)` features are considered at each split. - If \"auto\", then `max_features=n_features`. - If \"sqrt\", then `max_features=sqrt(n_features)`. - If \"log2\", then `max_features=log2(n_features)`. - If None, then `max_features=n_features`. Choosing `max_features < n_features` leads to a reduction of variance and an increase in bias. Note: the search for a split does not stop until at least one valid partition of the node samples is found, even if it requires to effectively inspect more than ``max_features`` features. alpha : float, default=0.9 The alpha-quantile of the huber loss function and the quantile loss function. Only if loss='huber' or loss='quantile' . verbose : int, default=0 Enable verbose output. If 1 then it prints progress and performance once in a while (the more trees the lower the frequency). If greater than 1 then it prints progress and performance for every tree. max_leaf_nodes : int, default=None Grow trees with max_leaf_nodes in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes. warm_start : bool, default=False When set to True , reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just erase the previous solution. See :term: the Glossary <warm_start> . validation_fraction : float, default=0.1 The proportion of training data to set aside as validation set for early stopping. Must be between 0 and 1. Only used if n_iter_no_change is set to an integer. .. versionadded:: 0.20 n_iter_no_change : int, default=None n_iter_no_change is used to decide if early stopping will be used to terminate training when validation score is not improving. By default it is set to None to disable early stopping. If set to a number, it will set aside validation_fraction size of the training data as validation and terminate training when validation score is not improving in all of the previous n_iter_no_change numbers of iterations. .. versionadded:: 0.20 tol : float, default=1e-4 Tolerance for the early stopping. When the loss is not improving by at least tol for n_iter_no_change iterations (if set to a number), the training stops. .. versionadded:: 0.20 ccp_alpha : non-negative float, default=0.0 Complexity parameter used for Minimal Cost-Complexity Pruning. The subtree with the largest cost complexity that is smaller than ccp_alpha will be chosen. By default, no pruning is performed. See :ref: minimal_cost_complexity_pruning for details. .. versionadded:: 0.22 Attributes \u00b6 feature_importances_ : ndarray of shape (n_features,) The impurity-based feature importances. The higher, the more important the feature. The importance of a feature is computed as the (normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance. Warning: impurity-based feature importances can be misleading for high cardinality features (many unique values). See :func:`sklearn.inspection.permutation_importance` as an alternative. oob_improvement_ : ndarray of shape (n_estimators,) The improvement in loss (= deviance) on the out-of-bag samples relative to the previous iteration. oob_improvement_[0] is the improvement in loss of the first stage over the init estimator. Only available if subsample < 1.0 train_score_ : ndarray of shape (n_estimators,) The i-th score train_score_[i] is the deviance (= loss) of the model at iteration i on the in-bag sample. If subsample == 1 this is the deviance on the training data. loss_ : LossFunction The concrete LossFunction object. init_ : estimator The estimator that provides the initial predictions. Set via the init argument or loss.init_estimator . estimators_ : ndarray of DecisionTreeRegressor of shape (n_estimators, 1) The collection of fitted sub-estimators. n_classes_ : int The number of classes, set to 1 for regressors. .. deprecated:: 0.24 Attribute ``n_classes_`` was deprecated in version 0.24 and will be removed in 1.1 (renaming of 0.26). n_estimators_ : int The number of estimators as selected by early stopping (if n_iter_no_change is specified). Otherwise it is set to n_estimators . n_features_ : int The number of data features. .. deprecated:: 1.0 Attribute `n_features_` was deprecated in version 1.0 and will be removed in 1.2. Use `n_features_in_` instead. n_features_in_ : int Number of features seen during :term: fit . .. versionadded:: 0.24 feature_names_in_ : ndarray of shape ( n_features_in_ ,) Names of features seen during :term: fit . Defined only when X has feature names that are all strings. .. versionadded:: 1.0 max_features_ : int The inferred value of max_features. See Also \u00b6 HistGradientBoostingRegressor : Histogram-based Gradient Boosting Classification Tree. sklearn.tree.DecisionTreeRegressor : A decision tree regressor. sklearn.ensemble.RandomForestRegressor : A random forest regressor. Notes \u00b6 The features are always randomly permuted at each split. Therefore, the best found split may vary, even with the same training data and max_features=n_features , if the improvement of the criterion is identical for several splits enumerated during the search of the best split. To obtain a deterministic behaviour during fitting, random_state has to be fixed. References \u00b6 J. Friedman, Greedy Function Approximation: A Gradient Boosting Machine, The Annals of Statistics, Vol. 29, No. 5, 2001. J. Friedman, Stochastic Gradient Boosting, 1999 T. Hastie, R. Tibshirani and J. Friedman. Elements of Statistical Learning Ed. 2, Springer, 2009. Examples \u00b6 from sklearn.datasets import make_regression from sklearn.ensemble import GradientBoostingRegressor from sklearn.model_selection import train_test_split X, y = make_regression(random_state=0) X_train, X_test, y_train, y_test = train_test_split( ... X, y, random_state=0) reg = GradientBoostingRegressor(random_state=0) reg.fit(X_train, y_train) GradientBoostingRegressor(random_state=0) reg.predict(X_test[1:2]) array([-61...]) reg.score(X_test, y_test) 0.4... n_classes_ : None property readonly \u00b6 DEPRECATED: Attribute n_classes_ was deprecated in version 0.24 and will be removed in 1.1 (renaming of 0.26). apply ( self , X ) \u00b6 Apply trees in the ensemble to X, return leaf indices. .. versionadded:: 0.17 Parameters \u00b6 X : {array-like, sparse matrix} of shape (n_samples, n_features) The input samples. Internally, its dtype will be converted to dtype=np.float32 . If a sparse matrix is provided, it will be converted to a sparse csr_matrix . Returns \u00b6 X_leaves : array-like of shape (n_samples, n_estimators) For each datapoint x in X and for each tree in the ensemble, return the index of the leaf x ends up in each estimator. Source code in giants/config.py def apply ( self , X ): \"\"\"Apply trees in the ensemble to X, return leaf indices. .. versionadded:: 0.17 Parameters ---------- X : {array-like, sparse matrix} of shape (n_samples, n_features) The input samples. Internally, its dtype will be converted to ``dtype=np.float32``. If a sparse matrix is provided, it will be converted to a sparse ``csr_matrix``. Returns ------- X_leaves : array-like of shape (n_samples, n_estimators) For each datapoint x in X and for each tree in the ensemble, return the index of the leaf x ends up in each estimator. \"\"\" leaves = super () . apply ( X ) leaves = leaves . reshape ( X . shape [ 0 ], self . estimators_ . shape [ 0 ]) return leaves predict ( self , X ) \u00b6 Predict regression target for X. Parameters \u00b6 X : {array-like, sparse matrix} of shape (n_samples, n_features) The input samples. Internally, it will be converted to dtype=np.float32 and if a sparse matrix is provided to a sparse csr_matrix . Returns \u00b6 y : ndarray of shape (n_samples,) The predicted values. Source code in giants/config.py def predict ( self , X ): \"\"\"Predict regression target for X. Parameters ---------- X : {array-like, sparse matrix} of shape (n_samples, n_features) The input samples. Internally, it will be converted to ``dtype=np.float32`` and if a sparse matrix is provided to a sparse ``csr_matrix``. Returns ------- y : ndarray of shape (n_samples,) The predicted values. \"\"\" X = self . _validate_data ( X , dtype = DTYPE , order = \"C\" , accept_sparse = \"csr\" , reset = False ) # In regression we can directly return the raw value from the trees. return self . _raw_predict ( X ) . ravel () staged_predict ( self , X ) \u00b6 Predict regression target at each stage for X. This method allows monitoring (i.e. determine error on testing set) after each stage. Parameters \u00b6 X : {array-like, sparse matrix} of shape (n_samples, n_features) The input samples. Internally, it will be converted to dtype=np.float32 and if a sparse matrix is provided to a sparse csr_matrix . Yields \u00b6 y : generator of ndarray of shape (n_samples,) The predicted value of the input samples. Source code in giants/config.py def staged_predict ( self , X ): \"\"\"Predict regression target at each stage for X. This method allows monitoring (i.e. determine error on testing set) after each stage. Parameters ---------- X : {array-like, sparse matrix} of shape (n_samples, n_features) The input samples. Internally, it will be converted to ``dtype=np.float32`` and if a sparse matrix is provided to a sparse ``csr_matrix``. Yields ------ y : generator of ndarray of shape (n_samples,) The predicted value of the input samples. \"\"\" for raw_predictions in self . _staged_raw_predict ( X ): yield raw_predictions . ravel () LinearRegression ( MultiOutputMixin , RegressorMixin , LinearModel ) \u00b6 Ordinary least squares Linear Regression. LinearRegression fits a linear model with coefficients w = (w1, ..., wp) to minimize the residual sum of squares between the observed targets in the dataset, and the targets predicted by the linear approximation. Parameters \u00b6 fit_intercept : bool, default=True Whether to calculate the intercept for this model. If set to False, no intercept will be used in calculations (i.e. data is expected to be centered). normalize : bool, default=False This parameter is ignored when fit_intercept is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm. If you wish to standardize, please use :class: ~sklearn.preprocessing.StandardScaler before calling fit on an estimator with normalize=False . .. deprecated:: 1.0 `normalize` was deprecated in version 1.0 and will be removed in 1.2. copy_X : bool, default=True If True, X will be copied; else, it may be overwritten. n_jobs : int, default=None The number of jobs to use for the computation. This will only provide speedup in case of sufficiently large problems, that is if firstly n_targets > 1 and secondly X is sparse or if positive is set to True . None means 1 unless in a :obj: joblib.parallel_backend context. -1 means using all processors. See :term: Glossary <n_jobs> for more details. positive : bool, default=False When set to True , forces the coefficients to be positive. This option is only supported for dense arrays. .. versionadded:: 0.24 Attributes \u00b6 coef_ : array of shape (n_features, ) or (n_targets, n_features) Estimated coefficients for the linear regression problem. If multiple targets are passed during the fit (y 2D), this is a 2D array of shape (n_targets, n_features), while if only one target is passed, this is a 1D array of length n_features. rank_ : int Rank of matrix X . Only available when X is dense. singular_ : array of shape (min(X, y),) Singular values of X . Only available when X is dense. intercept_ : float or array of shape (n_targets,) Independent term in the linear model. Set to 0.0 if fit_intercept = False . n_features_in_ : int Number of features seen during :term: fit . .. versionadded:: 0.24 feature_names_in_ : ndarray of shape ( n_features_in_ ,) Names of features seen during :term: fit . Defined only when X has feature names that are all strings. .. versionadded:: 1.0 See Also \u00b6 Ridge : Ridge regression addresses some of the problems of Ordinary Least Squares by imposing a penalty on the size of the coefficients with l2 regularization. Lasso : The Lasso is a linear model that estimates sparse coefficients with l1 regularization. ElasticNet : Elastic-Net is a linear regression model trained with both l1 and l2 -norm regularization of the coefficients. Notes \u00b6 From the implementation point of view, this is just plain Ordinary Least Squares (scipy.linalg.lstsq) or Non Negative Least Squares (scipy.optimize.nnls) wrapped as a predictor object. Examples \u00b6 import numpy as np from sklearn.linear_model import LinearRegression X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]]) y = 1 * x_0 + 2 * x_1 + 3 \u00b6 y = np.dot(X, np.array([1, 2])) + 3 reg = LinearRegression().fit(X, y) reg.score(X, y) 1.0 reg.coef_ array([1., 2.]) reg.intercept_ 3.0... reg.predict(np.array([[3, 5]])) array([16.]) fit ( self , X , y , sample_weight = None ) \u00b6 Fit linear model. Parameters \u00b6 X : {array-like, sparse matrix} of shape (n_samples, n_features) Training data. y : array-like of shape (n_samples,) or (n_samples, n_targets) Target values. Will be cast to X's dtype if necessary. sample_weight : array-like of shape (n_samples,), default=None Individual weights for each sample. .. versionadded:: 0.17 parameter *sample_weight* support to LinearRegression. Returns \u00b6 self : object Fitted Estimator. Source code in giants/config.py def fit ( self , X , y , sample_weight = None ): \"\"\" Fit linear model. Parameters ---------- X : {array-like, sparse matrix} of shape (n_samples, n_features) Training data. y : array-like of shape (n_samples,) or (n_samples, n_targets) Target values. Will be cast to X's dtype if necessary. sample_weight : array-like of shape (n_samples,), default=None Individual weights for each sample. .. versionadded:: 0.17 parameter *sample_weight* support to LinearRegression. Returns ------- self : object Fitted Estimator. \"\"\" _normalize = _deprecate_normalize ( self . normalize , default = False , estimator_name = self . __class__ . __name__ ) n_jobs_ = self . n_jobs accept_sparse = False if self . positive else [ \"csr\" , \"csc\" , \"coo\" ] X , y = self . _validate_data ( X , y , accept_sparse = accept_sparse , y_numeric = True , multi_output = True ) if sample_weight is not None : sample_weight = _check_sample_weight ( sample_weight , X , dtype = X . dtype ) X , y , X_offset , y_offset , X_scale = self . _preprocess_data ( X , y , fit_intercept = self . fit_intercept , normalize = _normalize , copy = self . copy_X , sample_weight = sample_weight , return_mean = True , ) if sample_weight is not None : # Sample weight can be implemented via a simple rescaling. X , y = _rescale_data ( X , y , sample_weight ) if self . positive : if y . ndim < 2 : self . coef_ , self . _residues = optimize . nnls ( X , y ) else : # scipy.optimize.nnls cannot handle y with shape (M, K) outs = Parallel ( n_jobs = n_jobs_ )( delayed ( optimize . nnls )( X , y [:, j ]) for j in range ( y . shape [ 1 ]) ) self . coef_ , self . _residues = map ( np . vstack , zip ( * outs )) elif sp . issparse ( X ): X_offset_scale = X_offset / X_scale def matvec ( b ): return X . dot ( b ) - b . dot ( X_offset_scale ) def rmatvec ( b ): return X . T . dot ( b ) - X_offset_scale * np . sum ( b ) X_centered = sparse . linalg . LinearOperator ( shape = X . shape , matvec = matvec , rmatvec = rmatvec ) if y . ndim < 2 : out = sparse_lsqr ( X_centered , y ) self . coef_ = out [ 0 ] self . _residues = out [ 3 ] else : # sparse_lstsq cannot handle y with shape (M, K) outs = Parallel ( n_jobs = n_jobs_ )( delayed ( sparse_lsqr )( X_centered , y [:, j ] . ravel ()) for j in range ( y . shape [ 1 ]) ) self . coef_ = np . vstack ([ out [ 0 ] for out in outs ]) self . _residues = np . vstack ([ out [ 3 ] for out in outs ]) else : self . coef_ , self . _residues , self . rank_ , self . singular_ = linalg . lstsq ( X , y ) self . coef_ = self . coef_ . T if y . ndim == 1 : self . coef_ = np . ravel ( self . coef_ ) self . _set_intercept ( X_offset , y_offset , X_scale ) return self LinearSVC ( LinearClassifierMixin , SparseCoefMixin , BaseEstimator ) \u00b6 Linear Support Vector Classification. Similar to SVC with parameter kernel='linear', but implemented in terms of liblinear rather than libsvm, so it has more flexibility in the choice of penalties and loss functions and should scale better to large numbers of samples. This class supports both dense and sparse input and the multiclass support is handled according to a one-vs-the-rest scheme. Read more in the :ref: User Guide <svm_classification> . Parameters \u00b6 penalty : {'l1', 'l2'}, default='l2' Specifies the norm used in the penalization. The 'l2' penalty is the standard used in SVC. The 'l1' leads to coef_ vectors that are sparse. loss : {'hinge', 'squared_hinge'}, default='squared_hinge' Specifies the loss function. 'hinge' is the standard SVM loss (used e.g. by the SVC class) while 'squared_hinge' is the square of the hinge loss. The combination of penalty='l1' and loss='hinge' is not supported. dual : bool, default=True Select the algorithm to either solve the dual or primal optimization problem. Prefer dual=False when n_samples > n_features. tol : float, default=1e-4 Tolerance for stopping criteria. C : float, default=1.0 Regularization parameter. The strength of the regularization is inversely proportional to C. Must be strictly positive. multi_class : {'ovr', 'crammer_singer'}, default='ovr' Determines the multi-class strategy if y contains more than two classes. \"ovr\" trains n_classes one-vs-rest classifiers, while \"crammer_singer\" optimizes a joint objective over all classes. While crammer_singer is interesting from a theoretical perspective as it is consistent, it is seldom used in practice as it rarely leads to better accuracy and is more expensive to compute. If \"crammer_singer\" is chosen, the options loss, penalty and dual will be ignored. fit_intercept : bool, default=True Whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (i.e. data is expected to be already centered). intercept_scaling : float, default=1 When self.fit_intercept is True, instance vector x becomes [x, self.intercept_scaling] , i.e. a \"synthetic\" feature with constant value equals to intercept_scaling is appended to the instance vector. The intercept becomes intercept_scaling * synthetic feature weight Note! the synthetic feature weight is subject to l1/l2 regularization as all other features. To lessen the effect of regularization on synthetic feature weight (and therefore on the intercept) intercept_scaling has to be increased. class_weight : dict or 'balanced', default=None Set the parameter C of class i to class_weight[i]*C for SVC. If not given, all classes are supposed to have weight one. The \"balanced\" mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as n_samples / (n_classes * np.bincount(y)) . verbose : int, default=0 Enable verbose output. Note that this setting takes advantage of a per-process runtime setting in liblinear that, if enabled, may not work properly in a multithreaded context. random_state : int, RandomState instance or None, default=None Controls the pseudo random number generation for shuffling the data for the dual coordinate descent (if dual=True ). When dual=False the underlying implementation of :class: LinearSVC is not random and random_state has no effect on the results. Pass an int for reproducible output across multiple function calls. See :term: Glossary <random_state> . max_iter : int, default=1000 The maximum number of iterations to be run. Attributes \u00b6 coef_ : ndarray of shape (1, n_features) if n_classes == 2 else (n_classes, n_features) Weights assigned to the features (coefficients in the primal problem). ``coef_`` is a readonly property derived from ``raw_coef_`` that follows the internal memory layout of liblinear. intercept_ : ndarray of shape (1,) if n_classes == 2 else (n_classes,) Constants in decision function. classes_ : ndarray of shape (n_classes,) The unique classes labels. n_features_in_ : int Number of features seen during :term: fit . .. versionadded:: 0.24 feature_names_in_ : ndarray of shape ( n_features_in_ ,) Names of features seen during :term: fit . Defined only when X has feature names that are all strings. .. versionadded:: 1.0 n_iter_ : int Maximum number of iterations run across all classes. See Also \u00b6 SVC : Implementation of Support Vector Machine classifier using libsvm: the kernel can be non-linear but its SMO algorithm does not scale to large number of samples as LinearSVC does. Furthermore SVC multi-class mode is implemented using one vs one scheme while LinearSVC uses one vs the rest. It is possible to implement one vs the rest with SVC by using the :class:`~sklearn.multiclass.OneVsRestClassifier` wrapper. Finally SVC can fit dense data without memory copy if the input is C-contiguous. Sparse data will still incur memory copy though. sklearn.linear_model.SGDClassifier : SGDClassifier can optimize the same cost function as LinearSVC by adjusting the penalty and loss parameters. In addition it requires less memory, allows incremental (online) learning, and implements various loss functions and regularization regimes. Notes \u00b6 The underlying C implementation uses a random number generator to select features when fitting the model. It is thus not uncommon to have slightly different results for the same input data. If that happens, try with a smaller tol parameter. The underlying implementation, liblinear, uses a sparse internal representation for the data that will incur a memory copy. Predict output may not match that of standalone liblinear in certain cases. See :ref: differences from liblinear <liblinear_differences> in the narrative documentation. References \u00b6 LIBLINEAR: A Library for Large Linear Classification <https://www.csie.ntu.edu.tw/~cjlin/liblinear/> __ Examples \u00b6 from sklearn.svm import LinearSVC from sklearn.pipeline import make_pipeline from sklearn.preprocessing import StandardScaler from sklearn.datasets import make_classification X, y = make_classification(n_features=4, random_state=0) clf = make_pipeline(StandardScaler(), ... LinearSVC(random_state=0, tol=1e-5)) clf.fit(X, y) Pipeline(steps=[('standardscaler', StandardScaler()), ('linearsvc', LinearSVC(random_state=0, tol=1e-05))]) print(clf.named_steps['linearsvc'].coef_) [[0.141... 0.526... 0.679... 0.493...]] print(clf.named_steps['linearsvc'].intercept_) [0.1693...] print(clf.predict([[0, 0, 0, 0]])) [1] fit ( self , X , y , sample_weight = None ) \u00b6 Fit the model according to the given training data. Parameters \u00b6 X : {array-like, sparse matrix} of shape (n_samples, n_features) Training vector, where n_samples is the number of samples and n_features is the number of features. y : array-like of shape (n_samples,) Target vector relative to X. sample_weight : array-like of shape (n_samples,), default=None Array of weights that are assigned to individual samples. If not provided, then each sample is given unit weight. .. versionadded:: 0.18 Returns \u00b6 self : object An instance of the estimator. Source code in giants/config.py def fit ( self , X , y , sample_weight = None ): \"\"\"Fit the model according to the given training data. Parameters ---------- X : {array-like, sparse matrix} of shape (n_samples, n_features) Training vector, where `n_samples` is the number of samples and `n_features` is the number of features. y : array-like of shape (n_samples,) Target vector relative to X. sample_weight : array-like of shape (n_samples,), default=None Array of weights that are assigned to individual samples. If not provided, then each sample is given unit weight. .. versionadded:: 0.18 Returns ------- self : object An instance of the estimator. \"\"\" if self . C < 0 : raise ValueError ( \"Penalty term must be positive; got (C= %r )\" % self . C ) X , y = self . _validate_data ( X , y , accept_sparse = \"csr\" , dtype = np . float64 , order = \"C\" , accept_large_sparse = False , ) check_classification_targets ( y ) self . classes_ = np . unique ( y ) self . coef_ , self . intercept_ , self . n_iter_ = _fit_liblinear ( X , y , self . C , self . fit_intercept , self . intercept_scaling , self . class_weight , self . penalty , self . dual , self . verbose , self . max_iter , self . tol , self . random_state , self . multi_class , self . loss , sample_weight = sample_weight , ) if self . multi_class == \"crammer_singer\" and len ( self . classes_ ) == 2 : self . coef_ = ( self . coef_ [ 1 ] - self . coef_ [ 0 ]) . reshape ( 1 , - 1 ) if self . fit_intercept : intercept = self . intercept_ [ 1 ] - self . intercept_ [ 0 ] self . intercept_ = np . array ([ intercept ]) return self LinearSVR ( RegressorMixin , LinearModel ) \u00b6 Linear Support Vector Regression. Similar to SVR with parameter kernel='linear', but implemented in terms of liblinear rather than libsvm, so it has more flexibility in the choice of penalties and loss functions and should scale better to large numbers of samples. This class supports both dense and sparse input. Read more in the :ref: User Guide <svm_regression> . .. versionadded:: 0.16 Parameters \u00b6 epsilon : float, default=0.0 Epsilon parameter in the epsilon-insensitive loss function. Note that the value of this parameter depends on the scale of the target variable y. If unsure, set epsilon=0 . tol : float, default=1e-4 Tolerance for stopping criteria. C : float, default=1.0 Regularization parameter. The strength of the regularization is inversely proportional to C. Must be strictly positive. loss : {'epsilon_insensitive', 'squared_epsilon_insensitive'}, default='epsilon_insensitive' Specifies the loss function. The epsilon-insensitive loss (standard SVR) is the L1 loss, while the squared epsilon-insensitive loss ('squared_epsilon_insensitive') is the L2 loss. fit_intercept : bool, default=True Whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (i.e. data is expected to be already centered). intercept_scaling : float, default=1.0 When self.fit_intercept is True, instance vector x becomes [x, self.intercept_scaling], i.e. a \"synthetic\" feature with constant value equals to intercept_scaling is appended to the instance vector. The intercept becomes intercept_scaling * synthetic feature weight Note! the synthetic feature weight is subject to l1/l2 regularization as all other features. To lessen the effect of regularization on synthetic feature weight (and therefore on the intercept) intercept_scaling has to be increased. dual : bool, default=True Select the algorithm to either solve the dual or primal optimization problem. Prefer dual=False when n_samples > n_features. verbose : int, default=0 Enable verbose output. Note that this setting takes advantage of a per-process runtime setting in liblinear that, if enabled, may not work properly in a multithreaded context. random_state : int, RandomState instance or None, default=None Controls the pseudo random number generation for shuffling the data. Pass an int for reproducible output across multiple function calls. See :term: Glossary <random_state> . max_iter : int, default=1000 The maximum number of iterations to be run. Attributes \u00b6 coef_ : ndarray of shape (n_features) if n_classes == 2 else (n_classes, n_features) Weights assigned to the features (coefficients in the primal problem). `coef_` is a readonly property derived from `raw_coef_` that follows the internal memory layout of liblinear. intercept_ : ndarray of shape (1) if n_classes == 2 else (n_classes) Constants in decision function. n_features_in_ : int Number of features seen during :term: fit . .. versionadded:: 0.24 feature_names_in_ : ndarray of shape ( n_features_in_ ,) Names of features seen during :term: fit . Defined only when X has feature names that are all strings. .. versionadded:: 1.0 n_iter_ : int Maximum number of iterations run across all classes. See Also \u00b6 LinearSVC : Implementation of Support Vector Machine classifier using the same library as this class (liblinear). SVR : Implementation of Support Vector Machine regression using libsvm: the kernel can be non-linear but its SMO algorithm does not scale to large number of samples as LinearSVC does. sklearn.linear_model.SGDRegressor : SGDRegressor can optimize the same cost function as LinearSVR by adjusting the penalty and loss parameters. In addition it requires less memory, allows incremental (online) learning, and implements various loss functions and regularization regimes. Examples \u00b6 from sklearn.svm import LinearSVR from sklearn.pipeline import make_pipeline from sklearn.preprocessing import StandardScaler from sklearn.datasets import make_regression X, y = make_regression(n_features=4, random_state=0) regr = make_pipeline(StandardScaler(), ... LinearSVR(random_state=0, tol=1e-5)) regr.fit(X, y) Pipeline(steps=[('standardscaler', StandardScaler()), ('linearsvr', LinearSVR(random_state=0, tol=1e-05))]) print(regr.named_steps['linearsvr'].coef_) [18.582... 27.023... 44.357... 64.522...] print(regr.named_steps['linearsvr'].intercept_) [-4...] print(regr.predict([[0, 0, 0, 0]])) [-2.384...] fit ( self , X , y , sample_weight = None ) \u00b6 Fit the model according to the given training data. Parameters \u00b6 X : {array-like, sparse matrix} of shape (n_samples, n_features) Training vector, where n_samples is the number of samples and n_features is the number of features. y : array-like of shape (n_samples,) Target vector relative to X. sample_weight : array-like of shape (n_samples,), default=None Array of weights that are assigned to individual samples. If not provided, then each sample is given unit weight. .. versionadded:: 0.18 Returns \u00b6 self : object An instance of the estimator. Source code in giants/config.py def fit ( self , X , y , sample_weight = None ): \"\"\"Fit the model according to the given training data. Parameters ---------- X : {array-like, sparse matrix} of shape (n_samples, n_features) Training vector, where `n_samples` is the number of samples and `n_features` is the number of features. y : array-like of shape (n_samples,) Target vector relative to X. sample_weight : array-like of shape (n_samples,), default=None Array of weights that are assigned to individual samples. If not provided, then each sample is given unit weight. .. versionadded:: 0.18 Returns ------- self : object An instance of the estimator. \"\"\" if self . C < 0 : raise ValueError ( \"Penalty term must be positive; got (C= %r )\" % self . C ) X , y = self . _validate_data ( X , y , accept_sparse = \"csr\" , dtype = np . float64 , order = \"C\" , accept_large_sparse = False , ) penalty = \"l2\" # SVR only accepts l2 penalty self . coef_ , self . intercept_ , self . n_iter_ = _fit_liblinear ( X , y , self . C , self . fit_intercept , self . intercept_scaling , None , penalty , self . dual , self . verbose , self . max_iter , self . tol , self . random_state , loss = self . loss , epsilon = self . epsilon , sample_weight = sample_weight , ) self . coef_ = self . coef_ . ravel () return self LogisticRegression ( LinearClassifierMixin , SparseCoefMixin , BaseEstimator ) \u00b6 Logistic Regression (aka logit, MaxEnt) classifier. In the multiclass case, the training algorithm uses the one-vs-rest (OvR) scheme if the 'multi_class' option is set to 'ovr', and uses the cross-entropy loss if the 'multi_class' option is set to 'multinomial'. (Currently the 'multinomial' option is supported only by the 'lbfgs', 'sag', 'saga' and 'newton-cg' solvers.) This class implements regularized logistic regression using the 'liblinear' library, 'newton-cg', 'sag', 'saga' and 'lbfgs' solvers. Note that regularization is applied by default . It can handle both dense and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit floats for optimal performance; any other input format will be converted (and copied). The 'newton-cg', 'sag', and 'lbfgs' solvers support only L2 regularization with primal formulation, or no regularization. The 'liblinear' solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty. The Elastic-Net regularization is only supported by the 'saga' solver. Read more in the :ref: User Guide <logistic_regression> . Parameters \u00b6 penalty : {'l1', 'l2', 'elasticnet', 'none'}, default='l2' Specify the norm of the penalty: - `'none'`: no penalty is added; - `'l2'`: add a L2 penalty term and it is the default choice; - `'l1'`: add a L1 penalty term; - `'elasticnet'`: both L1 and L2 penalty terms are added. .. warning:: Some penalties may not work with some solvers. See the parameter `solver` below, to know the compatibility between the penalty and solver. .. versionadded:: 0.19 l1 penalty with SAGA solver (allowing 'multinomial' + L1) dual : bool, default=False Dual or primal formulation. Dual formulation is only implemented for l2 penalty with liblinear solver. Prefer dual=False when n_samples > n_features. tol : float, default=1e-4 Tolerance for stopping criteria. C : float, default=1.0 Inverse of regularization strength; must be a positive float. Like in support vector machines, smaller values specify stronger regularization. fit_intercept : bool, default=True Specifies if a constant (a.k.a. bias or intercept) should be added to the decision function. intercept_scaling : float, default=1 Useful only when the solver 'liblinear' is used and self.fit_intercept is set to True. In this case, x becomes [x, self.intercept_scaling], i.e. a \"synthetic\" feature with constant value equal to intercept_scaling is appended to the instance vector. The intercept becomes intercept_scaling * synthetic_feature_weight . Note! the synthetic feature weight is subject to l1/l2 regularization as all other features. To lessen the effect of regularization on synthetic feature weight (and therefore on the intercept) intercept_scaling has to be increased. class_weight : dict or 'balanced', default=None Weights associated with classes in the form {class_label: weight} . If not given, all classes are supposed to have weight one. The \"balanced\" mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as ``n_samples / (n_classes * np.bincount(y))``. Note that these weights will be multiplied with sample_weight (passed through the fit method) if sample_weight is specified. .. versionadded:: 0.17 *class_weight='balanced'* random_state : int, RandomState instance, default=None Used when solver == 'sag', 'saga' or 'liblinear' to shuffle the data. See :term: Glossary <random_state> for details. solver : {'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'}, default='lbfgs' Algorithm to use in the optimization problem. Default is 'lbfgs'. To choose a solver, you might want to consider the following aspects: - For small datasets, 'liblinear' is a good choice, whereas 'sag' and 'saga' are faster for large ones; - For multiclass problems, only 'newton-cg', 'sag', 'saga' and 'lbfgs' handle multinomial loss; - 'liblinear' is limited to one-versus-rest schemes. .. warning:: The choice of the algorithm depends on the penalty chosen: Supported penalties by solver: - 'newton-cg' - ['l2', 'none'] - 'lbfgs' - ['l2', 'none'] - 'liblinear' - ['l1', 'l2'] - 'sag' - ['l2', 'none'] - 'saga' - ['elasticnet', 'l1', 'l2', 'none'] .. note:: 'sag' and 'saga' fast convergence is only guaranteed on features with approximately the same scale. You can preprocess the data with a scaler from :mod:`sklearn.preprocessing`. .. seealso:: Refer to the User Guide for more information regarding :class:`LogisticRegression` and more specifically the `Table <https://scikit-learn.org/dev/modules/linear_model.html#logistic-regression>`_ summarazing solver/penalty supports. <!-- # noqa: E501 --> .. versionadded:: 0.17 Stochastic Average Gradient descent solver. .. versionadded:: 0.19 SAGA solver. .. versionchanged:: 0.22 The default solver changed from 'liblinear' to 'lbfgs' in 0.22. max_iter : int, default=100 Maximum number of iterations taken for the solvers to converge. multi_class : {'auto', 'ovr', 'multinomial'}, default='auto' If the option chosen is 'ovr', then a binary problem is fit for each label. For 'multinomial' the loss minimised is the multinomial loss fit across the entire probability distribution, even when the data is binary . 'multinomial' is unavailable when solver='liblinear'. 'auto' selects 'ovr' if the data is binary, or if solver='liblinear', and otherwise selects 'multinomial'. .. versionadded:: 0.18 Stochastic Average Gradient descent solver for 'multinomial' case. .. versionchanged:: 0.22 Default changed from 'ovr' to 'auto' in 0.22. verbose : int, default=0 For the liblinear and lbfgs solvers set verbose to any positive number for verbosity. warm_start : bool, default=False When set to True, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. Useless for liblinear solver. See :term: the Glossary <warm_start> . .. versionadded:: 0.17 *warm_start* to support *lbfgs*, *newton-cg*, *sag*, *saga* solvers. n_jobs : int, default=None Number of CPU cores used when parallelizing over classes if multi_class='ovr'\". This parameter is ignored when the solver is set to 'liblinear' regardless of whether 'multi_class' is specified or not. None means 1 unless in a :obj: joblib.parallel_backend context. -1 means using all processors. See :term: Glossary <n_jobs> for more details. l1_ratio : float, default=None The Elastic-Net mixing parameter, with 0 <= l1_ratio <= 1 . Only used if penalty='elasticnet' . Setting l1_ratio=0 is equivalent to using penalty='l2' , while setting l1_ratio=1 is equivalent to using penalty='l1' . For 0 < l1_ratio <1 , the penalty is a combination of L1 and L2. Attributes \u00b6 classes_ : ndarray of shape (n_classes, ) A list of class labels known to the classifier. coef_ : ndarray of shape (1, n_features) or (n_classes, n_features) Coefficient of the features in the decision function. `coef_` is of shape (1, n_features) when the given problem is binary. In particular, when `multi_class='multinomial'`, `coef_` corresponds to outcome 1 (True) and `-coef_` corresponds to outcome 0 (False). intercept_ : ndarray of shape (1,) or (n_classes,) Intercept (a.k.a. bias) added to the decision function. If `fit_intercept` is set to False, the intercept is set to zero. `intercept_` is of shape (1,) when the given problem is binary. In particular, when `multi_class='multinomial'`, `intercept_` corresponds to outcome 1 (True) and `-intercept_` corresponds to outcome 0 (False). n_features_in_ : int Number of features seen during :term: fit . .. versionadded:: 0.24 feature_names_in_ : ndarray of shape ( n_features_in_ ,) Names of features seen during :term: fit . Defined only when X has feature names that are all strings. .. versionadded:: 1.0 n_iter_ : ndarray of shape (n_classes,) or (1, ) Actual number of iterations for all classes. If binary or multinomial, it returns only 1 element. For liblinear solver, only the maximum number of iteration across all classes is given. .. versionchanged:: 0.20 In SciPy <= 1.0.0 the number of lbfgs iterations may exceed ``max_iter``. ``n_iter_`` will now report at most ``max_iter``. See Also \u00b6 SGDClassifier : Incrementally trained logistic regression (when given the parameter loss=\"log\" ). LogisticRegressionCV : Logistic regression with built-in cross validation. Notes \u00b6 The underlying C implementation uses a random number generator to select features when fitting the model. It is thus not uncommon, to have slightly different results for the same input data. If that happens, try with a smaller tol parameter. Predict output may not match that of standalone liblinear in certain cases. See :ref: differences from liblinear <liblinear_differences> in the narrative documentation. References \u00b6 L-BFGS-B -- Software for Large-scale Bound-constrained Optimization Ciyou Zhu, Richard Byrd, Jorge Nocedal and Jose Luis Morales. users.iems.northwestern.edu/~nocedal/lbfgsb.html LIBLINEAR -- A Library for Large Linear Classification www.csie.ntu.edu.tw/~cjlin/liblinear/ SAG -- Mark Schmidt, Nicolas Le Roux, and Francis Bach Minimizing Finite Sums with the Stochastic Average Gradient hal.inria.fr/hal-00860051/document SAGA -- Defazio, A., Bach F. & Lacoste-Julien S. (2014). SAGA: A Fast Incremental Gradient Method With Support for Non-Strongly Convex Composite Objectives arxiv.org/abs/1407.0202 Hsiang-Fu Yu, Fang-Lan Huang, Chih-Jen Lin (2011). Dual coordinate descent methods for logistic regression and maximum entropy models. Machine Learning 85(1-2):41-75. www.csie.ntu.edu.tw/~cjlin/papers/maxent_dual.pdf Examples \u00b6 from sklearn.datasets import load_iris from sklearn.linear_model import LogisticRegression X, y = load_iris(return_X_y=True) clf = LogisticRegression(random_state=0).fit(X, y) clf.predict(X[:2, :]) array([0, 0]) clf.predict_proba(X[:2, :]) array([[9.8...e-01, 1.8...e-02, 1.4...e-08], [9.7...e-01, 2.8...e-02, ...e-08]]) clf.score(X, y) 0.97... fit ( self , X , y , sample_weight = None ) \u00b6 Fit the model according to the given training data. Parameters \u00b6 X : {array-like, sparse matrix} of shape (n_samples, n_features) Training vector, where n_samples is the number of samples and n_features is the number of features. y : array-like of shape (n_samples,) Target vector relative to X. sample_weight : array-like of shape (n_samples,) default=None Array of weights that are assigned to individual samples. If not provided, then each sample is given unit weight. .. versionadded:: 0.17 *sample_weight* support to LogisticRegression. Returns \u00b6 self Fitted estimator. Notes \u00b6 The SAGA solver supports both float64 and float32 bit arrays. Source code in giants/config.py def fit ( self , X , y , sample_weight = None ): \"\"\" Fit the model according to the given training data. Parameters ---------- X : {array-like, sparse matrix} of shape (n_samples, n_features) Training vector, where `n_samples` is the number of samples and `n_features` is the number of features. y : array-like of shape (n_samples,) Target vector relative to X. sample_weight : array-like of shape (n_samples,) default=None Array of weights that are assigned to individual samples. If not provided, then each sample is given unit weight. .. versionadded:: 0.17 *sample_weight* support to LogisticRegression. Returns ------- self Fitted estimator. Notes ----- The SAGA solver supports both float64 and float32 bit arrays. \"\"\" solver = _check_solver ( self . solver , self . penalty , self . dual ) if not isinstance ( self . C , numbers . Number ) or self . C < 0 : raise ValueError ( \"Penalty term must be positive; got (C= %r )\" % self . C ) if self . penalty == \"elasticnet\" : if ( not isinstance ( self . l1_ratio , numbers . Number ) or self . l1_ratio < 0 or self . l1_ratio > 1 ): raise ValueError ( \"l1_ratio must be between 0 and 1; got (l1_ratio= %r )\" % self . l1_ratio ) elif self . l1_ratio is not None : warnings . warn ( \"l1_ratio parameter is only used when penalty is \" \"'elasticnet'. Got \" \"(penalty= {} )\" . format ( self . penalty ) ) if self . penalty == \"none\" : if self . C != 1.0 : # default values warnings . warn ( \"Setting penalty='none' will ignore the C and l1_ratio parameters\" ) # Note that check for l1_ratio is done right above C_ = np . inf penalty = \"l2\" else : C_ = self . C penalty = self . penalty if not isinstance ( self . max_iter , numbers . Number ) or self . max_iter < 0 : raise ValueError ( \"Maximum number of iteration must be positive; got (max_iter= %r )\" % self . max_iter ) if not isinstance ( self . tol , numbers . Number ) or self . tol < 0 : raise ValueError ( \"Tolerance for stopping criteria must be positive; got (tol= %r )\" % self . tol ) if solver == \"lbfgs\" : _dtype = np . float64 else : _dtype = [ np . float64 , np . float32 ] X , y = self . _validate_data ( X , y , accept_sparse = \"csr\" , dtype = _dtype , order = \"C\" , accept_large_sparse = solver not in [ \"liblinear\" , \"sag\" , \"saga\" ], ) check_classification_targets ( y ) self . classes_ = np . unique ( y ) multi_class = _check_multi_class ( self . multi_class , solver , len ( self . classes_ )) if solver == \"liblinear\" : if effective_n_jobs ( self . n_jobs ) != 1 : warnings . warn ( \"'n_jobs' > 1 does not have any effect when\" \" 'solver' is set to 'liblinear'. Got 'n_jobs'\" \" = {} .\" . format ( effective_n_jobs ( self . n_jobs )) ) self . coef_ , self . intercept_ , n_iter_ = _fit_liblinear ( X , y , self . C , self . fit_intercept , self . intercept_scaling , self . class_weight , self . penalty , self . dual , self . verbose , self . max_iter , self . tol , self . random_state , sample_weight = sample_weight , ) self . n_iter_ = np . array ([ n_iter_ ]) return self if solver in [ \"sag\" , \"saga\" ]: max_squared_sum = row_norms ( X , squared = True ) . max () else : max_squared_sum = None n_classes = len ( self . classes_ ) classes_ = self . classes_ if n_classes < 2 : raise ValueError ( \"This solver needs samples of at least 2 classes\" \" in the data, but the data contains only one\" \" class: %r \" % classes_ [ 0 ] ) if len ( self . classes_ ) == 2 : n_classes = 1 classes_ = classes_ [ 1 :] if self . warm_start : warm_start_coef = getattr ( self , \"coef_\" , None ) else : warm_start_coef = None if warm_start_coef is not None and self . fit_intercept : warm_start_coef = np . append ( warm_start_coef , self . intercept_ [:, np . newaxis ], axis = 1 ) # Hack so that we iterate only once for the multinomial case. if multi_class == \"multinomial\" : classes_ = [ None ] warm_start_coef = [ warm_start_coef ] if warm_start_coef is None : warm_start_coef = [ None ] * n_classes path_func = delayed ( _logistic_regression_path ) # The SAG solver releases the GIL so it's more efficient to use # threads for this solver. if solver in [ \"sag\" , \"saga\" ]: prefer = \"threads\" else : prefer = \"processes\" fold_coefs_ = Parallel ( n_jobs = self . n_jobs , verbose = self . verbose , ** _joblib_parallel_args ( prefer = prefer ), )( path_func ( X , y , pos_class = class_ , Cs = [ C_ ], l1_ratio = self . l1_ratio , fit_intercept = self . fit_intercept , tol = self . tol , verbose = self . verbose , solver = solver , multi_class = multi_class , max_iter = self . max_iter , class_weight = self . class_weight , check_input = False , random_state = self . random_state , coef = warm_start_coef_ , penalty = penalty , max_squared_sum = max_squared_sum , sample_weight = sample_weight , ) for class_ , warm_start_coef_ in zip ( classes_ , warm_start_coef ) ) fold_coefs_ , _ , n_iter_ = zip ( * fold_coefs_ ) self . n_iter_ = np . asarray ( n_iter_ , dtype = np . int32 )[:, 0 ] n_features = X . shape [ 1 ] if multi_class == \"multinomial\" : self . coef_ = fold_coefs_ [ 0 ][ 0 ] else : self . coef_ = np . asarray ( fold_coefs_ ) self . coef_ = self . coef_ . reshape ( n_classes , n_features + int ( self . fit_intercept ) ) if self . fit_intercept : self . intercept_ = self . coef_ [:, - 1 ] self . coef_ = self . coef_ [:, : - 1 ] else : self . intercept_ = np . zeros ( n_classes ) return self predict_log_proba ( self , X ) \u00b6 Predict logarithm of probability estimates. The returned estimates for all classes are ordered by the label of classes. Parameters \u00b6 X : array-like of shape (n_samples, n_features) Vector to be scored, where n_samples is the number of samples and n_features is the number of features. Returns \u00b6 T : array-like of shape (n_samples, n_classes) Returns the log-probability of the sample for each class in the model, where classes are ordered as they are in self.classes_ . Source code in giants/config.py def predict_log_proba ( self , X ): \"\"\" Predict logarithm of probability estimates. The returned estimates for all classes are ordered by the label of classes. Parameters ---------- X : array-like of shape (n_samples, n_features) Vector to be scored, where `n_samples` is the number of samples and `n_features` is the number of features. Returns ------- T : array-like of shape (n_samples, n_classes) Returns the log-probability of the sample for each class in the model, where classes are ordered as they are in ``self.classes_``. \"\"\" return np . log ( self . predict_proba ( X )) predict_proba ( self , X ) \u00b6 Probability estimates. The returned estimates for all classes are ordered by the label of classes. For a multi_class problem, if multi_class is set to be \"multinomial\" the softmax function is used to find the predicted probability of each class. Else use a one-vs-rest approach, i.e calculate the probability of each class assuming it to be positive using the logistic function. and normalize these values across all the classes. Parameters \u00b6 X : array-like of shape (n_samples, n_features) Vector to be scored, where n_samples is the number of samples and n_features is the number of features. Returns \u00b6 T : array-like of shape (n_samples, n_classes) Returns the probability of the sample for each class in the model, where classes are ordered as they are in self.classes_ . Source code in giants/config.py def predict_proba ( self , X ): \"\"\" Probability estimates. The returned estimates for all classes are ordered by the label of classes. For a multi_class problem, if multi_class is set to be \"multinomial\" the softmax function is used to find the predicted probability of each class. Else use a one-vs-rest approach, i.e calculate the probability of each class assuming it to be positive using the logistic function. and normalize these values across all the classes. Parameters ---------- X : array-like of shape (n_samples, n_features) Vector to be scored, where `n_samples` is the number of samples and `n_features` is the number of features. Returns ------- T : array-like of shape (n_samples, n_classes) Returns the probability of the sample for each class in the model, where classes are ordered as they are in ``self.classes_``. \"\"\" check_is_fitted ( self ) ovr = self . multi_class in [ \"ovr\" , \"warn\" ] or ( self . multi_class == \"auto\" and ( self . classes_ . size <= 2 or self . solver == \"liblinear\" ) ) if ovr : return super () . _predict_proba_lr ( X ) else : decision = self . decision_function ( X ) if decision . ndim == 1 : # Workaround for multi_class=\"multinomial\" and binary outcomes # which requires softmax prediction with only a 1D decision. decision_2d = np . c_ [ - decision , decision ] else : decision_2d = decision return softmax ( decision_2d , copy = False ) RandomForestClassifier ( ForestClassifier ) \u00b6 A random forest classifier. A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is controlled with the max_samples parameter if bootstrap=True (default), otherwise the whole dataset is used to build each tree. Read more in the :ref: User Guide <forest> . Parameters \u00b6 n_estimators : int, default=100 The number of trees in the forest. .. versionchanged:: 0.22 The default value of ``n_estimators`` changed from 10 to 100 in 0.22. criterion : {\"gini\", \"entropy\"}, default=\"gini\" The function to measure the quality of a split. Supported criteria are \"gini\" for the Gini impurity and \"entropy\" for the information gain. Note: this parameter is tree-specific. max_depth : int, default=None The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples. min_samples_split : int or float, default=2 The minimum number of samples required to split an internal node: - If int, then consider `min_samples_split` as the minimum number. - If float, then `min_samples_split` is a fraction and `ceil(min_samples_split * n_samples)` are the minimum number of samples for each split. .. versionchanged:: 0.18 Added float values for fractions. min_samples_leaf : int or float, default=1 The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least min_samples_leaf training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression. - If int, then consider `min_samples_leaf` as the minimum number. - If float, then `min_samples_leaf` is a fraction and `ceil(min_samples_leaf * n_samples)` are the minimum number of samples for each node. .. versionchanged:: 0.18 Added float values for fractions. min_weight_fraction_leaf : float, default=0.0 The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided. max_features : {\"auto\", \"sqrt\", \"log2\"}, int or float, default=\"auto\" The number of features to consider when looking for the best split: - If int, then consider `max_features` features at each split. - If float, then `max_features` is a fraction and `round(max_features * n_features)` features are considered at each split. - If \"auto\", then `max_features=sqrt(n_features)`. - If \"sqrt\", then `max_features=sqrt(n_features)` (same as \"auto\"). - If \"log2\", then `max_features=log2(n_features)`. - If None, then `max_features=n_features`. Note: the search for a split does not stop until at least one valid partition of the node samples is found, even if it requires to effectively inspect more than ``max_features`` features. max_leaf_nodes : int, default=None Grow trees with max_leaf_nodes in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes. min_impurity_decrease : float, default=0.0 A node will be split if this split induces a decrease of the impurity greater than or equal to this value. The weighted impurity decrease equation is the following:: N_t / N * (impurity - N_t_R / N_t * right_impurity - N_t_L / N_t * left_impurity) where ``N`` is the total number of samples, ``N_t`` is the number of samples at the current node, ``N_t_L`` is the number of samples in the left child, and ``N_t_R`` is the number of samples in the right child. ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum, if ``sample_weight`` is passed. .. versionadded:: 0.19 bootstrap : bool, default=True Whether bootstrap samples are used when building trees. If False, the whole dataset is used to build each tree. oob_score : bool, default=False Whether to use out-of-bag samples to estimate the generalization score. Only available if bootstrap=True. n_jobs : int, default=None The number of jobs to run in parallel. :meth: fit , :meth: predict , :meth: decision_path and :meth: apply are all parallelized over the trees. None means 1 unless in a :obj: joblib.parallel_backend context. -1 means using all processors. See :term: Glossary <n_jobs> for more details. random_state : int, RandomState instance or None, default=None Controls both the randomness of the bootstrapping of the samples used when building trees (if bootstrap=True ) and the sampling of the features to consider when looking for the best split at each node (if max_features < n_features ). See :term: Glossary <random_state> for details. verbose : int, default=0 Controls the verbosity when fitting and predicting. warm_start : bool, default=False When set to True , reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just fit a whole new forest. See :term: the Glossary <warm_start> . class_weight : {\"balanced\", \"balanced_subsample\"}, dict or list of dicts, default=None Weights associated with classes in the form {class_label: weight} . If not given, all classes are supposed to have weight one. For multi-output problems, a list of dicts can be provided in the same order as the columns of y. Note that for multioutput (including multilabel) weights should be defined for each class of every column in its own dict. For example, for four-class multilabel classification weights should be [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of [{1:1}, {2:5}, {3:1}, {4:1}]. The \"balanced\" mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as ``n_samples / (n_classes * np.bincount(y))`` The \"balanced_subsample\" mode is the same as \"balanced\" except that weights are computed based on the bootstrap sample for every tree grown. For multi-output, the weights of each column of y will be multiplied. Note that these weights will be multiplied with sample_weight (passed through the fit method) if sample_weight is specified. ccp_alpha : non-negative float, default=0.0 Complexity parameter used for Minimal Cost-Complexity Pruning. The subtree with the largest cost complexity that is smaller than ccp_alpha will be chosen. By default, no pruning is performed. See :ref: minimal_cost_complexity_pruning for details. .. versionadded:: 0.22 max_samples : int or float, default=None If bootstrap is True, the number of samples to draw from X to train each base estimator. - If None (default), then draw `X.shape[0]` samples. - If int, then draw `max_samples` samples. - If float, then draw `max_samples * X.shape[0]` samples. Thus, `max_samples` should be in the interval `(0.0, 1.0]`. .. versionadded:: 0.22 Attributes \u00b6 base_estimator_ : DecisionTreeClassifier The child estimator template used to create the collection of fitted sub-estimators. estimators_ : list of DecisionTreeClassifier The collection of fitted sub-estimators. classes_ : ndarray of shape (n_classes,) or a list of such arrays The classes labels (single output problem), or a list of arrays of class labels (multi-output problem). n_classes_ : int or list The number of classes (single output problem), or a list containing the number of classes for each output (multi-output problem). n_features_ : int The number of features when fit is performed. .. deprecated:: 1.0 Attribute `n_features_` was deprecated in version 1.0 and will be removed in 1.2. Use `n_features_in_` instead. n_features_in_ : int Number of features seen during :term: fit . .. versionadded:: 0.24 feature_names_in_ : ndarray of shape ( n_features_in_ ,) Names of features seen during :term: fit . Defined only when X has feature names that are all strings. .. versionadded:: 1.0 n_outputs_ : int The number of outputs when fit is performed. feature_importances_ : ndarray of shape (n_features,) The impurity-based feature importances. The higher, the more important the feature. The importance of a feature is computed as the (normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance. Warning: impurity-based feature importances can be misleading for high cardinality features (many unique values). See :func:`sklearn.inspection.permutation_importance` as an alternative. oob_score_ : float Score of the training dataset obtained using an out-of-bag estimate. This attribute exists only when oob_score is True. oob_decision_function_ : ndarray of shape (n_samples, n_classes) or (n_samples, n_classes, n_outputs) Decision function computed with out-of-bag estimate on the training set. If n_estimators is small it might be possible that a data point was never left out during the bootstrap. In this case, oob_decision_function_ might contain NaN. This attribute exists only when oob_score is True. See Also \u00b6 sklearn.tree.DecisionTreeClassifier : A decision tree classifier. sklearn.ensemble.ExtraTreesClassifier : Ensemble of extremely randomized tree classifiers. Notes \u00b6 The default values for the parameters controlling the size of the trees (e.g. max_depth , min_samples_leaf , etc.) lead to fully grown and unpruned trees which can potentially be very large on some data sets. To reduce memory consumption, the complexity and size of the trees should be controlled by setting those parameter values. The features are always randomly permuted at each split. Therefore, the best found split may vary, even with the same training data, max_features=n_features and bootstrap=False , if the improvement of the criterion is identical for several splits enumerated during the search of the best split. To obtain a deterministic behaviour during fitting, random_state has to be fixed. References \u00b6 .. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001. Examples \u00b6 from sklearn.ensemble import RandomForestClassifier from sklearn.datasets import make_classification X, y = make_classification(n_samples=1000, n_features=4, ... n_informative=2, n_redundant=0, ... random_state=0, shuffle=False) clf = RandomForestClassifier(max_depth=2, random_state=0) clf.fit(X, y) RandomForestClassifier(...) print(clf.predict([[0, 0, 0, 0]])) [1] RandomForestRegressor ( ForestRegressor ) \u00b6 A random forest regressor. A random forest is a meta estimator that fits a number of classifying decision trees on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is controlled with the max_samples parameter if bootstrap=True (default), otherwise the whole dataset is used to build each tree. Read more in the :ref: User Guide <forest> . Parameters \u00b6 n_estimators : int, default=100 The number of trees in the forest. .. versionchanged:: 0.22 The default value of ``n_estimators`` changed from 10 to 100 in 0.22. criterion : {\"squared_error\", \"absolute_error\", \"poisson\"}, default=\"squared_error\" The function to measure the quality of a split. Supported criteria are \"squared_error\" for the mean squared error, which is equal to variance reduction as feature selection criterion, \"absolute_error\" for the mean absolute error, and \"poisson\" which uses reduction in Poisson deviance to find splits. Training using \"absolute_error\" is significantly slower than when using \"squared_error\". .. versionadded:: 0.18 Mean Absolute Error (MAE) criterion. .. versionadded:: 1.0 Poisson criterion. .. deprecated:: 1.0 Criterion \"mse\" was deprecated in v1.0 and will be removed in version 1.2. Use `criterion=\"squared_error\"` which is equivalent. .. deprecated:: 1.0 Criterion \"mae\" was deprecated in v1.0 and will be removed in version 1.2. Use `criterion=\"absolute_error\"` which is equivalent. max_depth : int, default=None The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples. min_samples_split : int or float, default=2 The minimum number of samples required to split an internal node: - If int, then consider `min_samples_split` as the minimum number. - If float, then `min_samples_split` is a fraction and `ceil(min_samples_split * n_samples)` are the minimum number of samples for each split. .. versionchanged:: 0.18 Added float values for fractions. min_samples_leaf : int or float, default=1 The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least min_samples_leaf training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression. - If int, then consider `min_samples_leaf` as the minimum number. - If float, then `min_samples_leaf` is a fraction and `ceil(min_samples_leaf * n_samples)` are the minimum number of samples for each node. .. versionchanged:: 0.18 Added float values for fractions. min_weight_fraction_leaf : float, default=0.0 The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided. max_features : {\"auto\", \"sqrt\", \"log2\"}, int or float, default=\"auto\" The number of features to consider when looking for the best split: - If int, then consider `max_features` features at each split. - If float, then `max_features` is a fraction and `round(max_features * n_features)` features are considered at each split. - If \"auto\", then `max_features=n_features`. - If \"sqrt\", then `max_features=sqrt(n_features)`. - If \"log2\", then `max_features=log2(n_features)`. - If None, then `max_features=n_features`. Note: the search for a split does not stop until at least one valid partition of the node samples is found, even if it requires to effectively inspect more than ``max_features`` features. max_leaf_nodes : int, default=None Grow trees with max_leaf_nodes in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes. min_impurity_decrease : float, default=0.0 A node will be split if this split induces a decrease of the impurity greater than or equal to this value. The weighted impurity decrease equation is the following:: N_t / N * (impurity - N_t_R / N_t * right_impurity - N_t_L / N_t * left_impurity) where ``N`` is the total number of samples, ``N_t`` is the number of samples at the current node, ``N_t_L`` is the number of samples in the left child, and ``N_t_R`` is the number of samples in the right child. ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum, if ``sample_weight`` is passed. .. versionadded:: 0.19 bootstrap : bool, default=True Whether bootstrap samples are used when building trees. If False, the whole dataset is used to build each tree. oob_score : bool, default=False Whether to use out-of-bag samples to estimate the generalization score. Only available if bootstrap=True. n_jobs : int, default=None The number of jobs to run in parallel. :meth: fit , :meth: predict , :meth: decision_path and :meth: apply are all parallelized over the trees. None means 1 unless in a :obj: joblib.parallel_backend context. -1 means using all processors. See :term: Glossary <n_jobs> for more details. random_state : int, RandomState instance or None, default=None Controls both the randomness of the bootstrapping of the samples used when building trees (if bootstrap=True ) and the sampling of the features to consider when looking for the best split at each node (if max_features < n_features ). See :term: Glossary <random_state> for details. verbose : int, default=0 Controls the verbosity when fitting and predicting. warm_start : bool, default=False When set to True , reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just fit a whole new forest. See :term: the Glossary <warm_start> . ccp_alpha : non-negative float, default=0.0 Complexity parameter used for Minimal Cost-Complexity Pruning. The subtree with the largest cost complexity that is smaller than ccp_alpha will be chosen. By default, no pruning is performed. See :ref: minimal_cost_complexity_pruning for details. .. versionadded:: 0.22 max_samples : int or float, default=None If bootstrap is True, the number of samples to draw from X to train each base estimator. - If None (default), then draw `X.shape[0]` samples. - If int, then draw `max_samples` samples. - If float, then draw `max_samples * X.shape[0]` samples. Thus, `max_samples` should be in the interval `(0.0, 1.0]`. .. versionadded:: 0.22 Attributes \u00b6 base_estimator_ : DecisionTreeRegressor The child estimator template used to create the collection of fitted sub-estimators. estimators_ : list of DecisionTreeRegressor The collection of fitted sub-estimators. feature_importances_ : ndarray of shape (n_features,) The impurity-based feature importances. The higher, the more important the feature. The importance of a feature is computed as the (normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance. Warning: impurity-based feature importances can be misleading for high cardinality features (many unique values). See :func:`sklearn.inspection.permutation_importance` as an alternative. n_features_ : int The number of features when fit is performed. .. deprecated:: 1.0 Attribute `n_features_` was deprecated in version 1.0 and will be removed in 1.2. Use `n_features_in_` instead. n_features_in_ : int Number of features seen during :term: fit . .. versionadded:: 0.24 feature_names_in_ : ndarray of shape ( n_features_in_ ,) Names of features seen during :term: fit . Defined only when X has feature names that are all strings. .. versionadded:: 1.0 n_outputs_ : int The number of outputs when fit is performed. oob_score_ : float Score of the training dataset obtained using an out-of-bag estimate. This attribute exists only when oob_score is True. oob_prediction_ : ndarray of shape (n_samples,) or (n_samples, n_outputs) Prediction computed with out-of-bag estimate on the training set. This attribute exists only when oob_score is True. See Also \u00b6 sklearn.tree.DecisionTreeRegressor : A decision tree regressor. sklearn.ensemble.ExtraTreesRegressor : Ensemble of extremely randomized tree regressors. Notes \u00b6 The default values for the parameters controlling the size of the trees (e.g. max_depth , min_samples_leaf , etc.) lead to fully grown and unpruned trees which can potentially be very large on some data sets. To reduce memory consumption, the complexity and size of the trees should be controlled by setting those parameter values. The features are always randomly permuted at each split. Therefore, the best found split may vary, even with the same training data, max_features=n_features and bootstrap=False , if the improvement of the criterion is identical for several splits enumerated during the search of the best split. To obtain a deterministic behaviour during fitting, random_state has to be fixed. The default value max_features=\"auto\" uses n_features rather than n_features / 3 . The latter was originally suggested in [1], whereas the former was more recently justified empirically in [2]. References \u00b6 .. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001. .. [2] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized trees\", Machine Learning, 63(1), 3-42, 2006. Examples \u00b6 from sklearn.ensemble import RandomForestRegressor from sklearn.datasets import make_regression X, y = make_regression(n_features=4, n_informative=2, ... random_state=0, shuffle=False) regr = RandomForestRegressor(max_depth=2, random_state=0) regr.fit(X, y) RandomForestRegressor(...) print(regr.predict([[0, 0, 0, 0]])) [-8.32987858] SVC ( BaseSVC ) \u00b6 C-Support Vector Classification. The implementation is based on libsvm. The fit time scales at least quadratically with the number of samples and may be impractical beyond tens of thousands of samples. For large datasets consider using :class: ~sklearn.svm.LinearSVC or :class: ~sklearn.linear_model.SGDClassifier instead, possibly after a :class: ~sklearn.kernel_approximation.Nystroem transformer. The multiclass support is handled according to a one-vs-one scheme. For details on the precise mathematical formulation of the provided kernel functions and how gamma , coef0 and degree affect each other, see the corresponding section in the narrative documentation: :ref: svm_kernels . Read more in the :ref: User Guide <svm_classification> . Parameters \u00b6 C : float, default=1.0 Regularization parameter. The strength of the regularization is inversely proportional to C. Must be strictly positive. The penalty is a squared l2 penalty. kernel : {'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'}, default='rbf' Specifies the kernel type to be used in the algorithm. It must be one of 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed' or a callable. If none is given, 'rbf' will be used. If a callable is given it is used to pre-compute the kernel matrix from data matrices; that matrix should be an array of shape (n_samples, n_samples) . degree : int, default=3 Degree of the polynomial kernel function ('poly'). Ignored by all other kernels. gamma : {'scale', 'auto'} or float, default='scale' Kernel coefficient for 'rbf', 'poly' and 'sigmoid'. - if ``gamma='scale'`` (default) is passed then it uses 1 / (n_features * X.var()) as value of gamma, - if 'auto', uses 1 / n_features. .. versionchanged:: 0.22 The default value of ``gamma`` changed from 'auto' to 'scale'. coef0 : float, default=0.0 Independent term in kernel function. It is only significant in 'poly' and 'sigmoid'. shrinking : bool, default=True Whether to use the shrinking heuristic. See the :ref: User Guide <shrinking_svm> . probability : bool, default=False Whether to enable probability estimates. This must be enabled prior to calling fit , will slow down that method as it internally uses 5-fold cross-validation, and predict_proba may be inconsistent with predict . Read more in the :ref: User Guide <scores_probabilities> . tol : float, default=1e-3 Tolerance for stopping criterion. cache_size : float, default=200 Specify the size of the kernel cache (in MB). class_weight : dict or 'balanced', default=None Set the parameter C of class i to class_weight[i]*C for SVC. If not given, all classes are supposed to have weight one. The \"balanced\" mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as n_samples / (n_classes * np.bincount(y)) . verbose : bool, default=False Enable verbose output. Note that this setting takes advantage of a per-process runtime setting in libsvm that, if enabled, may not work properly in a multithreaded context. max_iter : int, default=-1 Hard limit on iterations within solver, or -1 for no limit. decision_function_shape : {'ovo', 'ovr'}, default='ovr' Whether to return a one-vs-rest ('ovr') decision function of shape (n_samples, n_classes) as all other classifiers, or the original one-vs-one ('ovo') decision function of libsvm which has shape (n_samples, n_classes * (n_classes - 1) / 2). However, one-vs-one ('ovo') is always used as multi-class strategy. The parameter is ignored for binary classification. .. versionchanged:: 0.19 decision_function_shape is 'ovr' by default. .. versionadded:: 0.17 *decision_function_shape='ovr'* is recommended. .. versionchanged:: 0.17 Deprecated *decision_function_shape='ovo' and None*. break_ties : bool, default=False If true, decision_function_shape='ovr' , and number of classes > 2, :term: predict will break ties according to the confidence values of :term: decision_function ; otherwise the first class among the tied classes is returned. Please note that breaking ties comes at a relatively high computational cost compared to a simple predict. .. versionadded:: 0.22 random_state : int, RandomState instance or None, default=None Controls the pseudo random number generation for shuffling the data for probability estimates. Ignored when probability is False. Pass an int for reproducible output across multiple function calls. See :term: Glossary <random_state> . Attributes \u00b6 class_weight_ : ndarray of shape (n_classes,) Multipliers of parameter C for each class. Computed based on the class_weight parameter. classes_ : ndarray of shape (n_classes,) The classes labels. coef_ : ndarray of shape (n_classes * (n_classes - 1) / 2, n_features) Weights assigned to the features (coefficients in the primal problem). This is only available in the case of a linear kernel. `coef_` is a readonly property derived from `dual_coef_` and `support_vectors_`. dual_coef_ : ndarray of shape (n_classes -1, n_SV) Dual coefficients of the support vector in the decision function (see :ref: sgd_mathematical_formulation ), multiplied by their targets. For multiclass, coefficient for all 1-vs-1 classifiers. The layout of the coefficients in the multiclass case is somewhat non-trivial. See the :ref: multi-class section of the User Guide <svm_multi_class> for details. fit_status_ : int 0 if correctly fitted, 1 otherwise (will raise warning) intercept_ : ndarray of shape (n_classes * (n_classes - 1) / 2,) Constants in decision function. n_features_in_ : int Number of features seen during :term: fit . .. versionadded:: 0.24 feature_names_in_ : ndarray of shape ( n_features_in_ ,) Names of features seen during :term: fit . Defined only when X has feature names that are all strings. .. versionadded:: 1.0 support_ : ndarray of shape (n_SV) Indices of support vectors. support_vectors_ : ndarray of shape (n_SV, n_features) Support vectors. n_support_ : ndarray of shape (n_classes,), dtype=int32 Number of support vectors for each class. probA_ : ndarray of shape (n_classes * (n_classes - 1) / 2) probB_ : ndarray of shape (n_classes * (n_classes - 1) / 2) If probability=True , it corresponds to the parameters learned in Platt scaling to produce probability estimates from decision values. If probability=False , it's an empty array. Platt scaling uses the logistic function 1 / (1 + exp(decision_value * probA_ + probB_)) where probA_ and probB_ are learned from the dataset [2] . For more information on the multiclass case and training procedure see section 8 of [1] . shape_fit_ : tuple of int of shape (n_dimensions_of_X,) Array dimensions of training vector X . See Also \u00b6 SVR : Support Vector Machine for Regression implemented using libsvm. LinearSVC : Scalable Linear Support Vector Machine for classification implemented using liblinear. Check the See Also section of LinearSVC for more comparison element. References \u00b6 .. [1] LIBSVM: A Library for Support Vector Machines <http://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf> _ .. [2] Platt, John (1999). \"Probabilistic outputs for support vector machines and comparison to regularizedlikelihood methods.\" <http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.41.1639> _ Examples \u00b6 import numpy as np from sklearn.pipeline import make_pipeline from sklearn.preprocessing import StandardScaler X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]]) y = np.array([1, 1, 2, 2]) from sklearn.svm import SVC clf = make_pipeline(StandardScaler(), SVC(gamma='auto')) clf.fit(X, y) Pipeline(steps=[('standardscaler', StandardScaler()), ('svc', SVC(gamma='auto'))]) print(clf.predict([[-0.8, -1]])) [1] SVR ( RegressorMixin , BaseLibSVM ) \u00b6 Epsilon-Support Vector Regression. The free parameters in the model are C and epsilon. The implementation is based on libsvm. The fit time complexity is more than quadratic with the number of samples which makes it hard to scale to datasets with more than a couple of 10000 samples. For large datasets consider using :class: ~sklearn.svm.LinearSVR or :class: ~sklearn.linear_model.SGDRegressor instead, possibly after a :class: ~sklearn.kernel_approximation.Nystroem transformer. Read more in the :ref: User Guide <svm_regression> . Parameters \u00b6 kernel : {'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'}, default='rbf' Specifies the kernel type to be used in the algorithm. It must be one of 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed' or a callable. If none is given, 'rbf' will be used. If a callable is given it is used to precompute the kernel matrix. degree : int, default=3 Degree of the polynomial kernel function ('poly'). Ignored by all other kernels. gamma : {'scale', 'auto'} or float, default='scale' Kernel coefficient for 'rbf', 'poly' and 'sigmoid'. - if ``gamma='scale'`` (default) is passed then it uses 1 / (n_features * X.var()) as value of gamma, - if 'auto', uses 1 / n_features. .. versionchanged:: 0.22 The default value of ``gamma`` changed from 'auto' to 'scale'. coef0 : float, default=0.0 Independent term in kernel function. It is only significant in 'poly' and 'sigmoid'. tol : float, default=1e-3 Tolerance for stopping criterion. C : float, default=1.0 Regularization parameter. The strength of the regularization is inversely proportional to C. Must be strictly positive. The penalty is a squared l2 penalty. epsilon : float, default=0.1 Epsilon in the epsilon-SVR model. It specifies the epsilon-tube within which no penalty is associated in the training loss function with points predicted within a distance epsilon from the actual value. shrinking : bool, default=True Whether to use the shrinking heuristic. See the :ref: User Guide <shrinking_svm> . cache_size : float, default=200 Specify the size of the kernel cache (in MB). verbose : bool, default=False Enable verbose output. Note that this setting takes advantage of a per-process runtime setting in libsvm that, if enabled, may not work properly in a multithreaded context. max_iter : int, default=-1 Hard limit on iterations within solver, or -1 for no limit. Attributes \u00b6 class_weight_ : ndarray of shape (n_classes,) Multipliers of parameter C for each class. Computed based on the class_weight parameter. coef_ : ndarray of shape (1, n_features) Weights assigned to the features (coefficients in the primal problem). This is only available in the case of a linear kernel. `coef_` is readonly property derived from `dual_coef_` and `support_vectors_`. dual_coef_ : ndarray of shape (1, n_SV) Coefficients of the support vector in the decision function. fit_status_ : int 0 if correctly fitted, 1 otherwise (will raise warning) intercept_ : ndarray of shape (1,) Constants in decision function. n_features_in_ : int Number of features seen during :term: fit . .. versionadded:: 0.24 feature_names_in_ : ndarray of shape ( n_features_in_ ,) Names of features seen during :term: fit . Defined only when X has feature names that are all strings. .. versionadded:: 1.0 n_support_ : ndarray of shape (n_classes,), dtype=int32 Number of support vectors for each class. shape_fit_ : tuple of int of shape (n_dimensions_of_X,) Array dimensions of training vector X . support_ : ndarray of shape (n_SV,) Indices of support vectors. support_vectors_ : ndarray of shape (n_SV, n_features) Support vectors. See Also \u00b6 NuSVR : Support Vector Machine for regression implemented using libsvm using a parameter to control the number of support vectors. LinearSVR : Scalable Linear Support Vector Machine for regression implemented using liblinear. References \u00b6 .. [1] LIBSVM: A Library for Support Vector Machines <http://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf> _ .. [2] Platt, John (1999). \"Probabilistic outputs for support vector machines and comparison to regularizedlikelihood methods.\" <http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.41.1639> _ Examples \u00b6 from sklearn.svm import SVR from sklearn.pipeline import make_pipeline from sklearn.preprocessing import StandardScaler import numpy as np n_samples, n_features = 10, 5 rng = np.random.RandomState(0) y = rng.randn(n_samples) X = rng.randn(n_samples, n_features) regr = make_pipeline(StandardScaler(), SVR(C=1.0, epsilon=0.2)) regr.fit(X, y) Pipeline(steps=[('standardscaler', StandardScaler()), ('svr', SVR(epsilon=0.2))]) ParamGridConfig \u00b6 Stores the default grid search parameters to explore for each model. TypeConfig \u00b6 Stores a series of python type hints for model-specific keywords. Array \u00b6 ndarray(shape, dtype=float, buffer=None, offset=0, strides=None, order=None) An array object represents a multidimensional, homogeneous array of fixed-size items. An associated data-type object describes the format of each element in the array (its byte-order, how many bytes it occupies in memory, whether it is an integer, a floating point number, or something else, etc.) Arrays should be constructed using array , zeros or empty (refer to the See Also section below). The parameters given here refer to a low-level method ( ndarray(...) ) for instantiating an array. For more information, refer to the numpy module and examine the methods and attributes of an array. Parameters \u00b6 (for the new method; see Notes below) shape : tuple of ints Shape of created array. dtype : data-type, optional Any object that can be interpreted as a numpy data type. buffer : object exposing buffer interface, optional Used to fill the array with data. offset : int, optional Offset of array data in buffer. strides : tuple of ints, optional Strides of data in memory. order : {'C', 'F'}, optional Row-major (C-style) or column-major (Fortran-style) order. Attributes \u00b6 T : ndarray Transpose of the array. data : buffer The array's elements, in memory. dtype : dtype object Describes the format of the elements in the array. flags : dict Dictionary containing information related to memory use, e.g., 'C_CONTIGUOUS', 'OWNDATA', 'WRITEABLE', etc. flat : numpy.flatiter object Flattened version of the array as an iterator. The iterator allows assignments, e.g., x.flat = 3 (See ndarray.flat for assignment examples; TODO). imag : ndarray Imaginary part of the array. real : ndarray Real part of the array. size : int Number of elements in the array. itemsize : int The memory use of each array element in bytes. nbytes : int The total number of bytes required to store the array data, i.e., itemsize * size . ndim : int The array's number of dimensions. shape : tuple of ints Shape of the array. strides : tuple of ints The step-size required to move from one element to the next in memory. For example, a contiguous (3, 4) array of type int16 in C-order has strides (8, 2) . This implies that to move from element to element in memory requires jumps of 2 bytes. To move from row-to-row, one needs to jump 8 bytes at a time ( 2 * 4 ). ctypes : ctypes object Class containing properties of the array needed for interaction with ctypes. base : ndarray If the array is a view into another array, that array is its base (unless that array is also a view). The base array is where the array data is actually stored. See Also \u00b6 array : Construct an array. zeros : Create an array, each element of which is zero. empty : Create an array, but leave its allocated memory unchanged (i.e., it contains \"garbage\"). dtype : Create a data-type. numpy.typing.NDArray : A :term: generic <generic type> version of ndarray. Notes \u00b6 There are two modes of creating an array using __new__ : If buffer is None, then only shape , dtype , and order are used. If buffer is an object exposing the buffer interface, then all keywords are interpreted. No __init__ method is needed because the array is fully initialized after the __new__ method. Examples \u00b6 These examples illustrate the low-level ndarray constructor. Refer to the See Also section above for easier ways of constructing an ndarray. First mode, buffer is None: np.ndarray(shape=(2,2), dtype=float, order='F') array([[0.0e+000, 0.0e+000], # random [ nan, 2.5e-323]]) Second mode: np.ndarray((2,), buffer=np.array([1,2,3]), ... offset=np.int_().itemsize, ... dtype=int) # offset = 1*itemsize, i.e. skip first element array([2, 3]) BaseSearchCV ( MetaEstimatorMixin , BaseEstimator ) \u00b6 Abstract base class for hyper parameter search with cross-validation. classes_ property readonly \u00b6 Class labels. Only available when refit=True and the estimator is a classifier. n_features_in_ property readonly \u00b6 Number of features seen during :term: fit . Only available when refit=True . decision_function ( self , X ) \u00b6 Call decision_function on the estimator with the best found parameters. Only available if refit=True and the underlying estimator supports decision_function . Parameters \u00b6 X : indexable, length n_samples Must fulfill the input assumptions of the underlying estimator. Returns \u00b6 y_score : ndarray of shape (n_samples,) or (n_samples, n_classes) or (n_samples, n_classes * (n_classes-1) / 2) Result of the decision function for X based on the estimator with the best found parameters. Source code in giants/config.py @available_if ( _estimator_has ( \"decision_function\" )) def decision_function ( self , X ): \"\"\"Call decision_function on the estimator with the best found parameters. Only available if ``refit=True`` and the underlying estimator supports ``decision_function``. Parameters ---------- X : indexable, length n_samples Must fulfill the input assumptions of the underlying estimator. Returns ------- y_score : ndarray of shape (n_samples,) or (n_samples, n_classes) \\ or (n_samples, n_classes * (n_classes-1) / 2) Result of the decision function for `X` based on the estimator with the best found parameters. \"\"\" check_is_fitted ( self ) return self . best_estimator_ . decision_function ( X ) fit ( self , X , y = None , * , groups = None , ** fit_params ) \u00b6 Run fit with all sets of parameters. Parameters \u00b6 X : array-like of shape (n_samples, n_features) Training vector, where n_samples is the number of samples and n_features is the number of features. y : array-like of shape (n_samples, n_output) or (n_samples,), default=None Target relative to X for classification or regression; None for unsupervised learning. groups : array-like of shape (n_samples,), default=None Group labels for the samples used while splitting the dataset into train/test set. Only used in conjunction with a \"Group\" :term: cv instance (e.g., :class: ~sklearn.model_selection.GroupKFold ). **fit_params : dict of str -> object Parameters passed to the fit method of the estimator. Returns \u00b6 self : object Instance of fitted estimator. Source code in giants/config.py def fit ( self , X , y = None , * , groups = None , ** fit_params ): \"\"\"Run fit with all sets of parameters. Parameters ---------- X : array-like of shape (n_samples, n_features) Training vector, where `n_samples` is the number of samples and `n_features` is the number of features. y : array-like of shape (n_samples, n_output) \\ or (n_samples,), default=None Target relative to X for classification or regression; None for unsupervised learning. groups : array-like of shape (n_samples,), default=None Group labels for the samples used while splitting the dataset into train/test set. Only used in conjunction with a \"Group\" :term:`cv` instance (e.g., :class:`~sklearn.model_selection.GroupKFold`). **fit_params : dict of str -> object Parameters passed to the ``fit`` method of the estimator. Returns ------- self : object Instance of fitted estimator. \"\"\" estimator = self . estimator refit_metric = \"score\" if callable ( self . scoring ): scorers = self . scoring elif self . scoring is None or isinstance ( self . scoring , str ): scorers = check_scoring ( self . estimator , self . scoring ) else : scorers = _check_multimetric_scoring ( self . estimator , self . scoring ) self . _check_refit_for_multimetric ( scorers ) refit_metric = self . refit X , y , groups = indexable ( X , y , groups ) fit_params = _check_fit_params ( X , fit_params ) cv_orig = check_cv ( self . cv , y , classifier = is_classifier ( estimator )) n_splits = cv_orig . get_n_splits ( X , y , groups ) base_estimator = clone ( self . estimator ) parallel = Parallel ( n_jobs = self . n_jobs , pre_dispatch = self . pre_dispatch ) fit_and_score_kwargs = dict ( scorer = scorers , fit_params = fit_params , return_train_score = self . return_train_score , return_n_test_samples = True , return_times = True , return_parameters = False , error_score = self . error_score , verbose = self . verbose , ) results = {} with parallel : all_candidate_params = [] all_out = [] all_more_results = defaultdict ( list ) def evaluate_candidates ( candidate_params , cv = None , more_results = None ): cv = cv or cv_orig candidate_params = list ( candidate_params ) n_candidates = len ( candidate_params ) if self . verbose > 0 : print ( \"Fitting {0} folds for each of {1} candidates,\" \" totalling {2} fits\" . format ( n_splits , n_candidates , n_candidates * n_splits ) ) out = parallel ( delayed ( _fit_and_score )( clone ( base_estimator ), X , y , train = train , test = test , parameters = parameters , split_progress = ( split_idx , n_splits ), candidate_progress = ( cand_idx , n_candidates ), ** fit_and_score_kwargs , ) for ( cand_idx , parameters ), ( split_idx , ( train , test )) in product ( enumerate ( candidate_params ), enumerate ( cv . split ( X , y , groups )) ) ) if len ( out ) < 1 : raise ValueError ( \"No fits were performed. \" \"Was the CV iterator empty? \" \"Were there no candidates?\" ) elif len ( out ) != n_candidates * n_splits : raise ValueError ( \"cv.split and cv.get_n_splits returned \" \"inconsistent results. Expected {} \" \"splits, got {} \" . format ( n_splits , len ( out ) // n_candidates ) ) _warn_about_fit_failures ( out , self . error_score ) # For callable self.scoring, the return type is only know after # calling. If the return type is a dictionary, the error scores # can now be inserted with the correct key. The type checking # of out will be done in `_insert_error_scores`. if callable ( self . scoring ): _insert_error_scores ( out , self . error_score ) all_candidate_params . extend ( candidate_params ) all_out . extend ( out ) if more_results is not None : for key , value in more_results . items (): all_more_results [ key ] . extend ( value ) nonlocal results results = self . _format_results ( all_candidate_params , n_splits , all_out , all_more_results ) return results self . _run_search ( evaluate_candidates ) # multimetric is determined here because in the case of a callable # self.scoring the return type is only known after calling first_test_score = all_out [ 0 ][ \"test_scores\" ] self . multimetric_ = isinstance ( first_test_score , dict ) # check refit_metric now for a callabe scorer that is multimetric if callable ( self . scoring ) and self . multimetric_ : self . _check_refit_for_multimetric ( first_test_score ) refit_metric = self . refit # For multi-metric evaluation, store the best_index_, best_params_ and # best_score_ iff refit is one of the scorer names # In single metric evaluation, refit_metric is \"score\" if self . refit or not self . multimetric_ : self . best_index_ = self . _select_best_index ( self . refit , refit_metric , results ) if not callable ( self . refit ): # With a non-custom callable, we can select the best score # based on the best index self . best_score_ = results [ f \"mean_test_ { refit_metric } \" ][ self . best_index_ ] self . best_params_ = results [ \"params\" ][ self . best_index_ ] if self . refit : # we clone again after setting params in case some # of the params are estimators as well. self . best_estimator_ = clone ( clone ( base_estimator ) . set_params ( ** self . best_params_ ) ) refit_start_time = time . time () if y is not None : self . best_estimator_ . fit ( X , y , ** fit_params ) else : self . best_estimator_ . fit ( X , ** fit_params ) refit_end_time = time . time () self . refit_time_ = refit_end_time - refit_start_time if hasattr ( self . best_estimator_ , \"feature_names_in_\" ): self . feature_names_in_ = self . best_estimator_ . feature_names_in_ # Store the only scorer not as a dict for single metric evaluation self . scorer_ = scorers self . cv_results_ = results self . n_splits_ = n_splits return self inverse_transform ( self , Xt ) \u00b6 Call inverse_transform on the estimator with the best found params. Only available if the underlying estimator implements inverse_transform and refit=True . Parameters \u00b6 Xt : indexable, length n_samples Must fulfill the input assumptions of the underlying estimator. Returns \u00b6 X : {ndarray, sparse matrix} of shape (n_samples, n_features) Result of the inverse_transform function for Xt based on the estimator with the best found parameters. Source code in giants/config.py @available_if ( _estimator_has ( \"inverse_transform\" )) def inverse_transform ( self , Xt ): \"\"\"Call inverse_transform on the estimator with the best found params. Only available if the underlying estimator implements ``inverse_transform`` and ``refit=True``. Parameters ---------- Xt : indexable, length n_samples Must fulfill the input assumptions of the underlying estimator. Returns ------- X : {ndarray, sparse matrix} of shape (n_samples, n_features) Result of the `inverse_transform` function for `Xt` based on the estimator with the best found parameters. \"\"\" check_is_fitted ( self ) return self . best_estimator_ . inverse_transform ( Xt ) predict ( self , X ) \u00b6 Call predict on the estimator with the best found parameters. Only available if refit=True and the underlying estimator supports predict . Parameters \u00b6 X : indexable, length n_samples Must fulfill the input assumptions of the underlying estimator. Returns \u00b6 y_pred : ndarray of shape (n_samples,) The predicted labels or values for X based on the estimator with the best found parameters. Source code in giants/config.py @available_if ( _estimator_has ( \"predict\" )) def predict ( self , X ): \"\"\"Call predict on the estimator with the best found parameters. Only available if ``refit=True`` and the underlying estimator supports ``predict``. Parameters ---------- X : indexable, length n_samples Must fulfill the input assumptions of the underlying estimator. Returns ------- y_pred : ndarray of shape (n_samples,) The predicted labels or values for `X` based on the estimator with the best found parameters. \"\"\" check_is_fitted ( self ) return self . best_estimator_ . predict ( X ) predict_log_proba ( self , X ) \u00b6 Call predict_log_proba on the estimator with the best found parameters. Only available if refit=True and the underlying estimator supports predict_log_proba . Parameters \u00b6 X : indexable, length n_samples Must fulfill the input assumptions of the underlying estimator. Returns \u00b6 y_pred : ndarray of shape (n_samples,) or (n_samples, n_classes) Predicted class log-probabilities for X based on the estimator with the best found parameters. The order of the classes corresponds to that in the fitted attribute :term: classes_ . Source code in giants/config.py @available_if ( _estimator_has ( \"predict_log_proba\" )) def predict_log_proba ( self , X ): \"\"\"Call predict_log_proba on the estimator with the best found parameters. Only available if ``refit=True`` and the underlying estimator supports ``predict_log_proba``. Parameters ---------- X : indexable, length n_samples Must fulfill the input assumptions of the underlying estimator. Returns ------- y_pred : ndarray of shape (n_samples,) or (n_samples, n_classes) Predicted class log-probabilities for `X` based on the estimator with the best found parameters. The order of the classes corresponds to that in the fitted attribute :term:`classes_`. \"\"\" check_is_fitted ( self ) return self . best_estimator_ . predict_log_proba ( X ) predict_proba ( self , X ) \u00b6 Call predict_proba on the estimator with the best found parameters. Only available if refit=True and the underlying estimator supports predict_proba . Parameters \u00b6 X : indexable, length n_samples Must fulfill the input assumptions of the underlying estimator. Returns \u00b6 y_pred : ndarray of shape (n_samples,) or (n_samples, n_classes) Predicted class probabilities for X based on the estimator with the best found parameters. The order of the classes corresponds to that in the fitted attribute :term: classes_ . Source code in giants/config.py @available_if ( _estimator_has ( \"predict_proba\" )) def predict_proba ( self , X ): \"\"\"Call predict_proba on the estimator with the best found parameters. Only available if ``refit=True`` and the underlying estimator supports ``predict_proba``. Parameters ---------- X : indexable, length n_samples Must fulfill the input assumptions of the underlying estimator. Returns ------- y_pred : ndarray of shape (n_samples,) or (n_samples, n_classes) Predicted class probabilities for `X` based on the estimator with the best found parameters. The order of the classes corresponds to that in the fitted attribute :term:`classes_`. \"\"\" check_is_fitted ( self ) return self . best_estimator_ . predict_proba ( X ) score ( self , X , y = None ) \u00b6 Return the score on the given data, if the estimator has been refit. This uses the score defined by scoring where provided, and the best_estimator_.score method otherwise. Parameters \u00b6 X : array-like of shape (n_samples, n_features) Input data, where n_samples is the number of samples and n_features is the number of features. y : array-like of shape (n_samples, n_output) or (n_samples,), default=None Target relative to X for classification or regression; None for unsupervised learning. Returns \u00b6 score : float The score defined by scoring if provided, and the best_estimator_.score method otherwise. Source code in giants/config.py def score ( self , X , y = None ): \"\"\"Return the score on the given data, if the estimator has been refit. This uses the score defined by ``scoring`` where provided, and the ``best_estimator_.score`` method otherwise. Parameters ---------- X : array-like of shape (n_samples, n_features) Input data, where `n_samples` is the number of samples and `n_features` is the number of features. y : array-like of shape (n_samples, n_output) \\ or (n_samples,), default=None Target relative to X for classification or regression; None for unsupervised learning. Returns ------- score : float The score defined by ``scoring`` if provided, and the ``best_estimator_.score`` method otherwise. \"\"\" _check_refit ( self , \"score\" ) check_is_fitted ( self ) if self . scorer_ is None : raise ValueError ( \"No score function explicitly defined, \" \"and the estimator doesn't provide one %s \" % self . best_estimator_ ) if isinstance ( self . scorer_ , dict ): if self . multimetric_ : scorer = self . scorer_ [ self . refit ] else : scorer = self . scorer_ return scorer ( self . best_estimator_ , X , y ) # callable score = self . scorer_ ( self . best_estimator_ , X , y ) if self . multimetric_ : score = score [ self . refit ] return score score_samples ( self , X ) \u00b6 Call score_samples on the estimator with the best found parameters. Only available if refit=True and the underlying estimator supports score_samples . .. versionadded:: 0.24 Parameters \u00b6 X : iterable Data to predict on. Must fulfill input requirements of the underlying estimator. Returns \u00b6 y_score : ndarray of shape (n_samples,) The best_estimator_.score_samples method. Source code in giants/config.py @available_if ( _estimator_has ( \"score_samples\" )) def score_samples ( self , X ): \"\"\"Call score_samples on the estimator with the best found parameters. Only available if ``refit=True`` and the underlying estimator supports ``score_samples``. .. versionadded:: 0.24 Parameters ---------- X : iterable Data to predict on. Must fulfill input requirements of the underlying estimator. Returns ------- y_score : ndarray of shape (n_samples,) The ``best_estimator_.score_samples`` method. \"\"\" check_is_fitted ( self ) return self . best_estimator_ . score_samples ( X ) transform ( self , X ) \u00b6 Call transform on the estimator with the best found parameters. Only available if the underlying estimator supports transform and refit=True . Parameters \u00b6 X : indexable, length n_samples Must fulfill the input assumptions of the underlying estimator. Returns \u00b6 Xt : {ndarray, sparse matrix} of shape (n_samples, n_features) X transformed in the new space based on the estimator with the best found parameters. Source code in giants/config.py @available_if ( _estimator_has ( \"transform\" )) def transform ( self , X ): \"\"\"Call transform on the estimator with the best found parameters. Only available if the underlying estimator supports ``transform`` and ``refit=True``. Parameters ---------- X : indexable, length n_samples Must fulfill the input assumptions of the underlying estimator. Returns ------- Xt : {ndarray, sparse matrix} of shape (n_samples, n_features) `X` transformed in the new space based on the estimator with the best found parameters. \"\"\" check_is_fitted ( self ) return self . best_estimator_ . transform ( X ) Number \u00b6 All numbers inherit from this class. If you just want to check if an argument x is a number, without caring what kind, use isinstance(x, Number).","title":"giants.config"},{"location":"module/config/#giants.config.ModelConfig","text":"Stores default model tuning parameters","title":"ModelConfig"},{"location":"module/config/#giants.config.ModelConfig.CVClassification","text":"Stratified K-Folds cross-validator. Provides train/test indices to split data in train/test sets. This cross-validation object is a variation of KFold that returns stratified folds. The folds are made by preserving the percentage of samples for each class. Read more in the :ref: User Guide <stratified_k_fold> .","title":"CVClassification"},{"location":"module/config/#giants.config.ModelConfig.CVClassification--parameters","text":"n_splits : int, default=5 Number of folds. Must be at least 2. .. versionchanged:: 0.22 ``n_splits`` default value changed from 3 to 5. shuffle : bool, default=False Whether to shuffle each class's samples before splitting into batches. Note that the samples within each split will not be shuffled. random_state : int, RandomState instance or None, default=None When shuffle is True, random_state affects the ordering of the indices, which controls the randomness of each fold for each class. Otherwise, leave random_state as None . Pass an int for reproducible output across multiple function calls. See :term: Glossary <random_state> .","title":"Parameters"},{"location":"module/config/#giants.config.ModelConfig.CVClassification--examples","text":"import numpy as np from sklearn.model_selection import StratifiedKFold X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]]) y = np.array([0, 0, 1, 1]) skf = StratifiedKFold(n_splits=2) skf.get_n_splits(X, y) 2 print(skf) StratifiedKFold(n_splits=2, random_state=None, shuffle=False) for train_index, test_index in skf.split(X, y): ... print(\"TRAIN:\", train_index, \"TEST:\", test_index) ... X_train, X_test = X[train_index], X[test_index] ... y_train, y_test = y[train_index], y[test_index] TRAIN: [1 3] TEST: [0 2] TRAIN: [0 2] TEST: [1 3]","title":"Examples"},{"location":"module/config/#giants.config.ModelConfig.CVClassification--notes","text":"The implementation is designed to: Generate test sets such that all contain the same distribution of classes, or as close as possible. Be invariant to class label: relabelling y = [\"Happy\", \"Sad\"] to y = [1, 0] should not change the indices generated. Preserve order dependencies in the dataset ordering, when shuffle=False : all samples from class k in some test set were contiguous in y, or separated in y by samples from classes other than k. Generate test sets where the smallest and largest differ by at most one sample. .. versionchanged:: 0.22 The previous implementation did not follow the last constraint.","title":"Notes"},{"location":"module/config/#giants.config.ModelConfig.CVClassification--see-also","text":"RepeatedStratifiedKFold : Repeats Stratified K-Fold n times.","title":"See Also"},{"location":"module/config/#giants.config.ModelConfig.CVClassification.split","text":"Generate indices to split data into training and test set.","title":"split()"},{"location":"module/config/#giants.config.ModelConfig.CVClassification.split--parameters","text":"X : array-like of shape (n_samples, n_features) Training data, where n_samples is the number of samples and n_features is the number of features. Note that providing ``y`` is sufficient to generate the splits and hence ``np.zeros(n_samples)`` may be used as a placeholder for ``X`` instead of actual training data. y : array-like of shape (n_samples,) The target variable for supervised learning problems. Stratification is done based on the y labels. groups : object Always ignored, exists for compatibility.","title":"Parameters"},{"location":"module/config/#giants.config.ModelConfig.CVClassification.split--yields","text":"train : ndarray The training set indices for that split. test : ndarray The testing set indices for that split.","title":"Yields"},{"location":"module/config/#giants.config.ModelConfig.CVClassification.split--notes","text":"Randomized CV splitters may return different results for each call of split. You can make the results identical by setting random_state to an integer. Source code in giants/config.py def split ( self , X , y , groups = None ): \"\"\"Generate indices to split data into training and test set. Parameters ---------- X : array-like of shape (n_samples, n_features) Training data, where `n_samples` is the number of samples and `n_features` is the number of features. Note that providing ``y`` is sufficient to generate the splits and hence ``np.zeros(n_samples)`` may be used as a placeholder for ``X`` instead of actual training data. y : array-like of shape (n_samples,) The target variable for supervised learning problems. Stratification is done based on the y labels. groups : object Always ignored, exists for compatibility. Yields ------ train : ndarray The training set indices for that split. test : ndarray The testing set indices for that split. Notes ----- Randomized CV splitters may return different results for each call of split. You can make the results identical by setting `random_state` to an integer. \"\"\" y = check_array ( y , ensure_2d = False , dtype = None ) return super () . split ( X , y , groups )","title":"Notes"},{"location":"module/config/#giants.config.ModelConfig.CVRegression","text":"K-Folds cross-validator Provides train/test indices to split data in train/test sets. Split dataset into k consecutive folds (without shuffling by default). Each fold is then used once as a validation while the k - 1 remaining folds form the training set. Read more in the :ref: User Guide <k_fold> .","title":"CVRegression"},{"location":"module/config/#giants.config.ModelConfig.CVRegression--parameters","text":"n_splits : int, default=5 Number of folds. Must be at least 2. .. versionchanged:: 0.22 ``n_splits`` default value changed from 3 to 5. shuffle : bool, default=False Whether to shuffle the data before splitting into batches. Note that the samples within each split will not be shuffled. random_state : int, RandomState instance or None, default=None When shuffle is True, random_state affects the ordering of the indices, which controls the randomness of each fold. Otherwise, this parameter has no effect. Pass an int for reproducible output across multiple function calls. See :term: Glossary <random_state> .","title":"Parameters"},{"location":"module/config/#giants.config.ModelConfig.CVRegression--examples","text":"import numpy as np from sklearn.model_selection import KFold X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]]) y = np.array([1, 2, 3, 4]) kf = KFold(n_splits=2) kf.get_n_splits(X) 2 print(kf) KFold(n_splits=2, random_state=None, shuffle=False) for train_index, test_index in kf.split(X): ... print(\"TRAIN:\", train_index, \"TEST:\", test_index) ... X_train, X_test = X[train_index], X[test_index] ... y_train, y_test = y[train_index], y[test_index] TRAIN: [2 3] TEST: [0 1] TRAIN: [0 1] TEST: [2 3]","title":"Examples"},{"location":"module/config/#giants.config.ModelConfig.CVRegression--notes","text":"The first n_samples % n_splits folds have size n_samples // n_splits + 1 , other folds have size n_samples // n_splits , where n_samples is the number of samples. Randomized CV splitters may return different results for each call of split. You can make the results identical by setting random_state to an integer.","title":"Notes"},{"location":"module/config/#giants.config.ModelConfig.CVRegression--see-also","text":"StratifiedKFold : Takes group information into account to avoid building folds with imbalanced class distributions (for binary or multiclass classification tasks). GroupKFold : K-fold iterator variant with non-overlapping groups. RepeatedKFold : Repeats K-Fold n times.","title":"See Also"},{"location":"module/config/#giants.config.ModelConfig.Optimizer","text":"Exhaustive search over specified parameter values for an estimator. Important members are fit, predict. GridSearchCV implements a \"fit\" and a \"score\" method. It also implements \"score_samples\", \"predict\", \"predict_proba\", \"decision_function\", \"transform\" and \"inverse_transform\" if they are implemented in the estimator used. The parameters of the estimator used to apply these methods are optimized by cross-validated grid-search over a parameter grid. Read more in the :ref: User Guide <grid_search> .","title":"Optimizer"},{"location":"module/config/#giants.config.ModelConfig.Optimizer--parameters","text":"estimator : estimator object This is assumed to implement the scikit-learn estimator interface. Either estimator needs to provide a score function, or scoring must be passed. param_grid : dict or list of dictionaries Dictionary with parameters names ( str ) as keys and lists of parameter settings to try as values, or a list of such dictionaries, in which case the grids spanned by each dictionary in the list are explored. This enables searching over any sequence of parameter settings. scoring : str, callable, list, tuple or dict, default=None Strategy to evaluate the performance of the cross-validated model on the test set. If `scoring` represents a single score, one can use: - a single string (see :ref:`scoring_parameter`); - a callable (see :ref:`scoring`) that returns a single value. If `scoring` represents multiple scores, one can use: - a list or tuple of unique strings; - a callable returning a dictionary where the keys are the metric names and the values are the metric scores; - a dictionary with metric names as keys and callables a values. See :ref:`multimetric_grid_search` for an example. n_jobs : int, default=None Number of jobs to run in parallel. None means 1 unless in a :obj: joblib.parallel_backend context. -1 means using all processors. See :term: Glossary <n_jobs> for more details. .. versionchanged:: v0.20 `n_jobs` default changed from 1 to None refit : bool, str, or callable, default=True Refit an estimator using the best found parameters on the whole dataset. For multiple metric evaluation, this needs to be a `str` denoting the scorer that would be used to find the best parameters for refitting the estimator at the end. Where there are considerations other than maximum score in choosing a best estimator, ``refit`` can be set to a function which returns the selected ``best_index_`` given ``cv_results_``. In that case, the ``best_estimator_`` and ``best_params_`` will be set according to the returned ``best_index_`` while the ``best_score_`` attribute will not be available. The refitted estimator is made available at the ``best_estimator_`` attribute and permits using ``predict`` directly on this ``GridSearchCV`` instance. Also for multiple metric evaluation, the attributes ``best_index_``, ``best_score_`` and ``best_params_`` will only be available if ``refit`` is set and all of them will be determined w.r.t this specific scorer. See ``scoring`` parameter to know more about multiple metric evaluation. .. versionchanged:: 0.20 Support for callable added. cv : int, cross-validation generator or an iterable, default=None Determines the cross-validation splitting strategy. Possible inputs for cv are: - None, to use the default 5-fold cross validation, - integer, to specify the number of folds in a `(Stratified)KFold`, - :term:`CV splitter`, - An iterable yielding (train, test) splits as arrays of indices. For integer/None inputs, if the estimator is a classifier and ``y`` is either binary or multiclass, :class:`StratifiedKFold` is used. In all other cases, :class:`KFold` is used. These splitters are instantiated with `shuffle=False` so the splits will be the same across calls. Refer :ref:`User Guide <cross_validation>` for the various cross-validation strategies that can be used here. .. versionchanged:: 0.22 ``cv`` default value if None changed from 3-fold to 5-fold. verbose : int Controls the verbosity: the higher, the more messages. - >1 : the computation time for each fold and parameter candidate is displayed; - >2 : the score is also displayed; - >3 : the fold and candidate parameter indexes are also displayed together with the starting time of the computation. pre_dispatch : int, or str, default='2*n_jobs' Controls the number of jobs that get dispatched during parallel execution. Reducing this number can be useful to avoid an explosion of memory consumption when more jobs get dispatched than CPUs can process. This parameter can be: - None, in which case all the jobs are immediately created and spawned. Use this for lightweight and fast-running jobs, to avoid delays due to on-demand spawning of the jobs - An int, giving the exact number of total jobs that are spawned - A str, giving an expression as a function of n_jobs, as in '2*n_jobs' error_score : 'raise' or numeric, default=np.nan Value to assign to the score if an error occurs in estimator fitting. If set to 'raise', the error is raised. If a numeric value is given, FitFailedWarning is raised. This parameter does not affect the refit step, which will always raise the error. return_train_score : bool, default=False If False , the cv_results_ attribute will not include training scores. Computing training scores is used to get insights on how different parameter settings impact the overfitting/underfitting trade-off. However computing the scores on the training set can be computationally expensive and is not strictly required to select the parameters that yield the best generalization performance. .. versionadded:: 0.19 .. versionchanged:: 0.21 Default value was changed from ``True`` to ``False``","title":"Parameters"},{"location":"module/config/#giants.config.ModelConfig.Optimizer--attributes","text":"cv_results_ : dict of numpy (masked) ndarrays A dict with keys as column headers and values as columns, that can be imported into a pandas DataFrame . For instance the below given table +------------+-----------+------------+-----------------+---+---------+ |param_kernel|param_gamma|param_degree|split0_test_score|...|rank_t...| +============+===========+============+=================+===+=========+ | 'poly' | -- | 2 | 0.80 |...| 2 | +------------+-----------+------------+-----------------+---+---------+ | 'poly' | -- | 3 | 0.70 |...| 4 | +------------+-----------+------------+-----------------+---+---------+ | 'rbf' | 0.1 | -- | 0.80 |...| 3 | +------------+-----------+------------+-----------------+---+---------+ | 'rbf' | 0.2 | -- | 0.93 |...| 1 | +------------+-----------+------------+-----------------+---+---------+ will be represented by a ``cv_results_`` dict of:: { 'param_kernel': masked_array(data = ['poly', 'poly', 'rbf', 'rbf'], mask = [False False False False]...) 'param_gamma': masked_array(data = [-- -- 0.1 0.2], mask = [ True True False False]...), 'param_degree': masked_array(data = [2.0 3.0 -- --], mask = [False False True True]...), 'split0_test_score' : [0.80, 0.70, 0.80, 0.93], 'split1_test_score' : [0.82, 0.50, 0.70, 0.78], 'mean_test_score' : [0.81, 0.60, 0.75, 0.85], 'std_test_score' : [0.01, 0.10, 0.05, 0.08], 'rank_test_score' : [2, 4, 3, 1], 'split0_train_score' : [0.80, 0.92, 0.70, 0.93], 'split1_train_score' : [0.82, 0.55, 0.70, 0.87], 'mean_train_score' : [0.81, 0.74, 0.70, 0.90], 'std_train_score' : [0.01, 0.19, 0.00, 0.03], 'mean_fit_time' : [0.73, 0.63, 0.43, 0.49], 'std_fit_time' : [0.01, 0.02, 0.01, 0.01], 'mean_score_time' : [0.01, 0.06, 0.04, 0.04], 'std_score_time' : [0.00, 0.00, 0.00, 0.01], 'params' : [{'kernel': 'poly', 'degree': 2}, ...], } NOTE The key ``'params'`` is used to store a list of parameter settings dicts for all the parameter candidates. The ``mean_fit_time``, ``std_fit_time``, ``mean_score_time`` and ``std_score_time`` are all in seconds. For multi-metric evaluation, the scores for all the scorers are available in the ``cv_results_`` dict at the keys ending with that scorer's name (``'_<scorer_name>'``) instead of ``'_score'`` shown above. ('split0_test_precision', 'mean_train_precision' etc.) best_estimator_ : estimator Estimator that was chosen by the search, i.e. estimator which gave highest score (or smallest loss if specified) on the left out data. Not available if refit=False . See ``refit`` parameter for more information on allowed values. best_score_ : float Mean cross-validated score of the best_estimator For multi-metric evaluation, this is present only if ``refit`` is specified. This attribute is not available if ``refit`` is a function. best_params_ : dict Parameter setting that gave the best results on the hold out data. For multi-metric evaluation, this is present only if ``refit`` is specified. best_index_ : int The index (of the cv_results_ arrays) which corresponds to the best candidate parameter setting. The dict at ``search.cv_results_['params'][search.best_index_]`` gives the parameter setting for the best model, that gives the highest mean score (``search.best_score_``). For multi-metric evaluation, this is present only if ``refit`` is specified. scorer_ : function or a dict Scorer function used on the held out data to choose the best parameters for the model. For multi-metric evaluation, this attribute holds the validated ``scoring`` dict which maps the scorer key to the scorer callable. n_splits_ : int The number of cross-validation splits (folds/iterations). refit_time_ : float Seconds used for refitting the best model on the whole dataset. This is present only if ``refit`` is not False. .. versionadded:: 0.20 multimetric_ : bool Whether or not the scorers compute several metrics. classes_ : ndarray of shape (n_classes,) The classes labels. This is present only if refit is specified and the underlying estimator is a classifier. n_features_in_ : int Number of features seen during :term: fit . Only defined if best_estimator_ is defined (see the documentation for the refit parameter for more details) and that best_estimator_ exposes n_features_in_ when fit. .. versionadded:: 0.24 feature_names_in_ : ndarray of shape ( n_features_in_ ,) Names of features seen during :term: fit . Only defined if best_estimator_ is defined (see the documentation for the refit parameter for more details) and that best_estimator_ exposes feature_names_in_ when fit. .. versionadded:: 1.0","title":"Attributes"},{"location":"module/config/#giants.config.ModelConfig.Optimizer--notes","text":"The parameters selected are those that maximize the score of the left out data, unless an explicit score is passed in which case it is used instead. If n_jobs was set to a value higher than one, the data is copied for each point in the grid (and not n_jobs times). This is done for efficiency reasons if individual jobs take very little time, but may raise errors if the dataset is large and not enough memory is available. A workaround in this case is to set pre_dispatch . Then, the memory is copied only pre_dispatch many times. A reasonable value for pre_dispatch is 2 * n_jobs .","title":"Notes"},{"location":"module/config/#giants.config.ModelConfig.Optimizer--see-also","text":"ParameterGrid : Generates all the combinations of a hyperparameter grid. train_test_split : Utility function to split the data into a development set usable for fitting a GridSearchCV instance and an evaluation set for its final evaluation. sklearn.metrics.make_scorer : Make a scorer from a performance metric or loss function.","title":"See Also"},{"location":"module/config/#giants.config.ModelConfig.Optimizer--examples","text":"from sklearn import svm, datasets from sklearn.model_selection import GridSearchCV iris = datasets.load_iris() parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]} svc = svm.SVC() clf = GridSearchCV(svc, parameters) clf.fit(iris.data, iris.target) GridSearchCV(estimator=SVC(), param_grid={'C': [1, 10], 'kernel': ('linear', 'rbf')}) sorted(clf.cv_results_.keys()) ['mean_fit_time', 'mean_score_time', 'mean_test_score',... 'param_C', 'param_kernel', 'params',... 'rank_test_score', 'split0_test_score',... 'split2_test_score', ... 'std_fit_time', 'std_score_time', 'std_test_score']","title":"Examples"},{"location":"module/config/#giants.config.ModelEstimatorConfig","text":"Stores sklearn model estimators","title":"ModelEstimatorConfig"},{"location":"module/config/#giants.config.ModelEstimatorConfig.AdaBoostClassifier","text":"An AdaBoost classifier. An AdaBoost [1] classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases. This class implements the algorithm known as AdaBoost-SAMME [2]. Read more in the :ref: User Guide <adaboost> . .. versionadded:: 0.14","title":"AdaBoostClassifier"},{"location":"module/config/#giants.config.ModelEstimatorConfig.AdaBoostClassifier--parameters","text":"base_estimator : object, default=None The base estimator from which the boosted ensemble is built. Support for sample weighting is required, as well as proper classes_ and n_classes_ attributes. If None , then the base estimator is :class: ~sklearn.tree.DecisionTreeClassifier initialized with max_depth=1 . n_estimators : int, default=50 The maximum number of estimators at which boosting is terminated. In case of perfect fit, the learning procedure is stopped early. learning_rate : float, default=1.0 Weight applied to each classifier at each boosting iteration. A higher learning rate increases the contribution of each classifier. There is a trade-off between the learning_rate and n_estimators parameters. algorithm : {'SAMME', 'SAMME.R'}, default='SAMME.R' If 'SAMME.R' then use the SAMME.R real boosting algorithm. base_estimator must support calculation of class probabilities. If 'SAMME' then use the SAMME discrete boosting algorithm. The SAMME.R algorithm typically converges faster than SAMME, achieving a lower test error with fewer boosting iterations. random_state : int, RandomState instance or None, default=None Controls the random seed given at each base_estimator at each boosting iteration. Thus, it is only used when base_estimator exposes a random_state . Pass an int for reproducible output across multiple function calls. See :term: Glossary <random_state> .","title":"Parameters"},{"location":"module/config/#giants.config.ModelEstimatorConfig.AdaBoostClassifier--attributes","text":"base_estimator_ : estimator The base estimator from which the ensemble is grown. estimators_ : list of classifiers The collection of fitted sub-estimators. classes_ : ndarray of shape (n_classes,) The classes labels. n_classes_ : int The number of classes. estimator_weights_ : ndarray of floats Weights for each estimator in the boosted ensemble. estimator_errors_ : ndarray of floats Classification error for each estimator in the boosted ensemble. feature_importances_ : ndarray of shape (n_features,) The impurity-based feature importances if supported by the base_estimator (when based on decision trees). Warning: impurity-based feature importances can be misleading for high cardinality features (many unique values). See :func:`sklearn.inspection.permutation_importance` as an alternative. n_features_in_ : int Number of features seen during :term: fit . .. versionadded:: 0.24 feature_names_in_ : ndarray of shape ( n_features_in_ ,) Names of features seen during :term: fit . Defined only when X has feature names that are all strings. .. versionadded:: 1.0","title":"Attributes"},{"location":"module/config/#giants.config.ModelEstimatorConfig.AdaBoostClassifier--see-also","text":"AdaBoostRegressor : An AdaBoost regressor that begins by fitting a regressor on the original dataset and then fits additional copies of the regressor on the same dataset but where the weights of instances are adjusted according to the error of the current prediction. GradientBoostingClassifier : GB builds an additive model in a forward stage-wise fashion. Regression trees are fit on the negative gradient of the binomial or multinomial deviance loss function. Binary classification is a special case where only a single regression tree is induced. sklearn.tree.DecisionTreeClassifier : A non-parametric supervised learning method used for classification. Creates a model that predicts the value of a target variable by learning simple decision rules inferred from the data features.","title":"See Also"},{"location":"module/config/#giants.config.ModelEstimatorConfig.AdaBoostClassifier--references","text":".. [1] Y. Freund, R. Schapire, \"A Decision-Theoretic Generalization of on-Line Learning and an Application to Boosting\", 1995. .. [2] J. Zhu, H. Zou, S. Rosset, T. Hastie, \"Multi-class AdaBoost\", 2009.","title":"References"},{"location":"module/config/#giants.config.ModelEstimatorConfig.AdaBoostClassifier--examples","text":"from sklearn.ensemble import AdaBoostClassifier from sklearn.datasets import make_classification X, y = make_classification(n_samples=1000, n_features=4, ... n_informative=2, n_redundant=0, ... random_state=0, shuffle=False) clf = AdaBoostClassifier(n_estimators=100, random_state=0) clf.fit(X, y) AdaBoostClassifier(n_estimators=100, random_state=0) clf.predict([[0, 0, 0, 0]]) array([1]) clf.score(X, y) 0.983...","title":"Examples"},{"location":"module/config/#giants.config.ModelEstimatorConfig.AdaBoostClassifier.decision_function","text":"Compute the decision function of X .","title":"decision_function()"},{"location":"module/config/#giants.config.ModelEstimatorConfig.AdaBoostClassifier.decision_function--parameters","text":"X : {array-like, sparse matrix} of shape (n_samples, n_features) The training input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. COO, DOK, and LIL are converted to CSR.","title":"Parameters"},{"location":"module/config/#giants.config.ModelEstimatorConfig.AdaBoostClassifier.decision_function--returns","text":"score : ndarray of shape of (n_samples, k) The decision function of the input samples. The order of outputs is the same of that of the :term: classes_ attribute. Binary classification is a special cases with k == 1 , otherwise k==n_classes . For binary classification, values closer to -1 or 1 mean more like the first or second class in classes_ , respectively. Source code in giants/config.py def decision_function ( self , X ): \"\"\"Compute the decision function of ``X``. Parameters ---------- X : {array-like, sparse matrix} of shape (n_samples, n_features) The training input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. COO, DOK, and LIL are converted to CSR. Returns ------- score : ndarray of shape of (n_samples, k) The decision function of the input samples. The order of outputs is the same of that of the :term:`classes_` attribute. Binary classification is a special cases with ``k == 1``, otherwise ``k==n_classes``. For binary classification, values closer to -1 or 1 mean more like the first or second class in ``classes_``, respectively. \"\"\" check_is_fitted ( self ) X = self . _check_X ( X ) n_classes = self . n_classes_ classes = self . classes_ [:, np . newaxis ] if self . algorithm == \"SAMME.R\" : # The weights are all 1. for SAMME.R pred = sum ( _samme_proba ( estimator , n_classes , X ) for estimator in self . estimators_ ) else : # self.algorithm == \"SAMME\" pred = sum ( ( estimator . predict ( X ) == classes ) . T * w for estimator , w in zip ( self . estimators_ , self . estimator_weights_ ) ) pred /= self . estimator_weights_ . sum () if n_classes == 2 : pred [:, 0 ] *= - 1 return pred . sum ( axis = 1 ) return pred","title":"Returns"},{"location":"module/config/#giants.config.ModelEstimatorConfig.AdaBoostClassifier.fit","text":"Build a boosted classifier from the training set (X, y).","title":"fit()"},{"location":"module/config/#giants.config.ModelEstimatorConfig.AdaBoostClassifier.fit--parameters","text":"X : {array-like, sparse matrix} of shape (n_samples, n_features) The training input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. COO, DOK, and LIL are converted to CSR. y : array-like of shape (n_samples,) The target values (class labels). sample_weight : array-like of shape (n_samples,), default=None Sample weights. If None, the sample weights are initialized to 1 / n_samples .","title":"Parameters"},{"location":"module/config/#giants.config.ModelEstimatorConfig.AdaBoostClassifier.fit--returns","text":"self : object Fitted estimator. Source code in giants/config.py def fit ( self , X , y , sample_weight = None ): \"\"\"Build a boosted classifier from the training set (X, y). Parameters ---------- X : {array-like, sparse matrix} of shape (n_samples, n_features) The training input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. COO, DOK, and LIL are converted to CSR. y : array-like of shape (n_samples,) The target values (class labels). sample_weight : array-like of shape (n_samples,), default=None Sample weights. If None, the sample weights are initialized to ``1 / n_samples``. Returns ------- self : object Fitted estimator. \"\"\" # Check that algorithm is supported if self . algorithm not in ( \"SAMME\" , \"SAMME.R\" ): raise ValueError ( \"algorithm %s is not supported\" % self . algorithm ) # Fit return super () . fit ( X , y , sample_weight )","title":"Returns"},{"location":"module/config/#giants.config.ModelEstimatorConfig.AdaBoostClassifier.predict","text":"Predict classes for X. The predicted class of an input sample is computed as the weighted mean prediction of the classifiers in the ensemble.","title":"predict()"},{"location":"module/config/#giants.config.ModelEstimatorConfig.AdaBoostClassifier.predict--parameters","text":"X : {array-like, sparse matrix} of shape (n_samples, n_features) The training input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. COO, DOK, and LIL are converted to CSR.","title":"Parameters"},{"location":"module/config/#giants.config.ModelEstimatorConfig.AdaBoostClassifier.predict--returns","text":"y : ndarray of shape (n_samples,) The predicted classes. Source code in giants/config.py def predict ( self , X ): \"\"\"Predict classes for X. The predicted class of an input sample is computed as the weighted mean prediction of the classifiers in the ensemble. Parameters ---------- X : {array-like, sparse matrix} of shape (n_samples, n_features) The training input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. COO, DOK, and LIL are converted to CSR. Returns ------- y : ndarray of shape (n_samples,) The predicted classes. \"\"\" pred = self . decision_function ( X ) if self . n_classes_ == 2 : return self . classes_ . take ( pred > 0 , axis = 0 ) return self . classes_ . take ( np . argmax ( pred , axis = 1 ), axis = 0 )","title":"Returns"},{"location":"module/config/#giants.config.ModelEstimatorConfig.AdaBoostClassifier.predict_log_proba","text":"Predict class log-probabilities for X. The predicted class log-probabilities of an input sample is computed as the weighted mean predicted class log-probabilities of the classifiers in the ensemble.","title":"predict_log_proba()"},{"location":"module/config/#giants.config.ModelEstimatorConfig.AdaBoostClassifier.predict_log_proba--parameters","text":"X : {array-like, sparse matrix} of shape (n_samples, n_features) The training input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. COO, DOK, and LIL are converted to CSR.","title":"Parameters"},{"location":"module/config/#giants.config.ModelEstimatorConfig.AdaBoostClassifier.predict_log_proba--returns","text":"p : ndarray of shape (n_samples, n_classes) The class probabilities of the input samples. The order of outputs is the same of that of the :term: classes_ attribute. Source code in giants/config.py def predict_log_proba ( self , X ): \"\"\"Predict class log-probabilities for X. The predicted class log-probabilities of an input sample is computed as the weighted mean predicted class log-probabilities of the classifiers in the ensemble. Parameters ---------- X : {array-like, sparse matrix} of shape (n_samples, n_features) The training input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. COO, DOK, and LIL are converted to CSR. Returns ------- p : ndarray of shape (n_samples, n_classes) The class probabilities of the input samples. The order of outputs is the same of that of the :term:`classes_` attribute. \"\"\" return np . log ( self . predict_proba ( X ))","title":"Returns"},{"location":"module/config/#giants.config.ModelEstimatorConfig.AdaBoostClassifier.predict_proba","text":"Predict class probabilities for X. The predicted class probabilities of an input sample is computed as the weighted mean predicted class probabilities of the classifiers in the ensemble.","title":"predict_proba()"},{"location":"module/config/#giants.config.ModelEstimatorConfig.AdaBoostClassifier.predict_proba--parameters","text":"X : {array-like, sparse matrix} of shape (n_samples, n_features) The training input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. COO, DOK, and LIL are converted to CSR.","title":"Parameters"},{"location":"module/config/#giants.config.ModelEstimatorConfig.AdaBoostClassifier.predict_proba--returns","text":"p : ndarray of shape (n_samples, n_classes) The class probabilities of the input samples. The order of outputs is the same of that of the :term: classes_ attribute. Source code in giants/config.py def predict_proba ( self , X ): \"\"\"Predict class probabilities for X. The predicted class probabilities of an input sample is computed as the weighted mean predicted class probabilities of the classifiers in the ensemble. Parameters ---------- X : {array-like, sparse matrix} of shape (n_samples, n_features) The training input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. COO, DOK, and LIL are converted to CSR. Returns ------- p : ndarray of shape (n_samples, n_classes) The class probabilities of the input samples. The order of outputs is the same of that of the :term:`classes_` attribute. \"\"\" check_is_fitted ( self ) n_classes = self . n_classes_ if n_classes == 1 : return np . ones (( _num_samples ( X ), 1 )) decision = self . decision_function ( X ) return self . _compute_proba_from_decision ( decision , n_classes )","title":"Returns"},{"location":"module/config/#giants.config.ModelEstimatorConfig.AdaBoostClassifier.staged_decision_function","text":"Compute decision function of X for each boosting iteration. This method allows monitoring (i.e. determine error on testing set) after each boosting iteration.","title":"staged_decision_function()"},{"location":"module/config/#giants.config.ModelEstimatorConfig.AdaBoostClassifier.staged_decision_function--parameters","text":"X : {array-like, sparse matrix} of shape (n_samples, n_features) The training input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. COO, DOK, and LIL are converted to CSR.","title":"Parameters"},{"location":"module/config/#giants.config.ModelEstimatorConfig.AdaBoostClassifier.staged_decision_function--yields","text":"score : generator of ndarray of shape (n_samples, k) The decision function of the input samples. The order of outputs is the same of that of the :term: classes_ attribute. Binary classification is a special cases with k == 1 , otherwise k==n_classes . For binary classification, values closer to -1 or 1 mean more like the first or second class in classes_ , respectively. Source code in giants/config.py def staged_decision_function ( self , X ): \"\"\"Compute decision function of ``X`` for each boosting iteration. This method allows monitoring (i.e. determine error on testing set) after each boosting iteration. Parameters ---------- X : {array-like, sparse matrix} of shape (n_samples, n_features) The training input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. COO, DOK, and LIL are converted to CSR. Yields ------ score : generator of ndarray of shape (n_samples, k) The decision function of the input samples. The order of outputs is the same of that of the :term:`classes_` attribute. Binary classification is a special cases with ``k == 1``, otherwise ``k==n_classes``. For binary classification, values closer to -1 or 1 mean more like the first or second class in ``classes_``, respectively. \"\"\" check_is_fitted ( self ) X = self . _check_X ( X ) n_classes = self . n_classes_ classes = self . classes_ [:, np . newaxis ] pred = None norm = 0.0 for weight , estimator in zip ( self . estimator_weights_ , self . estimators_ ): norm += weight if self . algorithm == \"SAMME.R\" : # The weights are all 1. for SAMME.R current_pred = _samme_proba ( estimator , n_classes , X ) else : # elif self.algorithm == \"SAMME\": current_pred = estimator . predict ( X ) current_pred = ( current_pred == classes ) . T * weight if pred is None : pred = current_pred else : pred += current_pred if n_classes == 2 : tmp_pred = np . copy ( pred ) tmp_pred [:, 0 ] *= - 1 yield ( tmp_pred / norm ) . sum ( axis = 1 ) else : yield pred / norm","title":"Yields"},{"location":"module/config/#giants.config.ModelEstimatorConfig.AdaBoostClassifier.staged_predict","text":"Return staged predictions for X. The predicted class of an input sample is computed as the weighted mean prediction of the classifiers in the ensemble. This generator method yields the ensemble prediction after each iteration of boosting and therefore allows monitoring, such as to determine the prediction on a test set after each boost.","title":"staged_predict()"},{"location":"module/config/#giants.config.ModelEstimatorConfig.AdaBoostClassifier.staged_predict--parameters","text":"X : array-like of shape (n_samples, n_features) The input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. COO, DOK, and LIL are converted to CSR.","title":"Parameters"},{"location":"module/config/#giants.config.ModelEstimatorConfig.AdaBoostClassifier.staged_predict--yields","text":"y : generator of ndarray of shape (n_samples,) The predicted classes. Source code in giants/config.py def staged_predict ( self , X ): \"\"\"Return staged predictions for X. The predicted class of an input sample is computed as the weighted mean prediction of the classifiers in the ensemble. This generator method yields the ensemble prediction after each iteration of boosting and therefore allows monitoring, such as to determine the prediction on a test set after each boost. Parameters ---------- X : array-like of shape (n_samples, n_features) The input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. COO, DOK, and LIL are converted to CSR. Yields ------ y : generator of ndarray of shape (n_samples,) The predicted classes. \"\"\" X = self . _check_X ( X ) n_classes = self . n_classes_ classes = self . classes_ if n_classes == 2 : for pred in self . staged_decision_function ( X ): yield np . array ( classes . take ( pred > 0 , axis = 0 )) else : for pred in self . staged_decision_function ( X ): yield np . array ( classes . take ( np . argmax ( pred , axis = 1 ), axis = 0 ))","title":"Yields"},{"location":"module/config/#giants.config.ModelEstimatorConfig.AdaBoostClassifier.staged_predict_proba","text":"Predict class probabilities for X. The predicted class probabilities of an input sample is computed as the weighted mean predicted class probabilities of the classifiers in the ensemble. This generator method yields the ensemble predicted class probabilities after each iteration of boosting and therefore allows monitoring, such as to determine the predicted class probabilities on a test set after each boost.","title":"staged_predict_proba()"},{"location":"module/config/#giants.config.ModelEstimatorConfig.AdaBoostClassifier.staged_predict_proba--parameters","text":"X : {array-like, sparse matrix} of shape (n_samples, n_features) The training input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. COO, DOK, and LIL are converted to CSR.","title":"Parameters"},{"location":"module/config/#giants.config.ModelEstimatorConfig.AdaBoostClassifier.staged_predict_proba--yields","text":"p : generator of ndarray of shape (n_samples,) The class probabilities of the input samples. The order of outputs is the same of that of the :term: classes_ attribute. Source code in giants/config.py def staged_predict_proba ( self , X ): \"\"\"Predict class probabilities for X. The predicted class probabilities of an input sample is computed as the weighted mean predicted class probabilities of the classifiers in the ensemble. This generator method yields the ensemble predicted class probabilities after each iteration of boosting and therefore allows monitoring, such as to determine the predicted class probabilities on a test set after each boost. Parameters ---------- X : {array-like, sparse matrix} of shape (n_samples, n_features) The training input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. COO, DOK, and LIL are converted to CSR. Yields ------ p : generator of ndarray of shape (n_samples,) The class probabilities of the input samples. The order of outputs is the same of that of the :term:`classes_` attribute. \"\"\" n_classes = self . n_classes_ for decision in self . staged_decision_function ( X ): yield self . _compute_proba_from_decision ( decision , n_classes )","title":"Yields"},{"location":"module/config/#giants.config.ModelEstimatorConfig.AdaBoostRegressor","text":"An AdaBoost regressor. An AdaBoost [1] regressor is a meta-estimator that begins by fitting a regressor on the original dataset and then fits additional copies of the regressor on the same dataset but where the weights of instances are adjusted according to the error of the current prediction. As such, subsequent regressors focus more on difficult cases. This class implements the algorithm known as AdaBoost.R2 [2]. Read more in the :ref: User Guide <adaboost> . .. versionadded:: 0.14","title":"AdaBoostRegressor"},{"location":"module/config/#giants.config.ModelEstimatorConfig.AdaBoostRegressor--parameters","text":"base_estimator : object, default=None The base estimator from which the boosted ensemble is built. If None , then the base estimator is :class: ~sklearn.tree.DecisionTreeRegressor initialized with max_depth=3 . n_estimators : int, default=50 The maximum number of estimators at which boosting is terminated. In case of perfect fit, the learning procedure is stopped early. learning_rate : float, default=1.0 Weight applied to each regressor at each boosting iteration. A higher learning rate increases the contribution of each regressor. There is a trade-off between the learning_rate and n_estimators parameters. loss : {'linear', 'square', 'exponential'}, default='linear' The loss function to use when updating the weights after each boosting iteration. random_state : int, RandomState instance or None, default=None Controls the random seed given at each base_estimator at each boosting iteration. Thus, it is only used when base_estimator exposes a random_state . In addition, it controls the bootstrap of the weights used to train the base_estimator at each boosting iteration. Pass an int for reproducible output across multiple function calls. See :term: Glossary <random_state> .","title":"Parameters"},{"location":"module/config/#giants.config.ModelEstimatorConfig.AdaBoostRegressor--attributes","text":"base_estimator_ : estimator The base estimator from which the ensemble is grown. estimators_ : list of regressors The collection of fitted sub-estimators. estimator_weights_ : ndarray of floats Weights for each estimator in the boosted ensemble. estimator_errors_ : ndarray of floats Regression error for each estimator in the boosted ensemble. feature_importances_ : ndarray of shape (n_features,) The impurity-based feature importances if supported by the base_estimator (when based on decision trees). Warning: impurity-based feature importances can be misleading for high cardinality features (many unique values). See :func:`sklearn.inspection.permutation_importance` as an alternative. n_features_in_ : int Number of features seen during :term: fit . .. versionadded:: 0.24 feature_names_in_ : ndarray of shape ( n_features_in_ ,) Names of features seen during :term: fit . Defined only when X has feature names that are all strings. .. versionadded:: 1.0","title":"Attributes"},{"location":"module/config/#giants.config.ModelEstimatorConfig.AdaBoostRegressor--see-also","text":"AdaBoostClassifier : An AdaBoost classifier. GradientBoostingRegressor : Gradient Boosting Classification Tree. sklearn.tree.DecisionTreeRegressor : A decision tree regressor.","title":"See Also"},{"location":"module/config/#giants.config.ModelEstimatorConfig.AdaBoostRegressor--references","text":".. [1] Y. Freund, R. Schapire, \"A Decision-Theoretic Generalization of on-Line Learning and an Application to Boosting\", 1995. .. [2] H. Drucker, \"Improving Regressors using Boosting Techniques\", 1997.","title":"References"},{"location":"module/config/#giants.config.ModelEstimatorConfig.AdaBoostRegressor--examples","text":"from sklearn.ensemble import AdaBoostRegressor from sklearn.datasets import make_regression X, y = make_regression(n_features=4, n_informative=2, ... random_state=0, shuffle=False) regr = AdaBoostRegressor(random_state=0, n_estimators=100) regr.fit(X, y) AdaBoostRegressor(n_estimators=100, random_state=0) regr.predict([[0, 0, 0, 0]]) array([4.7972...]) regr.score(X, y) 0.9771...","title":"Examples"},{"location":"module/config/#giants.config.ModelEstimatorConfig.AdaBoostRegressor.fit","text":"Build a boosted regressor from the training set (X, y).","title":"fit()"},{"location":"module/config/#giants.config.ModelEstimatorConfig.AdaBoostRegressor.fit--parameters","text":"X : {array-like, sparse matrix} of shape (n_samples, n_features) The training input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. COO, DOK, and LIL are converted to CSR. y : array-like of shape (n_samples,) The target values (real numbers). sample_weight : array-like of shape (n_samples,), default=None Sample weights. If None, the sample weights are initialized to 1 / n_samples.","title":"Parameters"},{"location":"module/config/#giants.config.ModelEstimatorConfig.AdaBoostRegressor.fit--returns","text":"self : object Fitted AdaBoostRegressor estimator. Source code in giants/config.py def fit ( self , X , y , sample_weight = None ): \"\"\"Build a boosted regressor from the training set (X, y). Parameters ---------- X : {array-like, sparse matrix} of shape (n_samples, n_features) The training input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. COO, DOK, and LIL are converted to CSR. y : array-like of shape (n_samples,) The target values (real numbers). sample_weight : array-like of shape (n_samples,), default=None Sample weights. If None, the sample weights are initialized to 1 / n_samples. Returns ------- self : object Fitted AdaBoostRegressor estimator. \"\"\" # Check loss if self . loss not in ( \"linear\" , \"square\" , \"exponential\" ): raise ValueError ( \"loss must be 'linear', 'square', or 'exponential'\" ) # Fit return super () . fit ( X , y , sample_weight )","title":"Returns"},{"location":"module/config/#giants.config.ModelEstimatorConfig.AdaBoostRegressor.predict","text":"Predict regression value for X. The predicted regression value of an input sample is computed as the weighted median prediction of the regressors in the ensemble.","title":"predict()"},{"location":"module/config/#giants.config.ModelEstimatorConfig.AdaBoostRegressor.predict--parameters","text":"X : {array-like, sparse matrix} of shape (n_samples, n_features) The training input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. COO, DOK, and LIL are converted to CSR.","title":"Parameters"},{"location":"module/config/#giants.config.ModelEstimatorConfig.AdaBoostRegressor.predict--returns","text":"y : ndarray of shape (n_samples,) The predicted regression values. Source code in giants/config.py def predict ( self , X ): \"\"\"Predict regression value for X. The predicted regression value of an input sample is computed as the weighted median prediction of the regressors in the ensemble. Parameters ---------- X : {array-like, sparse matrix} of shape (n_samples, n_features) The training input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. COO, DOK, and LIL are converted to CSR. Returns ------- y : ndarray of shape (n_samples,) The predicted regression values. \"\"\" check_is_fitted ( self ) X = self . _check_X ( X ) return self . _get_median_predict ( X , len ( self . estimators_ ))","title":"Returns"},{"location":"module/config/#giants.config.ModelEstimatorConfig.AdaBoostRegressor.staged_predict","text":"Return staged predictions for X. The predicted regression value of an input sample is computed as the weighted median prediction of the regressors in the ensemble. This generator method yields the ensemble prediction after each iteration of boosting and therefore allows monitoring, such as to determine the prediction on a test set after each boost.","title":"staged_predict()"},{"location":"module/config/#giants.config.ModelEstimatorConfig.AdaBoostRegressor.staged_predict--parameters","text":"X : {array-like, sparse matrix} of shape (n_samples, n_features) The training input samples.","title":"Parameters"},{"location":"module/config/#giants.config.ModelEstimatorConfig.AdaBoostRegressor.staged_predict--yields","text":"y : generator of ndarray of shape (n_samples,) The predicted regression values. Source code in giants/config.py def staged_predict ( self , X ): \"\"\"Return staged predictions for X. The predicted regression value of an input sample is computed as the weighted median prediction of the regressors in the ensemble. This generator method yields the ensemble prediction after each iteration of boosting and therefore allows monitoring, such as to determine the prediction on a test set after each boost. Parameters ---------- X : {array-like, sparse matrix} of shape (n_samples, n_features) The training input samples. Yields ------- y : generator of ndarray of shape (n_samples,) The predicted regression values. \"\"\" check_is_fitted ( self ) X = self . _check_X ( X ) for i , _ in enumerate ( self . estimators_ , 1 ): yield self . _get_median_predict ( X , limit = i )","title":"Yields"},{"location":"module/config/#giants.config.ModelEstimatorConfig.DecisionTreeClassifier","text":"A decision tree classifier. Read more in the :ref: User Guide <tree> .","title":"DecisionTreeClassifier"},{"location":"module/config/#giants.config.ModelEstimatorConfig.DecisionTreeClassifier--parameters","text":"criterion : {\"gini\", \"entropy\"}, default=\"gini\" The function to measure the quality of a split. Supported criteria are \"gini\" for the Gini impurity and \"entropy\" for the information gain. splitter : {\"best\", \"random\"}, default=\"best\" The strategy used to choose the split at each node. Supported strategies are \"best\" to choose the best split and \"random\" to choose the best random split. max_depth : int, default=None The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples. min_samples_split : int or float, default=2 The minimum number of samples required to split an internal node: - If int, then consider `min_samples_split` as the minimum number. - If float, then `min_samples_split` is a fraction and `ceil(min_samples_split * n_samples)` are the minimum number of samples for each split. .. versionchanged:: 0.18 Added float values for fractions. min_samples_leaf : int or float, default=1 The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least min_samples_leaf training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression. - If int, then consider `min_samples_leaf` as the minimum number. - If float, then `min_samples_leaf` is a fraction and `ceil(min_samples_leaf * n_samples)` are the minimum number of samples for each node. .. versionchanged:: 0.18 Added float values for fractions. min_weight_fraction_leaf : float, default=0.0 The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided. max_features : int, float or {\"auto\", \"sqrt\", \"log2\"}, default=None The number of features to consider when looking for the best split: - If int, then consider `max_features` features at each split. - If float, then `max_features` is a fraction and `int(max_features * n_features)` features are considered at each split. - If \"auto\", then `max_features=sqrt(n_features)`. - If \"sqrt\", then `max_features=sqrt(n_features)`. - If \"log2\", then `max_features=log2(n_features)`. - If None, then `max_features=n_features`. Note: the search for a split does not stop until at least one valid partition of the node samples is found, even if it requires to effectively inspect more than ``max_features`` features. random_state : int, RandomState instance or None, default=None Controls the randomness of the estimator. The features are always randomly permuted at each split, even if splitter is set to \"best\" . When max_features < n_features , the algorithm will select max_features at random at each split before finding the best split among them. But the best found split may vary across different runs, even if max_features=n_features . That is the case, if the improvement of the criterion is identical for several splits and one split has to be selected at random. To obtain a deterministic behaviour during fitting, random_state has to be fixed to an integer. See :term: Glossary <random_state> for details. max_leaf_nodes : int, default=None Grow a tree with max_leaf_nodes in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes. min_impurity_decrease : float, default=0.0 A node will be split if this split induces a decrease of the impurity greater than or equal to this value. The weighted impurity decrease equation is the following:: N_t / N * (impurity - N_t_R / N_t * right_impurity - N_t_L / N_t * left_impurity) where ``N`` is the total number of samples, ``N_t`` is the number of samples at the current node, ``N_t_L`` is the number of samples in the left child, and ``N_t_R`` is the number of samples in the right child. ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum, if ``sample_weight`` is passed. .. versionadded:: 0.19 class_weight : dict, list of dict or \"balanced\", default=None Weights associated with classes in the form {class_label: weight} . If None, all classes are supposed to have weight one. For multi-output problems, a list of dicts can be provided in the same order as the columns of y. Note that for multioutput (including multilabel) weights should be defined for each class of every column in its own dict. For example, for four-class multilabel classification weights should be [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of [{1:1}, {2:5}, {3:1}, {4:1}]. The \"balanced\" mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as ``n_samples / (n_classes * np.bincount(y))`` For multi-output, the weights of each column of y will be multiplied. Note that these weights will be multiplied with sample_weight (passed through the fit method) if sample_weight is specified. ccp_alpha : non-negative float, default=0.0 Complexity parameter used for Minimal Cost-Complexity Pruning. The subtree with the largest cost complexity that is smaller than ccp_alpha will be chosen. By default, no pruning is performed. See :ref: minimal_cost_complexity_pruning for details. .. versionadded:: 0.22","title":"Parameters"},{"location":"module/config/#giants.config.ModelEstimatorConfig.DecisionTreeClassifier--attributes","text":"classes_ : ndarray of shape (n_classes,) or list of ndarray The classes labels (single output problem), or a list of arrays of class labels (multi-output problem). feature_importances_ : ndarray of shape (n_features,) The impurity-based feature importances. The higher, the more important the feature. The importance of a feature is computed as the (normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance [4]_. Warning: impurity-based feature importances can be misleading for high cardinality features (many unique values). See :func:`sklearn.inspection.permutation_importance` as an alternative. max_features_ : int The inferred value of max_features. n_classes_ : int or list of int The number of classes (for single output problems), or a list containing the number of classes for each output (for multi-output problems). n_features_ : int The number of features when fit is performed. .. deprecated:: 1.0 `n_features_` is deprecated in 1.0 and will be removed in 1.2. Use `n_features_in_` instead. n_features_in_ : int Number of features seen during :term: fit . .. versionadded:: 0.24 feature_names_in_ : ndarray of shape ( n_features_in_ ,) Names of features seen during :term: fit . Defined only when X has feature names that are all strings. .. versionadded:: 1.0 n_outputs_ : int The number of outputs when fit is performed. tree_ : Tree instance The underlying Tree object. Please refer to help(sklearn.tree._tree.Tree) for attributes of Tree object and :ref: sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py for basic usage of these attributes.","title":"Attributes"},{"location":"module/config/#giants.config.ModelEstimatorConfig.DecisionTreeClassifier--see-also","text":"DecisionTreeRegressor : A decision tree regressor.","title":"See Also"},{"location":"module/config/#giants.config.ModelEstimatorConfig.DecisionTreeClassifier--notes","text":"The default values for the parameters controlling the size of the trees (e.g. max_depth , min_samples_leaf , etc.) lead to fully grown and unpruned trees which can potentially be very large on some data sets. To reduce memory consumption, the complexity and size of the trees should be controlled by setting those parameter values. The :meth: predict method operates using the :func: numpy.argmax function on the outputs of :meth: predict_proba . This means that in case the highest predicted probabilities are tied, the classifier will predict the tied class with the lowest index in :term: classes_ .","title":"Notes"},{"location":"module/config/#giants.config.ModelEstimatorConfig.DecisionTreeClassifier--references","text":".. [1] en.wikipedia.org/wiki/Decision_tree_learning .. [2] L. Breiman, J. Friedman, R. Olshen, and C. Stone, \"Classification and Regression Trees\", Wadsworth, Belmont, CA, 1984. .. [3] T. Hastie, R. Tibshirani and J. Friedman. \"Elements of Statistical Learning\", Springer, 2009. .. [4] L. Breiman, and A. Cutler, \"Random Forests\", www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm","title":"References"},{"location":"module/config/#giants.config.ModelEstimatorConfig.DecisionTreeClassifier--examples","text":"from sklearn.datasets import load_iris from sklearn.model_selection import cross_val_score from sklearn.tree import DecisionTreeClassifier clf = DecisionTreeClassifier(random_state=0) iris = load_iris() cross_val_score(clf, iris.data, iris.target, cv=10) ... # doctest: +SKIP ... array([ 1. , 0.93..., 0.86..., 0.93..., 0.93..., 0.93..., 0.93..., 1. , 0.93..., 1. ])","title":"Examples"},{"location":"module/config/#giants.config.ModelEstimatorConfig.DecisionTreeClassifier.n_features_","text":"DEPRECATED: The attribute n_features_ is deprecated in 1.0 and will be removed in 1.2. Use n_features_in_ instead.","title":"n_features_"},{"location":"module/config/#giants.config.ModelEstimatorConfig.DecisionTreeClassifier.fit","text":"Build a decision tree classifier from the training set (X, y).","title":"fit()"},{"location":"module/config/#giants.config.ModelEstimatorConfig.DecisionTreeClassifier.fit--parameters","text":"X : {array-like, sparse matrix} of shape (n_samples, n_features) The training input samples. Internally, it will be converted to dtype=np.float32 and if a sparse matrix is provided to a sparse csc_matrix . y : array-like of shape (n_samples,) or (n_samples, n_outputs) The target values (class labels) as integers or strings. sample_weight : array-like of shape (n_samples,), default=None Sample weights. If None, then samples are equally weighted. Splits that would create child nodes with net zero or negative weight are ignored while searching for a split in each node. Splits are also ignored if they would result in any single class carrying a negative weight in either child node. check_input : bool, default=True Allow to bypass several input checking. Don't use this parameter unless you know what you do. X_idx_sorted : deprecated, default=\"deprecated\" This parameter is deprecated and has no effect. It will be removed in 1.1 (renaming of 0.26). .. deprecated:: 0.24","title":"Parameters"},{"location":"module/config/#giants.config.ModelEstimatorConfig.DecisionTreeClassifier.fit--returns","text":"self : DecisionTreeClassifier Fitted estimator. Source code in giants/config.py def fit ( self , X , y , sample_weight = None , check_input = True , X_idx_sorted = \"deprecated\" ): \"\"\"Build a decision tree classifier from the training set (X, y). Parameters ---------- X : {array-like, sparse matrix} of shape (n_samples, n_features) The training input samples. Internally, it will be converted to ``dtype=np.float32`` and if a sparse matrix is provided to a sparse ``csc_matrix``. y : array-like of shape (n_samples,) or (n_samples, n_outputs) The target values (class labels) as integers or strings. sample_weight : array-like of shape (n_samples,), default=None Sample weights. If None, then samples are equally weighted. Splits that would create child nodes with net zero or negative weight are ignored while searching for a split in each node. Splits are also ignored if they would result in any single class carrying a negative weight in either child node. check_input : bool, default=True Allow to bypass several input checking. Don't use this parameter unless you know what you do. X_idx_sorted : deprecated, default=\"deprecated\" This parameter is deprecated and has no effect. It will be removed in 1.1 (renaming of 0.26). .. deprecated:: 0.24 Returns ------- self : DecisionTreeClassifier Fitted estimator. \"\"\" super () . fit ( X , y , sample_weight = sample_weight , check_input = check_input , X_idx_sorted = X_idx_sorted , ) return self","title":"Returns"},{"location":"module/config/#giants.config.ModelEstimatorConfig.DecisionTreeClassifier.predict_log_proba","text":"Predict class log-probabilities of the input samples X.","title":"predict_log_proba()"},{"location":"module/config/#giants.config.ModelEstimatorConfig.DecisionTreeClassifier.predict_log_proba--parameters","text":"X : {array-like, sparse matrix} of shape (n_samples, n_features) The input samples. Internally, it will be converted to dtype=np.float32 and if a sparse matrix is provided to a sparse csr_matrix .","title":"Parameters"},{"location":"module/config/#giants.config.ModelEstimatorConfig.DecisionTreeClassifier.predict_log_proba--returns","text":"proba : ndarray of shape (n_samples, n_classes) or list of n_outputs such arrays if n_outputs > 1 The class log-probabilities of the input samples. The order of the classes corresponds to that in the attribute :term: classes_ . Source code in giants/config.py def predict_log_proba ( self , X ): \"\"\"Predict class log-probabilities of the input samples X. Parameters ---------- X : {array-like, sparse matrix} of shape (n_samples, n_features) The input samples. Internally, it will be converted to ``dtype=np.float32`` and if a sparse matrix is provided to a sparse ``csr_matrix``. Returns ------- proba : ndarray of shape (n_samples, n_classes) or list of n_outputs \\ such arrays if n_outputs > 1 The class log-probabilities of the input samples. The order of the classes corresponds to that in the attribute :term:`classes_`. \"\"\" proba = self . predict_proba ( X ) if self . n_outputs_ == 1 : return np . log ( proba ) else : for k in range ( self . n_outputs_ ): proba [ k ] = np . log ( proba [ k ]) return proba","title":"Returns"},{"location":"module/config/#giants.config.ModelEstimatorConfig.DecisionTreeClassifier.predict_proba","text":"Predict class probabilities of the input samples X. The predicted class probability is the fraction of samples of the same class in a leaf.","title":"predict_proba()"},{"location":"module/config/#giants.config.ModelEstimatorConfig.DecisionTreeClassifier.predict_proba--parameters","text":"X : {array-like, sparse matrix} of shape (n_samples, n_features) The input samples. Internally, it will be converted to dtype=np.float32 and if a sparse matrix is provided to a sparse csr_matrix . check_input : bool, default=True Allow to bypass several input checking. Don't use this parameter unless you know what you do.","title":"Parameters"},{"location":"module/config/#giants.config.ModelEstimatorConfig.DecisionTreeClassifier.predict_proba--returns","text":"proba : ndarray of shape (n_samples, n_classes) or list of n_outputs such arrays if n_outputs > 1 The class probabilities of the input samples. The order of the classes corresponds to that in the attribute :term: classes_ . Source code in giants/config.py def predict_proba ( self , X , check_input = True ): \"\"\"Predict class probabilities of the input samples X. The predicted class probability is the fraction of samples of the same class in a leaf. Parameters ---------- X : {array-like, sparse matrix} of shape (n_samples, n_features) The input samples. Internally, it will be converted to ``dtype=np.float32`` and if a sparse matrix is provided to a sparse ``csr_matrix``. check_input : bool, default=True Allow to bypass several input checking. Don't use this parameter unless you know what you do. Returns ------- proba : ndarray of shape (n_samples, n_classes) or list of n_outputs \\ such arrays if n_outputs > 1 The class probabilities of the input samples. The order of the classes corresponds to that in the attribute :term:`classes_`. \"\"\" check_is_fitted ( self ) X = self . _validate_X_predict ( X , check_input ) proba = self . tree_ . predict ( X ) if self . n_outputs_ == 1 : proba = proba [:, : self . n_classes_ ] normalizer = proba . sum ( axis = 1 )[:, np . newaxis ] normalizer [ normalizer == 0.0 ] = 1.0 proba /= normalizer return proba else : all_proba = [] for k in range ( self . n_outputs_ ): proba_k = proba [:, k , : self . n_classes_ [ k ]] normalizer = proba_k . sum ( axis = 1 )[:, np . newaxis ] normalizer [ normalizer == 0.0 ] = 1.0 proba_k /= normalizer all_proba . append ( proba_k ) return all_proba","title":"Returns"},{"location":"module/config/#giants.config.ModelEstimatorConfig.GradientBoostingClassifier","text":"Gradient Boosting for classification. GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage n_classes_ regression trees are fit on the negative gradient of the binomial or multinomial deviance loss function. Binary classification is a special case where only a single regression tree is induced. Read more in the :ref: User Guide <gradient_boosting> .","title":"GradientBoostingClassifier"},{"location":"module/config/#giants.config.ModelEstimatorConfig.GradientBoostingClassifier--parameters","text":"loss : {'deviance', 'exponential'}, default='deviance' The loss function to be optimized. 'deviance' refers to deviance (= logistic regression) for classification with probabilistic outputs. For loss 'exponential' gradient boosting recovers the AdaBoost algorithm. learning_rate : float, default=0.1 Learning rate shrinks the contribution of each tree by learning_rate . There is a trade-off between learning_rate and n_estimators. n_estimators : int, default=100 The number of boosting stages to perform. Gradient boosting is fairly robust to over-fitting so a large number usually results in better performance. subsample : float, default=1.0 The fraction of samples to be used for fitting the individual base learners. If smaller than 1.0 this results in Stochastic Gradient Boosting. subsample interacts with the parameter n_estimators . Choosing subsample < 1.0 leads to a reduction of variance and an increase in bias. criterion : {'friedman_mse', 'squared_error', 'mse', 'mae'}, default='friedman_mse' The function to measure the quality of a split. Supported criteria are 'friedman_mse' for the mean squared error with improvement score by Friedman, 'squared_error' for mean squared error, and 'mae' for the mean absolute error. The default value of 'friedman_mse' is generally the best as it can provide a better approximation in some cases. .. versionadded:: 0.18 .. deprecated:: 0.24 `criterion='mae'` is deprecated and will be removed in version 1.1 (renaming of 0.26). Use `criterion='friedman_mse'` or `'squared_error'` instead, as trees should use a squared error criterion in Gradient Boosting. .. deprecated:: 1.0 Criterion 'mse' was deprecated in v1.0 and will be removed in version 1.2. Use `criterion='squared_error'` which is equivalent. min_samples_split : int or float, default=2 The minimum number of samples required to split an internal node: - If int, then consider `min_samples_split` as the minimum number. - If float, then `min_samples_split` is a fraction and `ceil(min_samples_split * n_samples)` are the minimum number of samples for each split. .. versionchanged:: 0.18 Added float values for fractions. min_samples_leaf : int or float, default=1 The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least min_samples_leaf training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression. - If int, then consider `min_samples_leaf` as the minimum number. - If float, then `min_samples_leaf` is a fraction and `ceil(min_samples_leaf * n_samples)` are the minimum number of samples for each node. .. versionchanged:: 0.18 Added float values for fractions. min_weight_fraction_leaf : float, default=0.0 The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided. max_depth : int, default=3 The maximum depth of the individual regression estimators. The maximum depth limits the number of nodes in the tree. Tune this parameter for best performance; the best value depends on the interaction of the input variables. min_impurity_decrease : float, default=0.0 A node will be split if this split induces a decrease of the impurity greater than or equal to this value. The weighted impurity decrease equation is the following:: N_t / N * (impurity - N_t_R / N_t * right_impurity - N_t_L / N_t * left_impurity) where ``N`` is the total number of samples, ``N_t`` is the number of samples at the current node, ``N_t_L`` is the number of samples in the left child, and ``N_t_R`` is the number of samples in the right child. ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum, if ``sample_weight`` is passed. .. versionadded:: 0.19 init : estimator or 'zero', default=None An estimator object that is used to compute the initial predictions. init has to provide :meth: fit and :meth: predict_proba . If 'zero', the initial raw predictions are set to zero. By default, a DummyEstimator predicting the classes priors is used. random_state : int, RandomState instance or None, default=None Controls the random seed given to each Tree estimator at each boosting iteration. In addition, it controls the random permutation of the features at each split (see Notes for more details). It also controls the random splitting of the training data to obtain a validation set if n_iter_no_change is not None. Pass an int for reproducible output across multiple function calls. See :term: Glossary <random_state> . max_features : {'auto', 'sqrt', 'log2'}, int or float, default=None The number of features to consider when looking for the best split: - If int, then consider `max_features` features at each split. - If float, then `max_features` is a fraction and `int(max_features * n_features)` features are considered at each split. - If 'auto', then `max_features=sqrt(n_features)`. - If 'sqrt', then `max_features=sqrt(n_features)`. - If 'log2', then `max_features=log2(n_features)`. - If None, then `max_features=n_features`. Choosing `max_features < n_features` leads to a reduction of variance and an increase in bias. Note: the search for a split does not stop until at least one valid partition of the node samples is found, even if it requires to effectively inspect more than ``max_features`` features. verbose : int, default=0 Enable verbose output. If 1 then it prints progress and performance once in a while (the more trees the lower the frequency). If greater than 1 then it prints progress and performance for every tree. max_leaf_nodes : int, default=None Grow trees with max_leaf_nodes in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes. warm_start : bool, default=False When set to True , reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just erase the previous solution. See :term: the Glossary <warm_start> . validation_fraction : float, default=0.1 The proportion of training data to set aside as validation set for early stopping. Must be between 0 and 1. Only used if n_iter_no_change is set to an integer. .. versionadded:: 0.20 n_iter_no_change : int, default=None n_iter_no_change is used to decide if early stopping will be used to terminate training when validation score is not improving. By default it is set to None to disable early stopping. If set to a number, it will set aside validation_fraction size of the training data as validation and terminate training when validation score is not improving in all of the previous n_iter_no_change numbers of iterations. The split is stratified. .. versionadded:: 0.20 tol : float, default=1e-4 Tolerance for the early stopping. When the loss is not improving by at least tol for n_iter_no_change iterations (if set to a number), the training stops. .. versionadded:: 0.20 ccp_alpha : non-negative float, default=0.0 Complexity parameter used for Minimal Cost-Complexity Pruning. The subtree with the largest cost complexity that is smaller than ccp_alpha will be chosen. By default, no pruning is performed. See :ref: minimal_cost_complexity_pruning for details. .. versionadded:: 0.22","title":"Parameters"},{"location":"module/config/#giants.config.ModelEstimatorConfig.GradientBoostingClassifier--attributes","text":"n_estimators_ : int The number of estimators as selected by early stopping (if n_iter_no_change is specified). Otherwise it is set to n_estimators . .. versionadded:: 0.20 feature_importances_ : ndarray of shape (n_features,) The impurity-based feature importances. The higher, the more important the feature. The importance of a feature is computed as the (normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance. Warning: impurity-based feature importances can be misleading for high cardinality features (many unique values). See :func:`sklearn.inspection.permutation_importance` as an alternative. oob_improvement_ : ndarray of shape (n_estimators,) The improvement in loss (= deviance) on the out-of-bag samples relative to the previous iteration. oob_improvement_[0] is the improvement in loss of the first stage over the init estimator. Only available if subsample < 1.0 train_score_ : ndarray of shape (n_estimators,) The i-th score train_score_[i] is the deviance (= loss) of the model at iteration i on the in-bag sample. If subsample == 1 this is the deviance on the training data. loss_ : LossFunction The concrete LossFunction object. init_ : estimator The estimator that provides the initial predictions. Set via the init argument or loss.init_estimator . estimators_ : ndarray of DecisionTreeRegressor of shape (n_estimators, loss_.K ) The collection of fitted sub-estimators. loss_.K is 1 for binary classification, otherwise n_classes. classes_ : ndarray of shape (n_classes,) The classes labels. n_features_ : int The number of data features. .. deprecated:: 1.0 Attribute `n_features_` was deprecated in version 1.0 and will be removed in 1.2. Use `n_features_in_` instead. n_features_in_ : int Number of features seen during :term: fit . .. versionadded:: 0.24 feature_names_in_ : ndarray of shape ( n_features_in_ ,) Names of features seen during :term: fit . Defined only when X has feature names that are all strings. .. versionadded:: 1.0 n_classes_ : int The number of classes. max_features_ : int The inferred value of max_features.","title":"Attributes"},{"location":"module/config/#giants.config.ModelEstimatorConfig.GradientBoostingClassifier--see-also","text":"HistGradientBoostingClassifier : Histogram-based Gradient Boosting Classification Tree. sklearn.tree.DecisionTreeClassifier : A decision tree classifier. RandomForestClassifier : A meta-estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. AdaBoostClassifier : A meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases.","title":"See Also"},{"location":"module/config/#giants.config.ModelEstimatorConfig.GradientBoostingClassifier--notes","text":"The features are always randomly permuted at each split. Therefore, the best found split may vary, even with the same training data and max_features=n_features , if the improvement of the criterion is identical for several splits enumerated during the search of the best split. To obtain a deterministic behaviour during fitting, random_state has to be fixed.","title":"Notes"},{"location":"module/config/#giants.config.ModelEstimatorConfig.GradientBoostingClassifier--references","text":"J. Friedman, Greedy Function Approximation: A Gradient Boosting Machine, The Annals of Statistics, Vol. 29, No. 5, 2001. J. Friedman, Stochastic Gradient Boosting, 1999 T. Hastie, R. Tibshirani and J. Friedman. Elements of Statistical Learning Ed. 2, Springer, 2009.","title":"References"},{"location":"module/config/#giants.config.ModelEstimatorConfig.GradientBoostingClassifier--examples","text":"The following example shows how to fit a gradient boosting classifier with 100 decision stumps as weak learners. from sklearn.datasets import make_hastie_10_2 from sklearn.ensemble import GradientBoostingClassifier X, y = make_hastie_10_2(random_state=0) X_train, X_test = X[:2000], X[2000:] y_train, y_test = y[:2000], y[2000:] clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, ... max_depth=1, random_state=0).fit(X_train, y_train) clf.score(X_test, y_test) 0.913...","title":"Examples"},{"location":"module/config/#giants.config.ModelEstimatorConfig.GradientBoostingClassifier.decision_function","text":"Compute the decision function of X .","title":"decision_function()"},{"location":"module/config/#giants.config.ModelEstimatorConfig.GradientBoostingClassifier.decision_function--parameters","text":"X : {array-like, sparse matrix} of shape (n_samples, n_features) The input samples. Internally, it will be converted to dtype=np.float32 and if a sparse matrix is provided to a sparse csr_matrix .","title":"Parameters"},{"location":"module/config/#giants.config.ModelEstimatorConfig.GradientBoostingClassifier.decision_function--returns","text":"score : ndarray of shape (n_samples, n_classes) or (n_samples,) The decision function of the input samples, which corresponds to the raw values predicted from the trees of the ensemble . The order of the classes corresponds to that in the attribute :term: classes_ . Regression and binary classification produce an array of shape (n_samples,). Source code in giants/config.py def decision_function ( self , X ): \"\"\"Compute the decision function of ``X``. Parameters ---------- X : {array-like, sparse matrix} of shape (n_samples, n_features) The input samples. Internally, it will be converted to ``dtype=np.float32`` and if a sparse matrix is provided to a sparse ``csr_matrix``. Returns ------- score : ndarray of shape (n_samples, n_classes) or (n_samples,) The decision function of the input samples, which corresponds to the raw values predicted from the trees of the ensemble . The order of the classes corresponds to that in the attribute :term:`classes_`. Regression and binary classification produce an array of shape (n_samples,). \"\"\" X = self . _validate_data ( X , dtype = DTYPE , order = \"C\" , accept_sparse = \"csr\" , reset = False ) raw_predictions = self . _raw_predict ( X ) if raw_predictions . shape [ 1 ] == 1 : return raw_predictions . ravel () return raw_predictions","title":"Returns"},{"location":"module/config/#giants.config.ModelEstimatorConfig.GradientBoostingClassifier.predict","text":"Predict class for X.","title":"predict()"},{"location":"module/config/#giants.config.ModelEstimatorConfig.GradientBoostingClassifier.predict--parameters","text":"X : {array-like, sparse matrix} of shape (n_samples, n_features) The input samples. Internally, it will be converted to dtype=np.float32 and if a sparse matrix is provided to a sparse csr_matrix .","title":"Parameters"},{"location":"module/config/#giants.config.ModelEstimatorConfig.GradientBoostingClassifier.predict--returns","text":"y : ndarray of shape (n_samples,) The predicted values. Source code in giants/config.py def predict ( self , X ): \"\"\"Predict class for X. Parameters ---------- X : {array-like, sparse matrix} of shape (n_samples, n_features) The input samples. Internally, it will be converted to ``dtype=np.float32`` and if a sparse matrix is provided to a sparse ``csr_matrix``. Returns ------- y : ndarray of shape (n_samples,) The predicted values. \"\"\" raw_predictions = self . decision_function ( X ) encoded_labels = self . loss_ . _raw_prediction_to_decision ( raw_predictions ) return self . classes_ . take ( encoded_labels , axis = 0 )","title":"Returns"},{"location":"module/config/#giants.config.ModelEstimatorConfig.GradientBoostingClassifier.predict_log_proba","text":"Predict class log-probabilities for X.","title":"predict_log_proba()"},{"location":"module/config/#giants.config.ModelEstimatorConfig.GradientBoostingClassifier.predict_log_proba--parameters","text":"X : {array-like, sparse matrix} of shape (n_samples, n_features) The input samples. Internally, it will be converted to dtype=np.float32 and if a sparse matrix is provided to a sparse csr_matrix .","title":"Parameters"},{"location":"module/config/#giants.config.ModelEstimatorConfig.GradientBoostingClassifier.predict_log_proba--returns","text":"p : ndarray of shape (n_samples, n_classes) The class log-probabilities of the input samples. The order of the classes corresponds to that in the attribute :term: classes_ .","title":"Returns"},{"location":"module/config/#giants.config.ModelEstimatorConfig.GradientBoostingClassifier.predict_log_proba--raises","text":"AttributeError If the loss does not support probabilities. Source code in giants/config.py def predict_log_proba ( self , X ): \"\"\"Predict class log-probabilities for X. Parameters ---------- X : {array-like, sparse matrix} of shape (n_samples, n_features) The input samples. Internally, it will be converted to ``dtype=np.float32`` and if a sparse matrix is provided to a sparse ``csr_matrix``. Returns ------- p : ndarray of shape (n_samples, n_classes) The class log-probabilities of the input samples. The order of the classes corresponds to that in the attribute :term:`classes_`. Raises ------ AttributeError If the ``loss`` does not support probabilities. \"\"\" proba = self . predict_proba ( X ) return np . log ( proba )","title":"Raises"},{"location":"module/config/#giants.config.ModelEstimatorConfig.GradientBoostingClassifier.predict_proba","text":"Predict class probabilities for X.","title":"predict_proba()"},{"location":"module/config/#giants.config.ModelEstimatorConfig.GradientBoostingClassifier.predict_proba--parameters","text":"X : {array-like, sparse matrix} of shape (n_samples, n_features) The input samples. Internally, it will be converted to dtype=np.float32 and if a sparse matrix is provided to a sparse csr_matrix .","title":"Parameters"},{"location":"module/config/#giants.config.ModelEstimatorConfig.GradientBoostingClassifier.predict_proba--returns","text":"p : ndarray of shape (n_samples, n_classes) The class probabilities of the input samples. The order of the classes corresponds to that in the attribute :term: classes_ .","title":"Returns"},{"location":"module/config/#giants.config.ModelEstimatorConfig.GradientBoostingClassifier.predict_proba--raises","text":"AttributeError If the loss does not support probabilities. Source code in giants/config.py def predict_proba ( self , X ): \"\"\"Predict class probabilities for X. Parameters ---------- X : {array-like, sparse matrix} of shape (n_samples, n_features) The input samples. Internally, it will be converted to ``dtype=np.float32`` and if a sparse matrix is provided to a sparse ``csr_matrix``. Returns ------- p : ndarray of shape (n_samples, n_classes) The class probabilities of the input samples. The order of the classes corresponds to that in the attribute :term:`classes_`. Raises ------ AttributeError If the ``loss`` does not support probabilities. \"\"\" raw_predictions = self . decision_function ( X ) try : return self . loss_ . _raw_prediction_to_proba ( raw_predictions ) except NotFittedError : raise except AttributeError as e : raise AttributeError ( \"loss= %r does not support predict_proba\" % self . loss ) from e","title":"Raises"},{"location":"module/config/#giants.config.ModelEstimatorConfig.GradientBoostingClassifier.staged_decision_function","text":"Compute decision function of X for each iteration. This method allows monitoring (i.e. determine error on testing set) after each stage.","title":"staged_decision_function()"},{"location":"module/config/#giants.config.ModelEstimatorConfig.GradientBoostingClassifier.staged_decision_function--parameters","text":"X : {array-like, sparse matrix} of shape (n_samples, n_features) The input samples. Internally, it will be converted to dtype=np.float32 and if a sparse matrix is provided to a sparse csr_matrix .","title":"Parameters"},{"location":"module/config/#giants.config.ModelEstimatorConfig.GradientBoostingClassifier.staged_decision_function--yields","text":"score : generator of ndarray of shape (n_samples, k) The decision function of the input samples, which corresponds to the raw values predicted from the trees of the ensemble . The classes corresponds to that in the attribute :term: classes_ . Regression and binary classification are special cases with k == 1 , otherwise k==n_classes . Source code in giants/config.py def staged_decision_function ( self , X ): \"\"\"Compute decision function of ``X`` for each iteration. This method allows monitoring (i.e. determine error on testing set) after each stage. Parameters ---------- X : {array-like, sparse matrix} of shape (n_samples, n_features) The input samples. Internally, it will be converted to ``dtype=np.float32`` and if a sparse matrix is provided to a sparse ``csr_matrix``. Yields ------ score : generator of ndarray of shape (n_samples, k) The decision function of the input samples, which corresponds to the raw values predicted from the trees of the ensemble . The classes corresponds to that in the attribute :term:`classes_`. Regression and binary classification are special cases with ``k == 1``, otherwise ``k==n_classes``. \"\"\" yield from self . _staged_raw_predict ( X )","title":"Yields"},{"location":"module/config/#giants.config.ModelEstimatorConfig.GradientBoostingClassifier.staged_predict","text":"Predict class at each stage for X. This method allows monitoring (i.e. determine error on testing set) after each stage.","title":"staged_predict()"},{"location":"module/config/#giants.config.ModelEstimatorConfig.GradientBoostingClassifier.staged_predict--parameters","text":"X : {array-like, sparse matrix} of shape (n_samples, n_features) The input samples. Internally, it will be converted to dtype=np.float32 and if a sparse matrix is provided to a sparse csr_matrix .","title":"Parameters"},{"location":"module/config/#giants.config.ModelEstimatorConfig.GradientBoostingClassifier.staged_predict--yields","text":"y : generator of ndarray of shape (n_samples,) The predicted value of the input samples. Source code in giants/config.py def staged_predict ( self , X ): \"\"\"Predict class at each stage for X. This method allows monitoring (i.e. determine error on testing set) after each stage. Parameters ---------- X : {array-like, sparse matrix} of shape (n_samples, n_features) The input samples. Internally, it will be converted to ``dtype=np.float32`` and if a sparse matrix is provided to a sparse ``csr_matrix``. Yields ------- y : generator of ndarray of shape (n_samples,) The predicted value of the input samples. \"\"\" for raw_predictions in self . _staged_raw_predict ( X ): encoded_labels = self . loss_ . _raw_prediction_to_decision ( raw_predictions ) yield self . classes_ . take ( encoded_labels , axis = 0 )","title":"Yields"},{"location":"module/config/#giants.config.ModelEstimatorConfig.GradientBoostingClassifier.staged_predict_proba","text":"Predict class probabilities at each stage for X. This method allows monitoring (i.e. determine error on testing set) after each stage.","title":"staged_predict_proba()"},{"location":"module/config/#giants.config.ModelEstimatorConfig.GradientBoostingClassifier.staged_predict_proba--parameters","text":"X : {array-like, sparse matrix} of shape (n_samples, n_features) The input samples. Internally, it will be converted to dtype=np.float32 and if a sparse matrix is provided to a sparse csr_matrix .","title":"Parameters"},{"location":"module/config/#giants.config.ModelEstimatorConfig.GradientBoostingClassifier.staged_predict_proba--yields","text":"y : generator of ndarray of shape (n_samples,) The predicted value of the input samples. Source code in giants/config.py def staged_predict_proba ( self , X ): \"\"\"Predict class probabilities at each stage for X. This method allows monitoring (i.e. determine error on testing set) after each stage. Parameters ---------- X : {array-like, sparse matrix} of shape (n_samples, n_features) The input samples. Internally, it will be converted to ``dtype=np.float32`` and if a sparse matrix is provided to a sparse ``csr_matrix``. Yields ------ y : generator of ndarray of shape (n_samples,) The predicted value of the input samples. \"\"\" try : for raw_predictions in self . _staged_raw_predict ( X ): yield self . loss_ . _raw_prediction_to_proba ( raw_predictions ) except NotFittedError : raise except AttributeError as e : raise AttributeError ( \"loss= %r does not support predict_proba\" % self . loss ) from e","title":"Yields"},{"location":"module/config/#giants.config.ModelEstimatorConfig.GradientBoostingRegressor","text":"Gradient Boosting for regression. GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage a regression tree is fit on the negative gradient of the given loss function. Read more in the :ref: User Guide <gradient_boosting> .","title":"GradientBoostingRegressor"},{"location":"module/config/#giants.config.ModelEstimatorConfig.GradientBoostingRegressor--parameters","text":"loss : {'squared_error', 'absolute_error', 'huber', 'quantile'}, default='squared_error' Loss function to be optimized. 'squared_error' refers to the squared error for regression. 'absolute_error' refers to the absolute error of regression and is a robust loss function. 'huber' is a combination of the two. 'quantile' allows quantile regression (use alpha to specify the quantile). .. deprecated:: 1.0 The loss 'ls' was deprecated in v1.0 and will be removed in version 1.2. Use `loss='squared_error'` which is equivalent. .. deprecated:: 1.0 The loss 'lad' was deprecated in v1.0 and will be removed in version 1.2. Use `loss='absolute_error'` which is equivalent. learning_rate : float, default=0.1 Learning rate shrinks the contribution of each tree by learning_rate . There is a trade-off between learning_rate and n_estimators. n_estimators : int, default=100 The number of boosting stages to perform. Gradient boosting is fairly robust to over-fitting so a large number usually results in better performance. subsample : float, default=1.0 The fraction of samples to be used for fitting the individual base learners. If smaller than 1.0 this results in Stochastic Gradient Boosting. subsample interacts with the parameter n_estimators . Choosing subsample < 1.0 leads to a reduction of variance and an increase in bias. criterion : {'friedman_mse', 'squared_error', 'mse', 'mae'}, default='friedman_mse' The function to measure the quality of a split. Supported criteria are \"friedman_mse\" for the mean squared error with improvement score by Friedman, \"squared_error\" for mean squared error, and \"mae\" for the mean absolute error. The default value of \"friedman_mse\" is generally the best as it can provide a better approximation in some cases. .. versionadded:: 0.18 .. deprecated:: 0.24 `criterion='mae'` is deprecated and will be removed in version 1.1 (renaming of 0.26). The correct way of minimizing the absolute error is to use `loss='absolute_error'` instead. .. deprecated:: 1.0 Criterion 'mse' was deprecated in v1.0 and will be removed in version 1.2. Use `criterion='squared_error'` which is equivalent. min_samples_split : int or float, default=2 The minimum number of samples required to split an internal node: - If int, then consider `min_samples_split` as the minimum number. - If float, then `min_samples_split` is a fraction and `ceil(min_samples_split * n_samples)` are the minimum number of samples for each split. .. versionchanged:: 0.18 Added float values for fractions. min_samples_leaf : int or float, default=1 The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least min_samples_leaf training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression. - If int, then consider `min_samples_leaf` as the minimum number. - If float, then `min_samples_leaf` is a fraction and `ceil(min_samples_leaf * n_samples)` are the minimum number of samples for each node. .. versionchanged:: 0.18 Added float values for fractions. min_weight_fraction_leaf : float, default=0.0 The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided. max_depth : int, default=3 Maximum depth of the individual regression estimators. The maximum depth limits the number of nodes in the tree. Tune this parameter for best performance; the best value depends on the interaction of the input variables. min_impurity_decrease : float, default=0.0 A node will be split if this split induces a decrease of the impurity greater than or equal to this value. The weighted impurity decrease equation is the following:: N_t / N * (impurity - N_t_R / N_t * right_impurity - N_t_L / N_t * left_impurity) where ``N`` is the total number of samples, ``N_t`` is the number of samples at the current node, ``N_t_L`` is the number of samples in the left child, and ``N_t_R`` is the number of samples in the right child. ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum, if ``sample_weight`` is passed. .. versionadded:: 0.19 init : estimator or 'zero', default=None An estimator object that is used to compute the initial predictions. init has to provide :term: fit and :term: predict . If 'zero', the initial raw predictions are set to zero. By default a DummyEstimator is used, predicting either the average target value (for loss='squared_error'), or a quantile for the other losses. random_state : int, RandomState instance or None, default=None Controls the random seed given to each Tree estimator at each boosting iteration. In addition, it controls the random permutation of the features at each split (see Notes for more details). It also controls the random splitting of the training data to obtain a validation set if n_iter_no_change is not None. Pass an int for reproducible output across multiple function calls. See :term: Glossary <random_state> . max_features : {'auto', 'sqrt', 'log2'}, int or float, default=None The number of features to consider when looking for the best split: - If int, then consider `max_features` features at each split. - If float, then `max_features` is a fraction and `int(max_features * n_features)` features are considered at each split. - If \"auto\", then `max_features=n_features`. - If \"sqrt\", then `max_features=sqrt(n_features)`. - If \"log2\", then `max_features=log2(n_features)`. - If None, then `max_features=n_features`. Choosing `max_features < n_features` leads to a reduction of variance and an increase in bias. Note: the search for a split does not stop until at least one valid partition of the node samples is found, even if it requires to effectively inspect more than ``max_features`` features. alpha : float, default=0.9 The alpha-quantile of the huber loss function and the quantile loss function. Only if loss='huber' or loss='quantile' . verbose : int, default=0 Enable verbose output. If 1 then it prints progress and performance once in a while (the more trees the lower the frequency). If greater than 1 then it prints progress and performance for every tree. max_leaf_nodes : int, default=None Grow trees with max_leaf_nodes in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes. warm_start : bool, default=False When set to True , reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just erase the previous solution. See :term: the Glossary <warm_start> . validation_fraction : float, default=0.1 The proportion of training data to set aside as validation set for early stopping. Must be between 0 and 1. Only used if n_iter_no_change is set to an integer. .. versionadded:: 0.20 n_iter_no_change : int, default=None n_iter_no_change is used to decide if early stopping will be used to terminate training when validation score is not improving. By default it is set to None to disable early stopping. If set to a number, it will set aside validation_fraction size of the training data as validation and terminate training when validation score is not improving in all of the previous n_iter_no_change numbers of iterations. .. versionadded:: 0.20 tol : float, default=1e-4 Tolerance for the early stopping. When the loss is not improving by at least tol for n_iter_no_change iterations (if set to a number), the training stops. .. versionadded:: 0.20 ccp_alpha : non-negative float, default=0.0 Complexity parameter used for Minimal Cost-Complexity Pruning. The subtree with the largest cost complexity that is smaller than ccp_alpha will be chosen. By default, no pruning is performed. See :ref: minimal_cost_complexity_pruning for details. .. versionadded:: 0.22","title":"Parameters"},{"location":"module/config/#giants.config.ModelEstimatorConfig.GradientBoostingRegressor--attributes","text":"feature_importances_ : ndarray of shape (n_features,) The impurity-based feature importances. The higher, the more important the feature. The importance of a feature is computed as the (normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance. Warning: impurity-based feature importances can be misleading for high cardinality features (many unique values). See :func:`sklearn.inspection.permutation_importance` as an alternative. oob_improvement_ : ndarray of shape (n_estimators,) The improvement in loss (= deviance) on the out-of-bag samples relative to the previous iteration. oob_improvement_[0] is the improvement in loss of the first stage over the init estimator. Only available if subsample < 1.0 train_score_ : ndarray of shape (n_estimators,) The i-th score train_score_[i] is the deviance (= loss) of the model at iteration i on the in-bag sample. If subsample == 1 this is the deviance on the training data. loss_ : LossFunction The concrete LossFunction object. init_ : estimator The estimator that provides the initial predictions. Set via the init argument or loss.init_estimator . estimators_ : ndarray of DecisionTreeRegressor of shape (n_estimators, 1) The collection of fitted sub-estimators. n_classes_ : int The number of classes, set to 1 for regressors. .. deprecated:: 0.24 Attribute ``n_classes_`` was deprecated in version 0.24 and will be removed in 1.1 (renaming of 0.26). n_estimators_ : int The number of estimators as selected by early stopping (if n_iter_no_change is specified). Otherwise it is set to n_estimators . n_features_ : int The number of data features. .. deprecated:: 1.0 Attribute `n_features_` was deprecated in version 1.0 and will be removed in 1.2. Use `n_features_in_` instead. n_features_in_ : int Number of features seen during :term: fit . .. versionadded:: 0.24 feature_names_in_ : ndarray of shape ( n_features_in_ ,) Names of features seen during :term: fit . Defined only when X has feature names that are all strings. .. versionadded:: 1.0 max_features_ : int The inferred value of max_features.","title":"Attributes"},{"location":"module/config/#giants.config.ModelEstimatorConfig.GradientBoostingRegressor--see-also","text":"HistGradientBoostingRegressor : Histogram-based Gradient Boosting Classification Tree. sklearn.tree.DecisionTreeRegressor : A decision tree regressor. sklearn.ensemble.RandomForestRegressor : A random forest regressor.","title":"See Also"},{"location":"module/config/#giants.config.ModelEstimatorConfig.GradientBoostingRegressor--notes","text":"The features are always randomly permuted at each split. Therefore, the best found split may vary, even with the same training data and max_features=n_features , if the improvement of the criterion is identical for several splits enumerated during the search of the best split. To obtain a deterministic behaviour during fitting, random_state has to be fixed.","title":"Notes"},{"location":"module/config/#giants.config.ModelEstimatorConfig.GradientBoostingRegressor--references","text":"J. Friedman, Greedy Function Approximation: A Gradient Boosting Machine, The Annals of Statistics, Vol. 29, No. 5, 2001. J. Friedman, Stochastic Gradient Boosting, 1999 T. Hastie, R. Tibshirani and J. Friedman. Elements of Statistical Learning Ed. 2, Springer, 2009.","title":"References"},{"location":"module/config/#giants.config.ModelEstimatorConfig.GradientBoostingRegressor--examples","text":"from sklearn.datasets import make_regression from sklearn.ensemble import GradientBoostingRegressor from sklearn.model_selection import train_test_split X, y = make_regression(random_state=0) X_train, X_test, y_train, y_test = train_test_split( ... X, y, random_state=0) reg = GradientBoostingRegressor(random_state=0) reg.fit(X_train, y_train) GradientBoostingRegressor(random_state=0) reg.predict(X_test[1:2]) array([-61...]) reg.score(X_test, y_test) 0.4...","title":"Examples"},{"location":"module/config/#giants.config.ModelEstimatorConfig.GradientBoostingRegressor.n_classes_","text":"DEPRECATED: Attribute n_classes_ was deprecated in version 0.24 and will be removed in 1.1 (renaming of 0.26).","title":"n_classes_"},{"location":"module/config/#giants.config.ModelEstimatorConfig.GradientBoostingRegressor.apply","text":"Apply trees in the ensemble to X, return leaf indices. .. versionadded:: 0.17","title":"apply()"},{"location":"module/config/#giants.config.ModelEstimatorConfig.GradientBoostingRegressor.apply--parameters","text":"X : {array-like, sparse matrix} of shape (n_samples, n_features) The input samples. Internally, its dtype will be converted to dtype=np.float32 . If a sparse matrix is provided, it will be converted to a sparse csr_matrix .","title":"Parameters"},{"location":"module/config/#giants.config.ModelEstimatorConfig.GradientBoostingRegressor.apply--returns","text":"X_leaves : array-like of shape (n_samples, n_estimators) For each datapoint x in X and for each tree in the ensemble, return the index of the leaf x ends up in each estimator. Source code in giants/config.py def apply ( self , X ): \"\"\"Apply trees in the ensemble to X, return leaf indices. .. versionadded:: 0.17 Parameters ---------- X : {array-like, sparse matrix} of shape (n_samples, n_features) The input samples. Internally, its dtype will be converted to ``dtype=np.float32``. If a sparse matrix is provided, it will be converted to a sparse ``csr_matrix``. Returns ------- X_leaves : array-like of shape (n_samples, n_estimators) For each datapoint x in X and for each tree in the ensemble, return the index of the leaf x ends up in each estimator. \"\"\" leaves = super () . apply ( X ) leaves = leaves . reshape ( X . shape [ 0 ], self . estimators_ . shape [ 0 ]) return leaves","title":"Returns"},{"location":"module/config/#giants.config.ModelEstimatorConfig.GradientBoostingRegressor.predict","text":"Predict regression target for X.","title":"predict()"},{"location":"module/config/#giants.config.ModelEstimatorConfig.GradientBoostingRegressor.predict--parameters","text":"X : {array-like, sparse matrix} of shape (n_samples, n_features) The input samples. Internally, it will be converted to dtype=np.float32 and if a sparse matrix is provided to a sparse csr_matrix .","title":"Parameters"},{"location":"module/config/#giants.config.ModelEstimatorConfig.GradientBoostingRegressor.predict--returns","text":"y : ndarray of shape (n_samples,) The predicted values. Source code in giants/config.py def predict ( self , X ): \"\"\"Predict regression target for X. Parameters ---------- X : {array-like, sparse matrix} of shape (n_samples, n_features) The input samples. Internally, it will be converted to ``dtype=np.float32`` and if a sparse matrix is provided to a sparse ``csr_matrix``. Returns ------- y : ndarray of shape (n_samples,) The predicted values. \"\"\" X = self . _validate_data ( X , dtype = DTYPE , order = \"C\" , accept_sparse = \"csr\" , reset = False ) # In regression we can directly return the raw value from the trees. return self . _raw_predict ( X ) . ravel ()","title":"Returns"},{"location":"module/config/#giants.config.ModelEstimatorConfig.GradientBoostingRegressor.staged_predict","text":"Predict regression target at each stage for X. This method allows monitoring (i.e. determine error on testing set) after each stage.","title":"staged_predict()"},{"location":"module/config/#giants.config.ModelEstimatorConfig.GradientBoostingRegressor.staged_predict--parameters","text":"X : {array-like, sparse matrix} of shape (n_samples, n_features) The input samples. Internally, it will be converted to dtype=np.float32 and if a sparse matrix is provided to a sparse csr_matrix .","title":"Parameters"},{"location":"module/config/#giants.config.ModelEstimatorConfig.GradientBoostingRegressor.staged_predict--yields","text":"y : generator of ndarray of shape (n_samples,) The predicted value of the input samples. Source code in giants/config.py def staged_predict ( self , X ): \"\"\"Predict regression target at each stage for X. This method allows monitoring (i.e. determine error on testing set) after each stage. Parameters ---------- X : {array-like, sparse matrix} of shape (n_samples, n_features) The input samples. Internally, it will be converted to ``dtype=np.float32`` and if a sparse matrix is provided to a sparse ``csr_matrix``. Yields ------ y : generator of ndarray of shape (n_samples,) The predicted value of the input samples. \"\"\" for raw_predictions in self . _staged_raw_predict ( X ): yield raw_predictions . ravel ()","title":"Yields"},{"location":"module/config/#giants.config.ModelEstimatorConfig.LinearRegression","text":"Ordinary least squares Linear Regression. LinearRegression fits a linear model with coefficients w = (w1, ..., wp) to minimize the residual sum of squares between the observed targets in the dataset, and the targets predicted by the linear approximation.","title":"LinearRegression"},{"location":"module/config/#giants.config.ModelEstimatorConfig.LinearRegression--parameters","text":"fit_intercept : bool, default=True Whether to calculate the intercept for this model. If set to False, no intercept will be used in calculations (i.e. data is expected to be centered). normalize : bool, default=False This parameter is ignored when fit_intercept is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm. If you wish to standardize, please use :class: ~sklearn.preprocessing.StandardScaler before calling fit on an estimator with normalize=False . .. deprecated:: 1.0 `normalize` was deprecated in version 1.0 and will be removed in 1.2. copy_X : bool, default=True If True, X will be copied; else, it may be overwritten. n_jobs : int, default=None The number of jobs to use for the computation. This will only provide speedup in case of sufficiently large problems, that is if firstly n_targets > 1 and secondly X is sparse or if positive is set to True . None means 1 unless in a :obj: joblib.parallel_backend context. -1 means using all processors. See :term: Glossary <n_jobs> for more details. positive : bool, default=False When set to True , forces the coefficients to be positive. This option is only supported for dense arrays. .. versionadded:: 0.24","title":"Parameters"},{"location":"module/config/#giants.config.ModelEstimatorConfig.LinearRegression--attributes","text":"coef_ : array of shape (n_features, ) or (n_targets, n_features) Estimated coefficients for the linear regression problem. If multiple targets are passed during the fit (y 2D), this is a 2D array of shape (n_targets, n_features), while if only one target is passed, this is a 1D array of length n_features. rank_ : int Rank of matrix X . Only available when X is dense. singular_ : array of shape (min(X, y),) Singular values of X . Only available when X is dense. intercept_ : float or array of shape (n_targets,) Independent term in the linear model. Set to 0.0 if fit_intercept = False . n_features_in_ : int Number of features seen during :term: fit . .. versionadded:: 0.24 feature_names_in_ : ndarray of shape ( n_features_in_ ,) Names of features seen during :term: fit . Defined only when X has feature names that are all strings. .. versionadded:: 1.0","title":"Attributes"},{"location":"module/config/#giants.config.ModelEstimatorConfig.LinearRegression--see-also","text":"Ridge : Ridge regression addresses some of the problems of Ordinary Least Squares by imposing a penalty on the size of the coefficients with l2 regularization. Lasso : The Lasso is a linear model that estimates sparse coefficients with l1 regularization. ElasticNet : Elastic-Net is a linear regression model trained with both l1 and l2 -norm regularization of the coefficients.","title":"See Also"},{"location":"module/config/#giants.config.ModelEstimatorConfig.LinearRegression--notes","text":"From the implementation point of view, this is just plain Ordinary Least Squares (scipy.linalg.lstsq) or Non Negative Least Squares (scipy.optimize.nnls) wrapped as a predictor object.","title":"Notes"},{"location":"module/config/#giants.config.ModelEstimatorConfig.LinearRegression--examples","text":"import numpy as np from sklearn.linear_model import LinearRegression X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])","title":"Examples"},{"location":"module/config/#giants.config.ModelEstimatorConfig.LinearRegression--y-1-x_0-2-x_1-3","text":"y = np.dot(X, np.array([1, 2])) + 3 reg = LinearRegression().fit(X, y) reg.score(X, y) 1.0 reg.coef_ array([1., 2.]) reg.intercept_ 3.0... reg.predict(np.array([[3, 5]])) array([16.])","title":"y = 1 * x_0 + 2 * x_1 + 3"},{"location":"module/config/#giants.config.ModelEstimatorConfig.LinearRegression.fit","text":"Fit linear model.","title":"fit()"},{"location":"module/config/#giants.config.ModelEstimatorConfig.LinearRegression.fit--parameters","text":"X : {array-like, sparse matrix} of shape (n_samples, n_features) Training data. y : array-like of shape (n_samples,) or (n_samples, n_targets) Target values. Will be cast to X's dtype if necessary. sample_weight : array-like of shape (n_samples,), default=None Individual weights for each sample. .. versionadded:: 0.17 parameter *sample_weight* support to LinearRegression.","title":"Parameters"},{"location":"module/config/#giants.config.ModelEstimatorConfig.LinearRegression.fit--returns","text":"self : object Fitted Estimator. Source code in giants/config.py def fit ( self , X , y , sample_weight = None ): \"\"\" Fit linear model. Parameters ---------- X : {array-like, sparse matrix} of shape (n_samples, n_features) Training data. y : array-like of shape (n_samples,) or (n_samples, n_targets) Target values. Will be cast to X's dtype if necessary. sample_weight : array-like of shape (n_samples,), default=None Individual weights for each sample. .. versionadded:: 0.17 parameter *sample_weight* support to LinearRegression. Returns ------- self : object Fitted Estimator. \"\"\" _normalize = _deprecate_normalize ( self . normalize , default = False , estimator_name = self . __class__ . __name__ ) n_jobs_ = self . n_jobs accept_sparse = False if self . positive else [ \"csr\" , \"csc\" , \"coo\" ] X , y = self . _validate_data ( X , y , accept_sparse = accept_sparse , y_numeric = True , multi_output = True ) if sample_weight is not None : sample_weight = _check_sample_weight ( sample_weight , X , dtype = X . dtype ) X , y , X_offset , y_offset , X_scale = self . _preprocess_data ( X , y , fit_intercept = self . fit_intercept , normalize = _normalize , copy = self . copy_X , sample_weight = sample_weight , return_mean = True , ) if sample_weight is not None : # Sample weight can be implemented via a simple rescaling. X , y = _rescale_data ( X , y , sample_weight ) if self . positive : if y . ndim < 2 : self . coef_ , self . _residues = optimize . nnls ( X , y ) else : # scipy.optimize.nnls cannot handle y with shape (M, K) outs = Parallel ( n_jobs = n_jobs_ )( delayed ( optimize . nnls )( X , y [:, j ]) for j in range ( y . shape [ 1 ]) ) self . coef_ , self . _residues = map ( np . vstack , zip ( * outs )) elif sp . issparse ( X ): X_offset_scale = X_offset / X_scale def matvec ( b ): return X . dot ( b ) - b . dot ( X_offset_scale ) def rmatvec ( b ): return X . T . dot ( b ) - X_offset_scale * np . sum ( b ) X_centered = sparse . linalg . LinearOperator ( shape = X . shape , matvec = matvec , rmatvec = rmatvec ) if y . ndim < 2 : out = sparse_lsqr ( X_centered , y ) self . coef_ = out [ 0 ] self . _residues = out [ 3 ] else : # sparse_lstsq cannot handle y with shape (M, K) outs = Parallel ( n_jobs = n_jobs_ )( delayed ( sparse_lsqr )( X_centered , y [:, j ] . ravel ()) for j in range ( y . shape [ 1 ]) ) self . coef_ = np . vstack ([ out [ 0 ] for out in outs ]) self . _residues = np . vstack ([ out [ 3 ] for out in outs ]) else : self . coef_ , self . _residues , self . rank_ , self . singular_ = linalg . lstsq ( X , y ) self . coef_ = self . coef_ . T if y . ndim == 1 : self . coef_ = np . ravel ( self . coef_ ) self . _set_intercept ( X_offset , y_offset , X_scale ) return self","title":"Returns"},{"location":"module/config/#giants.config.ModelEstimatorConfig.LinearSVC","text":"Linear Support Vector Classification. Similar to SVC with parameter kernel='linear', but implemented in terms of liblinear rather than libsvm, so it has more flexibility in the choice of penalties and loss functions and should scale better to large numbers of samples. This class supports both dense and sparse input and the multiclass support is handled according to a one-vs-the-rest scheme. Read more in the :ref: User Guide <svm_classification> .","title":"LinearSVC"},{"location":"module/config/#giants.config.ModelEstimatorConfig.LinearSVC--parameters","text":"penalty : {'l1', 'l2'}, default='l2' Specifies the norm used in the penalization. The 'l2' penalty is the standard used in SVC. The 'l1' leads to coef_ vectors that are sparse. loss : {'hinge', 'squared_hinge'}, default='squared_hinge' Specifies the loss function. 'hinge' is the standard SVM loss (used e.g. by the SVC class) while 'squared_hinge' is the square of the hinge loss. The combination of penalty='l1' and loss='hinge' is not supported. dual : bool, default=True Select the algorithm to either solve the dual or primal optimization problem. Prefer dual=False when n_samples > n_features. tol : float, default=1e-4 Tolerance for stopping criteria. C : float, default=1.0 Regularization parameter. The strength of the regularization is inversely proportional to C. Must be strictly positive. multi_class : {'ovr', 'crammer_singer'}, default='ovr' Determines the multi-class strategy if y contains more than two classes. \"ovr\" trains n_classes one-vs-rest classifiers, while \"crammer_singer\" optimizes a joint objective over all classes. While crammer_singer is interesting from a theoretical perspective as it is consistent, it is seldom used in practice as it rarely leads to better accuracy and is more expensive to compute. If \"crammer_singer\" is chosen, the options loss, penalty and dual will be ignored. fit_intercept : bool, default=True Whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (i.e. data is expected to be already centered). intercept_scaling : float, default=1 When self.fit_intercept is True, instance vector x becomes [x, self.intercept_scaling] , i.e. a \"synthetic\" feature with constant value equals to intercept_scaling is appended to the instance vector. The intercept becomes intercept_scaling * synthetic feature weight Note! the synthetic feature weight is subject to l1/l2 regularization as all other features. To lessen the effect of regularization on synthetic feature weight (and therefore on the intercept) intercept_scaling has to be increased. class_weight : dict or 'balanced', default=None Set the parameter C of class i to class_weight[i]*C for SVC. If not given, all classes are supposed to have weight one. The \"balanced\" mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as n_samples / (n_classes * np.bincount(y)) . verbose : int, default=0 Enable verbose output. Note that this setting takes advantage of a per-process runtime setting in liblinear that, if enabled, may not work properly in a multithreaded context. random_state : int, RandomState instance or None, default=None Controls the pseudo random number generation for shuffling the data for the dual coordinate descent (if dual=True ). When dual=False the underlying implementation of :class: LinearSVC is not random and random_state has no effect on the results. Pass an int for reproducible output across multiple function calls. See :term: Glossary <random_state> . max_iter : int, default=1000 The maximum number of iterations to be run.","title":"Parameters"},{"location":"module/config/#giants.config.ModelEstimatorConfig.LinearSVC--attributes","text":"coef_ : ndarray of shape (1, n_features) if n_classes == 2 else (n_classes, n_features) Weights assigned to the features (coefficients in the primal problem). ``coef_`` is a readonly property derived from ``raw_coef_`` that follows the internal memory layout of liblinear. intercept_ : ndarray of shape (1,) if n_classes == 2 else (n_classes,) Constants in decision function. classes_ : ndarray of shape (n_classes,) The unique classes labels. n_features_in_ : int Number of features seen during :term: fit . .. versionadded:: 0.24 feature_names_in_ : ndarray of shape ( n_features_in_ ,) Names of features seen during :term: fit . Defined only when X has feature names that are all strings. .. versionadded:: 1.0 n_iter_ : int Maximum number of iterations run across all classes.","title":"Attributes"},{"location":"module/config/#giants.config.ModelEstimatorConfig.LinearSVC--see-also","text":"SVC : Implementation of Support Vector Machine classifier using libsvm: the kernel can be non-linear but its SMO algorithm does not scale to large number of samples as LinearSVC does. Furthermore SVC multi-class mode is implemented using one vs one scheme while LinearSVC uses one vs the rest. It is possible to implement one vs the rest with SVC by using the :class:`~sklearn.multiclass.OneVsRestClassifier` wrapper. Finally SVC can fit dense data without memory copy if the input is C-contiguous. Sparse data will still incur memory copy though. sklearn.linear_model.SGDClassifier : SGDClassifier can optimize the same cost function as LinearSVC by adjusting the penalty and loss parameters. In addition it requires less memory, allows incremental (online) learning, and implements various loss functions and regularization regimes.","title":"See Also"},{"location":"module/config/#giants.config.ModelEstimatorConfig.LinearSVC--notes","text":"The underlying C implementation uses a random number generator to select features when fitting the model. It is thus not uncommon to have slightly different results for the same input data. If that happens, try with a smaller tol parameter. The underlying implementation, liblinear, uses a sparse internal representation for the data that will incur a memory copy. Predict output may not match that of standalone liblinear in certain cases. See :ref: differences from liblinear <liblinear_differences> in the narrative documentation.","title":"Notes"},{"location":"module/config/#giants.config.ModelEstimatorConfig.LinearSVC--references","text":"LIBLINEAR: A Library for Large Linear Classification <https://www.csie.ntu.edu.tw/~cjlin/liblinear/> __","title":"References"},{"location":"module/config/#giants.config.ModelEstimatorConfig.LinearSVC--examples","text":"from sklearn.svm import LinearSVC from sklearn.pipeline import make_pipeline from sklearn.preprocessing import StandardScaler from sklearn.datasets import make_classification X, y = make_classification(n_features=4, random_state=0) clf = make_pipeline(StandardScaler(), ... LinearSVC(random_state=0, tol=1e-5)) clf.fit(X, y) Pipeline(steps=[('standardscaler', StandardScaler()), ('linearsvc', LinearSVC(random_state=0, tol=1e-05))]) print(clf.named_steps['linearsvc'].coef_) [[0.141... 0.526... 0.679... 0.493...]] print(clf.named_steps['linearsvc'].intercept_) [0.1693...] print(clf.predict([[0, 0, 0, 0]])) [1]","title":"Examples"},{"location":"module/config/#giants.config.ModelEstimatorConfig.LinearSVC.fit","text":"Fit the model according to the given training data.","title":"fit()"},{"location":"module/config/#giants.config.ModelEstimatorConfig.LinearSVC.fit--parameters","text":"X : {array-like, sparse matrix} of shape (n_samples, n_features) Training vector, where n_samples is the number of samples and n_features is the number of features. y : array-like of shape (n_samples,) Target vector relative to X. sample_weight : array-like of shape (n_samples,), default=None Array of weights that are assigned to individual samples. If not provided, then each sample is given unit weight. .. versionadded:: 0.18","title":"Parameters"},{"location":"module/config/#giants.config.ModelEstimatorConfig.LinearSVC.fit--returns","text":"self : object An instance of the estimator. Source code in giants/config.py def fit ( self , X , y , sample_weight = None ): \"\"\"Fit the model according to the given training data. Parameters ---------- X : {array-like, sparse matrix} of shape (n_samples, n_features) Training vector, where `n_samples` is the number of samples and `n_features` is the number of features. y : array-like of shape (n_samples,) Target vector relative to X. sample_weight : array-like of shape (n_samples,), default=None Array of weights that are assigned to individual samples. If not provided, then each sample is given unit weight. .. versionadded:: 0.18 Returns ------- self : object An instance of the estimator. \"\"\" if self . C < 0 : raise ValueError ( \"Penalty term must be positive; got (C= %r )\" % self . C ) X , y = self . _validate_data ( X , y , accept_sparse = \"csr\" , dtype = np . float64 , order = \"C\" , accept_large_sparse = False , ) check_classification_targets ( y ) self . classes_ = np . unique ( y ) self . coef_ , self . intercept_ , self . n_iter_ = _fit_liblinear ( X , y , self . C , self . fit_intercept , self . intercept_scaling , self . class_weight , self . penalty , self . dual , self . verbose , self . max_iter , self . tol , self . random_state , self . multi_class , self . loss , sample_weight = sample_weight , ) if self . multi_class == \"crammer_singer\" and len ( self . classes_ ) == 2 : self . coef_ = ( self . coef_ [ 1 ] - self . coef_ [ 0 ]) . reshape ( 1 , - 1 ) if self . fit_intercept : intercept = self . intercept_ [ 1 ] - self . intercept_ [ 0 ] self . intercept_ = np . array ([ intercept ]) return self","title":"Returns"},{"location":"module/config/#giants.config.ModelEstimatorConfig.LinearSVR","text":"Linear Support Vector Regression. Similar to SVR with parameter kernel='linear', but implemented in terms of liblinear rather than libsvm, so it has more flexibility in the choice of penalties and loss functions and should scale better to large numbers of samples. This class supports both dense and sparse input. Read more in the :ref: User Guide <svm_regression> . .. versionadded:: 0.16","title":"LinearSVR"},{"location":"module/config/#giants.config.ModelEstimatorConfig.LinearSVR--parameters","text":"epsilon : float, default=0.0 Epsilon parameter in the epsilon-insensitive loss function. Note that the value of this parameter depends on the scale of the target variable y. If unsure, set epsilon=0 . tol : float, default=1e-4 Tolerance for stopping criteria. C : float, default=1.0 Regularization parameter. The strength of the regularization is inversely proportional to C. Must be strictly positive. loss : {'epsilon_insensitive', 'squared_epsilon_insensitive'}, default='epsilon_insensitive' Specifies the loss function. The epsilon-insensitive loss (standard SVR) is the L1 loss, while the squared epsilon-insensitive loss ('squared_epsilon_insensitive') is the L2 loss. fit_intercept : bool, default=True Whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (i.e. data is expected to be already centered). intercept_scaling : float, default=1.0 When self.fit_intercept is True, instance vector x becomes [x, self.intercept_scaling], i.e. a \"synthetic\" feature with constant value equals to intercept_scaling is appended to the instance vector. The intercept becomes intercept_scaling * synthetic feature weight Note! the synthetic feature weight is subject to l1/l2 regularization as all other features. To lessen the effect of regularization on synthetic feature weight (and therefore on the intercept) intercept_scaling has to be increased. dual : bool, default=True Select the algorithm to either solve the dual or primal optimization problem. Prefer dual=False when n_samples > n_features. verbose : int, default=0 Enable verbose output. Note that this setting takes advantage of a per-process runtime setting in liblinear that, if enabled, may not work properly in a multithreaded context. random_state : int, RandomState instance or None, default=None Controls the pseudo random number generation for shuffling the data. Pass an int for reproducible output across multiple function calls. See :term: Glossary <random_state> . max_iter : int, default=1000 The maximum number of iterations to be run.","title":"Parameters"},{"location":"module/config/#giants.config.ModelEstimatorConfig.LinearSVR--attributes","text":"coef_ : ndarray of shape (n_features) if n_classes == 2 else (n_classes, n_features) Weights assigned to the features (coefficients in the primal problem). `coef_` is a readonly property derived from `raw_coef_` that follows the internal memory layout of liblinear. intercept_ : ndarray of shape (1) if n_classes == 2 else (n_classes) Constants in decision function. n_features_in_ : int Number of features seen during :term: fit . .. versionadded:: 0.24 feature_names_in_ : ndarray of shape ( n_features_in_ ,) Names of features seen during :term: fit . Defined only when X has feature names that are all strings. .. versionadded:: 1.0 n_iter_ : int Maximum number of iterations run across all classes.","title":"Attributes"},{"location":"module/config/#giants.config.ModelEstimatorConfig.LinearSVR--see-also","text":"LinearSVC : Implementation of Support Vector Machine classifier using the same library as this class (liblinear). SVR : Implementation of Support Vector Machine regression using libsvm: the kernel can be non-linear but its SMO algorithm does not scale to large number of samples as LinearSVC does. sklearn.linear_model.SGDRegressor : SGDRegressor can optimize the same cost function as LinearSVR by adjusting the penalty and loss parameters. In addition it requires less memory, allows incremental (online) learning, and implements various loss functions and regularization regimes.","title":"See Also"},{"location":"module/config/#giants.config.ModelEstimatorConfig.LinearSVR--examples","text":"from sklearn.svm import LinearSVR from sklearn.pipeline import make_pipeline from sklearn.preprocessing import StandardScaler from sklearn.datasets import make_regression X, y = make_regression(n_features=4, random_state=0) regr = make_pipeline(StandardScaler(), ... LinearSVR(random_state=0, tol=1e-5)) regr.fit(X, y) Pipeline(steps=[('standardscaler', StandardScaler()), ('linearsvr', LinearSVR(random_state=0, tol=1e-05))]) print(regr.named_steps['linearsvr'].coef_) [18.582... 27.023... 44.357... 64.522...] print(regr.named_steps['linearsvr'].intercept_) [-4...] print(regr.predict([[0, 0, 0, 0]])) [-2.384...]","title":"Examples"},{"location":"module/config/#giants.config.ModelEstimatorConfig.LinearSVR.fit","text":"Fit the model according to the given training data.","title":"fit()"},{"location":"module/config/#giants.config.ModelEstimatorConfig.LinearSVR.fit--parameters","text":"X : {array-like, sparse matrix} of shape (n_samples, n_features) Training vector, where n_samples is the number of samples and n_features is the number of features. y : array-like of shape (n_samples,) Target vector relative to X. sample_weight : array-like of shape (n_samples,), default=None Array of weights that are assigned to individual samples. If not provided, then each sample is given unit weight. .. versionadded:: 0.18","title":"Parameters"},{"location":"module/config/#giants.config.ModelEstimatorConfig.LinearSVR.fit--returns","text":"self : object An instance of the estimator. Source code in giants/config.py def fit ( self , X , y , sample_weight = None ): \"\"\"Fit the model according to the given training data. Parameters ---------- X : {array-like, sparse matrix} of shape (n_samples, n_features) Training vector, where `n_samples` is the number of samples and `n_features` is the number of features. y : array-like of shape (n_samples,) Target vector relative to X. sample_weight : array-like of shape (n_samples,), default=None Array of weights that are assigned to individual samples. If not provided, then each sample is given unit weight. .. versionadded:: 0.18 Returns ------- self : object An instance of the estimator. \"\"\" if self . C < 0 : raise ValueError ( \"Penalty term must be positive; got (C= %r )\" % self . C ) X , y = self . _validate_data ( X , y , accept_sparse = \"csr\" , dtype = np . float64 , order = \"C\" , accept_large_sparse = False , ) penalty = \"l2\" # SVR only accepts l2 penalty self . coef_ , self . intercept_ , self . n_iter_ = _fit_liblinear ( X , y , self . C , self . fit_intercept , self . intercept_scaling , None , penalty , self . dual , self . verbose , self . max_iter , self . tol , self . random_state , loss = self . loss , epsilon = self . epsilon , sample_weight = sample_weight , ) self . coef_ = self . coef_ . ravel () return self","title":"Returns"},{"location":"module/config/#giants.config.ModelEstimatorConfig.LogisticRegression","text":"Logistic Regression (aka logit, MaxEnt) classifier. In the multiclass case, the training algorithm uses the one-vs-rest (OvR) scheme if the 'multi_class' option is set to 'ovr', and uses the cross-entropy loss if the 'multi_class' option is set to 'multinomial'. (Currently the 'multinomial' option is supported only by the 'lbfgs', 'sag', 'saga' and 'newton-cg' solvers.) This class implements regularized logistic regression using the 'liblinear' library, 'newton-cg', 'sag', 'saga' and 'lbfgs' solvers. Note that regularization is applied by default . It can handle both dense and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit floats for optimal performance; any other input format will be converted (and copied). The 'newton-cg', 'sag', and 'lbfgs' solvers support only L2 regularization with primal formulation, or no regularization. The 'liblinear' solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty. The Elastic-Net regularization is only supported by the 'saga' solver. Read more in the :ref: User Guide <logistic_regression> .","title":"LogisticRegression"},{"location":"module/config/#giants.config.ModelEstimatorConfig.LogisticRegression--parameters","text":"penalty : {'l1', 'l2', 'elasticnet', 'none'}, default='l2' Specify the norm of the penalty: - `'none'`: no penalty is added; - `'l2'`: add a L2 penalty term and it is the default choice; - `'l1'`: add a L1 penalty term; - `'elasticnet'`: both L1 and L2 penalty terms are added. .. warning:: Some penalties may not work with some solvers. See the parameter `solver` below, to know the compatibility between the penalty and solver. .. versionadded:: 0.19 l1 penalty with SAGA solver (allowing 'multinomial' + L1) dual : bool, default=False Dual or primal formulation. Dual formulation is only implemented for l2 penalty with liblinear solver. Prefer dual=False when n_samples > n_features. tol : float, default=1e-4 Tolerance for stopping criteria. C : float, default=1.0 Inverse of regularization strength; must be a positive float. Like in support vector machines, smaller values specify stronger regularization. fit_intercept : bool, default=True Specifies if a constant (a.k.a. bias or intercept) should be added to the decision function. intercept_scaling : float, default=1 Useful only when the solver 'liblinear' is used and self.fit_intercept is set to True. In this case, x becomes [x, self.intercept_scaling], i.e. a \"synthetic\" feature with constant value equal to intercept_scaling is appended to the instance vector. The intercept becomes intercept_scaling * synthetic_feature_weight . Note! the synthetic feature weight is subject to l1/l2 regularization as all other features. To lessen the effect of regularization on synthetic feature weight (and therefore on the intercept) intercept_scaling has to be increased. class_weight : dict or 'balanced', default=None Weights associated with classes in the form {class_label: weight} . If not given, all classes are supposed to have weight one. The \"balanced\" mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as ``n_samples / (n_classes * np.bincount(y))``. Note that these weights will be multiplied with sample_weight (passed through the fit method) if sample_weight is specified. .. versionadded:: 0.17 *class_weight='balanced'* random_state : int, RandomState instance, default=None Used when solver == 'sag', 'saga' or 'liblinear' to shuffle the data. See :term: Glossary <random_state> for details. solver : {'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'}, default='lbfgs' Algorithm to use in the optimization problem. Default is 'lbfgs'. To choose a solver, you might want to consider the following aspects: - For small datasets, 'liblinear' is a good choice, whereas 'sag' and 'saga' are faster for large ones; - For multiclass problems, only 'newton-cg', 'sag', 'saga' and 'lbfgs' handle multinomial loss; - 'liblinear' is limited to one-versus-rest schemes. .. warning:: The choice of the algorithm depends on the penalty chosen: Supported penalties by solver: - 'newton-cg' - ['l2', 'none'] - 'lbfgs' - ['l2', 'none'] - 'liblinear' - ['l1', 'l2'] - 'sag' - ['l2', 'none'] - 'saga' - ['elasticnet', 'l1', 'l2', 'none'] .. note:: 'sag' and 'saga' fast convergence is only guaranteed on features with approximately the same scale. You can preprocess the data with a scaler from :mod:`sklearn.preprocessing`. .. seealso:: Refer to the User Guide for more information regarding :class:`LogisticRegression` and more specifically the `Table <https://scikit-learn.org/dev/modules/linear_model.html#logistic-regression>`_ summarazing solver/penalty supports. <!-- # noqa: E501 --> .. versionadded:: 0.17 Stochastic Average Gradient descent solver. .. versionadded:: 0.19 SAGA solver. .. versionchanged:: 0.22 The default solver changed from 'liblinear' to 'lbfgs' in 0.22. max_iter : int, default=100 Maximum number of iterations taken for the solvers to converge. multi_class : {'auto', 'ovr', 'multinomial'}, default='auto' If the option chosen is 'ovr', then a binary problem is fit for each label. For 'multinomial' the loss minimised is the multinomial loss fit across the entire probability distribution, even when the data is binary . 'multinomial' is unavailable when solver='liblinear'. 'auto' selects 'ovr' if the data is binary, or if solver='liblinear', and otherwise selects 'multinomial'. .. versionadded:: 0.18 Stochastic Average Gradient descent solver for 'multinomial' case. .. versionchanged:: 0.22 Default changed from 'ovr' to 'auto' in 0.22. verbose : int, default=0 For the liblinear and lbfgs solvers set verbose to any positive number for verbosity. warm_start : bool, default=False When set to True, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. Useless for liblinear solver. See :term: the Glossary <warm_start> . .. versionadded:: 0.17 *warm_start* to support *lbfgs*, *newton-cg*, *sag*, *saga* solvers. n_jobs : int, default=None Number of CPU cores used when parallelizing over classes if multi_class='ovr'\". This parameter is ignored when the solver is set to 'liblinear' regardless of whether 'multi_class' is specified or not. None means 1 unless in a :obj: joblib.parallel_backend context. -1 means using all processors. See :term: Glossary <n_jobs> for more details. l1_ratio : float, default=None The Elastic-Net mixing parameter, with 0 <= l1_ratio <= 1 . Only used if penalty='elasticnet' . Setting l1_ratio=0 is equivalent to using penalty='l2' , while setting l1_ratio=1 is equivalent to using penalty='l1' . For 0 < l1_ratio <1 , the penalty is a combination of L1 and L2.","title":"Parameters"},{"location":"module/config/#giants.config.ModelEstimatorConfig.LogisticRegression--attributes","text":"classes_ : ndarray of shape (n_classes, ) A list of class labels known to the classifier. coef_ : ndarray of shape (1, n_features) or (n_classes, n_features) Coefficient of the features in the decision function. `coef_` is of shape (1, n_features) when the given problem is binary. In particular, when `multi_class='multinomial'`, `coef_` corresponds to outcome 1 (True) and `-coef_` corresponds to outcome 0 (False). intercept_ : ndarray of shape (1,) or (n_classes,) Intercept (a.k.a. bias) added to the decision function. If `fit_intercept` is set to False, the intercept is set to zero. `intercept_` is of shape (1,) when the given problem is binary. In particular, when `multi_class='multinomial'`, `intercept_` corresponds to outcome 1 (True) and `-intercept_` corresponds to outcome 0 (False). n_features_in_ : int Number of features seen during :term: fit . .. versionadded:: 0.24 feature_names_in_ : ndarray of shape ( n_features_in_ ,) Names of features seen during :term: fit . Defined only when X has feature names that are all strings. .. versionadded:: 1.0 n_iter_ : ndarray of shape (n_classes,) or (1, ) Actual number of iterations for all classes. If binary or multinomial, it returns only 1 element. For liblinear solver, only the maximum number of iteration across all classes is given. .. versionchanged:: 0.20 In SciPy <= 1.0.0 the number of lbfgs iterations may exceed ``max_iter``. ``n_iter_`` will now report at most ``max_iter``.","title":"Attributes"},{"location":"module/config/#giants.config.ModelEstimatorConfig.LogisticRegression--see-also","text":"SGDClassifier : Incrementally trained logistic regression (when given the parameter loss=\"log\" ). LogisticRegressionCV : Logistic regression with built-in cross validation.","title":"See Also"},{"location":"module/config/#giants.config.ModelEstimatorConfig.LogisticRegression--notes","text":"The underlying C implementation uses a random number generator to select features when fitting the model. It is thus not uncommon, to have slightly different results for the same input data. If that happens, try with a smaller tol parameter. Predict output may not match that of standalone liblinear in certain cases. See :ref: differences from liblinear <liblinear_differences> in the narrative documentation.","title":"Notes"},{"location":"module/config/#giants.config.ModelEstimatorConfig.LogisticRegression--references","text":"L-BFGS-B -- Software for Large-scale Bound-constrained Optimization Ciyou Zhu, Richard Byrd, Jorge Nocedal and Jose Luis Morales. users.iems.northwestern.edu/~nocedal/lbfgsb.html LIBLINEAR -- A Library for Large Linear Classification www.csie.ntu.edu.tw/~cjlin/liblinear/ SAG -- Mark Schmidt, Nicolas Le Roux, and Francis Bach Minimizing Finite Sums with the Stochastic Average Gradient hal.inria.fr/hal-00860051/document SAGA -- Defazio, A., Bach F. & Lacoste-Julien S. (2014). SAGA: A Fast Incremental Gradient Method With Support for Non-Strongly Convex Composite Objectives arxiv.org/abs/1407.0202 Hsiang-Fu Yu, Fang-Lan Huang, Chih-Jen Lin (2011). Dual coordinate descent methods for logistic regression and maximum entropy models. Machine Learning 85(1-2):41-75. www.csie.ntu.edu.tw/~cjlin/papers/maxent_dual.pdf","title":"References"},{"location":"module/config/#giants.config.ModelEstimatorConfig.LogisticRegression--examples","text":"from sklearn.datasets import load_iris from sklearn.linear_model import LogisticRegression X, y = load_iris(return_X_y=True) clf = LogisticRegression(random_state=0).fit(X, y) clf.predict(X[:2, :]) array([0, 0]) clf.predict_proba(X[:2, :]) array([[9.8...e-01, 1.8...e-02, 1.4...e-08], [9.7...e-01, 2.8...e-02, ...e-08]]) clf.score(X, y) 0.97...","title":"Examples"},{"location":"module/config/#giants.config.ModelEstimatorConfig.LogisticRegression.fit","text":"Fit the model according to the given training data.","title":"fit()"},{"location":"module/config/#giants.config.ModelEstimatorConfig.LogisticRegression.fit--parameters","text":"X : {array-like, sparse matrix} of shape (n_samples, n_features) Training vector, where n_samples is the number of samples and n_features is the number of features. y : array-like of shape (n_samples,) Target vector relative to X. sample_weight : array-like of shape (n_samples,) default=None Array of weights that are assigned to individual samples. If not provided, then each sample is given unit weight. .. versionadded:: 0.17 *sample_weight* support to LogisticRegression.","title":"Parameters"},{"location":"module/config/#giants.config.ModelEstimatorConfig.LogisticRegression.fit--returns","text":"self Fitted estimator.","title":"Returns"},{"location":"module/config/#giants.config.ModelEstimatorConfig.LogisticRegression.fit--notes","text":"The SAGA solver supports both float64 and float32 bit arrays. Source code in giants/config.py def fit ( self , X , y , sample_weight = None ): \"\"\" Fit the model according to the given training data. Parameters ---------- X : {array-like, sparse matrix} of shape (n_samples, n_features) Training vector, where `n_samples` is the number of samples and `n_features` is the number of features. y : array-like of shape (n_samples,) Target vector relative to X. sample_weight : array-like of shape (n_samples,) default=None Array of weights that are assigned to individual samples. If not provided, then each sample is given unit weight. .. versionadded:: 0.17 *sample_weight* support to LogisticRegression. Returns ------- self Fitted estimator. Notes ----- The SAGA solver supports both float64 and float32 bit arrays. \"\"\" solver = _check_solver ( self . solver , self . penalty , self . dual ) if not isinstance ( self . C , numbers . Number ) or self . C < 0 : raise ValueError ( \"Penalty term must be positive; got (C= %r )\" % self . C ) if self . penalty == \"elasticnet\" : if ( not isinstance ( self . l1_ratio , numbers . Number ) or self . l1_ratio < 0 or self . l1_ratio > 1 ): raise ValueError ( \"l1_ratio must be between 0 and 1; got (l1_ratio= %r )\" % self . l1_ratio ) elif self . l1_ratio is not None : warnings . warn ( \"l1_ratio parameter is only used when penalty is \" \"'elasticnet'. Got \" \"(penalty= {} )\" . format ( self . penalty ) ) if self . penalty == \"none\" : if self . C != 1.0 : # default values warnings . warn ( \"Setting penalty='none' will ignore the C and l1_ratio parameters\" ) # Note that check for l1_ratio is done right above C_ = np . inf penalty = \"l2\" else : C_ = self . C penalty = self . penalty if not isinstance ( self . max_iter , numbers . Number ) or self . max_iter < 0 : raise ValueError ( \"Maximum number of iteration must be positive; got (max_iter= %r )\" % self . max_iter ) if not isinstance ( self . tol , numbers . Number ) or self . tol < 0 : raise ValueError ( \"Tolerance for stopping criteria must be positive; got (tol= %r )\" % self . tol ) if solver == \"lbfgs\" : _dtype = np . float64 else : _dtype = [ np . float64 , np . float32 ] X , y = self . _validate_data ( X , y , accept_sparse = \"csr\" , dtype = _dtype , order = \"C\" , accept_large_sparse = solver not in [ \"liblinear\" , \"sag\" , \"saga\" ], ) check_classification_targets ( y ) self . classes_ = np . unique ( y ) multi_class = _check_multi_class ( self . multi_class , solver , len ( self . classes_ )) if solver == \"liblinear\" : if effective_n_jobs ( self . n_jobs ) != 1 : warnings . warn ( \"'n_jobs' > 1 does not have any effect when\" \" 'solver' is set to 'liblinear'. Got 'n_jobs'\" \" = {} .\" . format ( effective_n_jobs ( self . n_jobs )) ) self . coef_ , self . intercept_ , n_iter_ = _fit_liblinear ( X , y , self . C , self . fit_intercept , self . intercept_scaling , self . class_weight , self . penalty , self . dual , self . verbose , self . max_iter , self . tol , self . random_state , sample_weight = sample_weight , ) self . n_iter_ = np . array ([ n_iter_ ]) return self if solver in [ \"sag\" , \"saga\" ]: max_squared_sum = row_norms ( X , squared = True ) . max () else : max_squared_sum = None n_classes = len ( self . classes_ ) classes_ = self . classes_ if n_classes < 2 : raise ValueError ( \"This solver needs samples of at least 2 classes\" \" in the data, but the data contains only one\" \" class: %r \" % classes_ [ 0 ] ) if len ( self . classes_ ) == 2 : n_classes = 1 classes_ = classes_ [ 1 :] if self . warm_start : warm_start_coef = getattr ( self , \"coef_\" , None ) else : warm_start_coef = None if warm_start_coef is not None and self . fit_intercept : warm_start_coef = np . append ( warm_start_coef , self . intercept_ [:, np . newaxis ], axis = 1 ) # Hack so that we iterate only once for the multinomial case. if multi_class == \"multinomial\" : classes_ = [ None ] warm_start_coef = [ warm_start_coef ] if warm_start_coef is None : warm_start_coef = [ None ] * n_classes path_func = delayed ( _logistic_regression_path ) # The SAG solver releases the GIL so it's more efficient to use # threads for this solver. if solver in [ \"sag\" , \"saga\" ]: prefer = \"threads\" else : prefer = \"processes\" fold_coefs_ = Parallel ( n_jobs = self . n_jobs , verbose = self . verbose , ** _joblib_parallel_args ( prefer = prefer ), )( path_func ( X , y , pos_class = class_ , Cs = [ C_ ], l1_ratio = self . l1_ratio , fit_intercept = self . fit_intercept , tol = self . tol , verbose = self . verbose , solver = solver , multi_class = multi_class , max_iter = self . max_iter , class_weight = self . class_weight , check_input = False , random_state = self . random_state , coef = warm_start_coef_ , penalty = penalty , max_squared_sum = max_squared_sum , sample_weight = sample_weight , ) for class_ , warm_start_coef_ in zip ( classes_ , warm_start_coef ) ) fold_coefs_ , _ , n_iter_ = zip ( * fold_coefs_ ) self . n_iter_ = np . asarray ( n_iter_ , dtype = np . int32 )[:, 0 ] n_features = X . shape [ 1 ] if multi_class == \"multinomial\" : self . coef_ = fold_coefs_ [ 0 ][ 0 ] else : self . coef_ = np . asarray ( fold_coefs_ ) self . coef_ = self . coef_ . reshape ( n_classes , n_features + int ( self . fit_intercept ) ) if self . fit_intercept : self . intercept_ = self . coef_ [:, - 1 ] self . coef_ = self . coef_ [:, : - 1 ] else : self . intercept_ = np . zeros ( n_classes ) return self","title":"Notes"},{"location":"module/config/#giants.config.ModelEstimatorConfig.LogisticRegression.predict_log_proba","text":"Predict logarithm of probability estimates. The returned estimates for all classes are ordered by the label of classes.","title":"predict_log_proba()"},{"location":"module/config/#giants.config.ModelEstimatorConfig.LogisticRegression.predict_log_proba--parameters","text":"X : array-like of shape (n_samples, n_features) Vector to be scored, where n_samples is the number of samples and n_features is the number of features.","title":"Parameters"},{"location":"module/config/#giants.config.ModelEstimatorConfig.LogisticRegression.predict_log_proba--returns","text":"T : array-like of shape (n_samples, n_classes) Returns the log-probability of the sample for each class in the model, where classes are ordered as they are in self.classes_ . Source code in giants/config.py def predict_log_proba ( self , X ): \"\"\" Predict logarithm of probability estimates. The returned estimates for all classes are ordered by the label of classes. Parameters ---------- X : array-like of shape (n_samples, n_features) Vector to be scored, where `n_samples` is the number of samples and `n_features` is the number of features. Returns ------- T : array-like of shape (n_samples, n_classes) Returns the log-probability of the sample for each class in the model, where classes are ordered as they are in ``self.classes_``. \"\"\" return np . log ( self . predict_proba ( X ))","title":"Returns"},{"location":"module/config/#giants.config.ModelEstimatorConfig.LogisticRegression.predict_proba","text":"Probability estimates. The returned estimates for all classes are ordered by the label of classes. For a multi_class problem, if multi_class is set to be \"multinomial\" the softmax function is used to find the predicted probability of each class. Else use a one-vs-rest approach, i.e calculate the probability of each class assuming it to be positive using the logistic function. and normalize these values across all the classes.","title":"predict_proba()"},{"location":"module/config/#giants.config.ModelEstimatorConfig.LogisticRegression.predict_proba--parameters","text":"X : array-like of shape (n_samples, n_features) Vector to be scored, where n_samples is the number of samples and n_features is the number of features.","title":"Parameters"},{"location":"module/config/#giants.config.ModelEstimatorConfig.LogisticRegression.predict_proba--returns","text":"T : array-like of shape (n_samples, n_classes) Returns the probability of the sample for each class in the model, where classes are ordered as they are in self.classes_ . Source code in giants/config.py def predict_proba ( self , X ): \"\"\" Probability estimates. The returned estimates for all classes are ordered by the label of classes. For a multi_class problem, if multi_class is set to be \"multinomial\" the softmax function is used to find the predicted probability of each class. Else use a one-vs-rest approach, i.e calculate the probability of each class assuming it to be positive using the logistic function. and normalize these values across all the classes. Parameters ---------- X : array-like of shape (n_samples, n_features) Vector to be scored, where `n_samples` is the number of samples and `n_features` is the number of features. Returns ------- T : array-like of shape (n_samples, n_classes) Returns the probability of the sample for each class in the model, where classes are ordered as they are in ``self.classes_``. \"\"\" check_is_fitted ( self ) ovr = self . multi_class in [ \"ovr\" , \"warn\" ] or ( self . multi_class == \"auto\" and ( self . classes_ . size <= 2 or self . solver == \"liblinear\" ) ) if ovr : return super () . _predict_proba_lr ( X ) else : decision = self . decision_function ( X ) if decision . ndim == 1 : # Workaround for multi_class=\"multinomial\" and binary outcomes # which requires softmax prediction with only a 1D decision. decision_2d = np . c_ [ - decision , decision ] else : decision_2d = decision return softmax ( decision_2d , copy = False )","title":"Returns"},{"location":"module/config/#giants.config.ModelEstimatorConfig.RandomForestClassifier","text":"A random forest classifier. A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is controlled with the max_samples parameter if bootstrap=True (default), otherwise the whole dataset is used to build each tree. Read more in the :ref: User Guide <forest> .","title":"RandomForestClassifier"},{"location":"module/config/#giants.config.ModelEstimatorConfig.RandomForestClassifier--parameters","text":"n_estimators : int, default=100 The number of trees in the forest. .. versionchanged:: 0.22 The default value of ``n_estimators`` changed from 10 to 100 in 0.22. criterion : {\"gini\", \"entropy\"}, default=\"gini\" The function to measure the quality of a split. Supported criteria are \"gini\" for the Gini impurity and \"entropy\" for the information gain. Note: this parameter is tree-specific. max_depth : int, default=None The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples. min_samples_split : int or float, default=2 The minimum number of samples required to split an internal node: - If int, then consider `min_samples_split` as the minimum number. - If float, then `min_samples_split` is a fraction and `ceil(min_samples_split * n_samples)` are the minimum number of samples for each split. .. versionchanged:: 0.18 Added float values for fractions. min_samples_leaf : int or float, default=1 The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least min_samples_leaf training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression. - If int, then consider `min_samples_leaf` as the minimum number. - If float, then `min_samples_leaf` is a fraction and `ceil(min_samples_leaf * n_samples)` are the minimum number of samples for each node. .. versionchanged:: 0.18 Added float values for fractions. min_weight_fraction_leaf : float, default=0.0 The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided. max_features : {\"auto\", \"sqrt\", \"log2\"}, int or float, default=\"auto\" The number of features to consider when looking for the best split: - If int, then consider `max_features` features at each split. - If float, then `max_features` is a fraction and `round(max_features * n_features)` features are considered at each split. - If \"auto\", then `max_features=sqrt(n_features)`. - If \"sqrt\", then `max_features=sqrt(n_features)` (same as \"auto\"). - If \"log2\", then `max_features=log2(n_features)`. - If None, then `max_features=n_features`. Note: the search for a split does not stop until at least one valid partition of the node samples is found, even if it requires to effectively inspect more than ``max_features`` features. max_leaf_nodes : int, default=None Grow trees with max_leaf_nodes in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes. min_impurity_decrease : float, default=0.0 A node will be split if this split induces a decrease of the impurity greater than or equal to this value. The weighted impurity decrease equation is the following:: N_t / N * (impurity - N_t_R / N_t * right_impurity - N_t_L / N_t * left_impurity) where ``N`` is the total number of samples, ``N_t`` is the number of samples at the current node, ``N_t_L`` is the number of samples in the left child, and ``N_t_R`` is the number of samples in the right child. ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum, if ``sample_weight`` is passed. .. versionadded:: 0.19 bootstrap : bool, default=True Whether bootstrap samples are used when building trees. If False, the whole dataset is used to build each tree. oob_score : bool, default=False Whether to use out-of-bag samples to estimate the generalization score. Only available if bootstrap=True. n_jobs : int, default=None The number of jobs to run in parallel. :meth: fit , :meth: predict , :meth: decision_path and :meth: apply are all parallelized over the trees. None means 1 unless in a :obj: joblib.parallel_backend context. -1 means using all processors. See :term: Glossary <n_jobs> for more details. random_state : int, RandomState instance or None, default=None Controls both the randomness of the bootstrapping of the samples used when building trees (if bootstrap=True ) and the sampling of the features to consider when looking for the best split at each node (if max_features < n_features ). See :term: Glossary <random_state> for details. verbose : int, default=0 Controls the verbosity when fitting and predicting. warm_start : bool, default=False When set to True , reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just fit a whole new forest. See :term: the Glossary <warm_start> . class_weight : {\"balanced\", \"balanced_subsample\"}, dict or list of dicts, default=None Weights associated with classes in the form {class_label: weight} . If not given, all classes are supposed to have weight one. For multi-output problems, a list of dicts can be provided in the same order as the columns of y. Note that for multioutput (including multilabel) weights should be defined for each class of every column in its own dict. For example, for four-class multilabel classification weights should be [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of [{1:1}, {2:5}, {3:1}, {4:1}]. The \"balanced\" mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as ``n_samples / (n_classes * np.bincount(y))`` The \"balanced_subsample\" mode is the same as \"balanced\" except that weights are computed based on the bootstrap sample for every tree grown. For multi-output, the weights of each column of y will be multiplied. Note that these weights will be multiplied with sample_weight (passed through the fit method) if sample_weight is specified. ccp_alpha : non-negative float, default=0.0 Complexity parameter used for Minimal Cost-Complexity Pruning. The subtree with the largest cost complexity that is smaller than ccp_alpha will be chosen. By default, no pruning is performed. See :ref: minimal_cost_complexity_pruning for details. .. versionadded:: 0.22 max_samples : int or float, default=None If bootstrap is True, the number of samples to draw from X to train each base estimator. - If None (default), then draw `X.shape[0]` samples. - If int, then draw `max_samples` samples. - If float, then draw `max_samples * X.shape[0]` samples. Thus, `max_samples` should be in the interval `(0.0, 1.0]`. .. versionadded:: 0.22","title":"Parameters"},{"location":"module/config/#giants.config.ModelEstimatorConfig.RandomForestClassifier--attributes","text":"base_estimator_ : DecisionTreeClassifier The child estimator template used to create the collection of fitted sub-estimators. estimators_ : list of DecisionTreeClassifier The collection of fitted sub-estimators. classes_ : ndarray of shape (n_classes,) or a list of such arrays The classes labels (single output problem), or a list of arrays of class labels (multi-output problem). n_classes_ : int or list The number of classes (single output problem), or a list containing the number of classes for each output (multi-output problem). n_features_ : int The number of features when fit is performed. .. deprecated:: 1.0 Attribute `n_features_` was deprecated in version 1.0 and will be removed in 1.2. Use `n_features_in_` instead. n_features_in_ : int Number of features seen during :term: fit . .. versionadded:: 0.24 feature_names_in_ : ndarray of shape ( n_features_in_ ,) Names of features seen during :term: fit . Defined only when X has feature names that are all strings. .. versionadded:: 1.0 n_outputs_ : int The number of outputs when fit is performed. feature_importances_ : ndarray of shape (n_features,) The impurity-based feature importances. The higher, the more important the feature. The importance of a feature is computed as the (normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance. Warning: impurity-based feature importances can be misleading for high cardinality features (many unique values). See :func:`sklearn.inspection.permutation_importance` as an alternative. oob_score_ : float Score of the training dataset obtained using an out-of-bag estimate. This attribute exists only when oob_score is True. oob_decision_function_ : ndarray of shape (n_samples, n_classes) or (n_samples, n_classes, n_outputs) Decision function computed with out-of-bag estimate on the training set. If n_estimators is small it might be possible that a data point was never left out during the bootstrap. In this case, oob_decision_function_ might contain NaN. This attribute exists only when oob_score is True.","title":"Attributes"},{"location":"module/config/#giants.config.ModelEstimatorConfig.RandomForestClassifier--see-also","text":"sklearn.tree.DecisionTreeClassifier : A decision tree classifier. sklearn.ensemble.ExtraTreesClassifier : Ensemble of extremely randomized tree classifiers.","title":"See Also"},{"location":"module/config/#giants.config.ModelEstimatorConfig.RandomForestClassifier--notes","text":"The default values for the parameters controlling the size of the trees (e.g. max_depth , min_samples_leaf , etc.) lead to fully grown and unpruned trees which can potentially be very large on some data sets. To reduce memory consumption, the complexity and size of the trees should be controlled by setting those parameter values. The features are always randomly permuted at each split. Therefore, the best found split may vary, even with the same training data, max_features=n_features and bootstrap=False , if the improvement of the criterion is identical for several splits enumerated during the search of the best split. To obtain a deterministic behaviour during fitting, random_state has to be fixed.","title":"Notes"},{"location":"module/config/#giants.config.ModelEstimatorConfig.RandomForestClassifier--references","text":".. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.","title":"References"},{"location":"module/config/#giants.config.ModelEstimatorConfig.RandomForestClassifier--examples","text":"from sklearn.ensemble import RandomForestClassifier from sklearn.datasets import make_classification X, y = make_classification(n_samples=1000, n_features=4, ... n_informative=2, n_redundant=0, ... random_state=0, shuffle=False) clf = RandomForestClassifier(max_depth=2, random_state=0) clf.fit(X, y) RandomForestClassifier(...) print(clf.predict([[0, 0, 0, 0]])) [1]","title":"Examples"},{"location":"module/config/#giants.config.ModelEstimatorConfig.RandomForestRegressor","text":"A random forest regressor. A random forest is a meta estimator that fits a number of classifying decision trees on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is controlled with the max_samples parameter if bootstrap=True (default), otherwise the whole dataset is used to build each tree. Read more in the :ref: User Guide <forest> .","title":"RandomForestRegressor"},{"location":"module/config/#giants.config.ModelEstimatorConfig.RandomForestRegressor--parameters","text":"n_estimators : int, default=100 The number of trees in the forest. .. versionchanged:: 0.22 The default value of ``n_estimators`` changed from 10 to 100 in 0.22. criterion : {\"squared_error\", \"absolute_error\", \"poisson\"}, default=\"squared_error\" The function to measure the quality of a split. Supported criteria are \"squared_error\" for the mean squared error, which is equal to variance reduction as feature selection criterion, \"absolute_error\" for the mean absolute error, and \"poisson\" which uses reduction in Poisson deviance to find splits. Training using \"absolute_error\" is significantly slower than when using \"squared_error\". .. versionadded:: 0.18 Mean Absolute Error (MAE) criterion. .. versionadded:: 1.0 Poisson criterion. .. deprecated:: 1.0 Criterion \"mse\" was deprecated in v1.0 and will be removed in version 1.2. Use `criterion=\"squared_error\"` which is equivalent. .. deprecated:: 1.0 Criterion \"mae\" was deprecated in v1.0 and will be removed in version 1.2. Use `criterion=\"absolute_error\"` which is equivalent. max_depth : int, default=None The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples. min_samples_split : int or float, default=2 The minimum number of samples required to split an internal node: - If int, then consider `min_samples_split` as the minimum number. - If float, then `min_samples_split` is a fraction and `ceil(min_samples_split * n_samples)` are the minimum number of samples for each split. .. versionchanged:: 0.18 Added float values for fractions. min_samples_leaf : int or float, default=1 The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least min_samples_leaf training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression. - If int, then consider `min_samples_leaf` as the minimum number. - If float, then `min_samples_leaf` is a fraction and `ceil(min_samples_leaf * n_samples)` are the minimum number of samples for each node. .. versionchanged:: 0.18 Added float values for fractions. min_weight_fraction_leaf : float, default=0.0 The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided. max_features : {\"auto\", \"sqrt\", \"log2\"}, int or float, default=\"auto\" The number of features to consider when looking for the best split: - If int, then consider `max_features` features at each split. - If float, then `max_features` is a fraction and `round(max_features * n_features)` features are considered at each split. - If \"auto\", then `max_features=n_features`. - If \"sqrt\", then `max_features=sqrt(n_features)`. - If \"log2\", then `max_features=log2(n_features)`. - If None, then `max_features=n_features`. Note: the search for a split does not stop until at least one valid partition of the node samples is found, even if it requires to effectively inspect more than ``max_features`` features. max_leaf_nodes : int, default=None Grow trees with max_leaf_nodes in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes. min_impurity_decrease : float, default=0.0 A node will be split if this split induces a decrease of the impurity greater than or equal to this value. The weighted impurity decrease equation is the following:: N_t / N * (impurity - N_t_R / N_t * right_impurity - N_t_L / N_t * left_impurity) where ``N`` is the total number of samples, ``N_t`` is the number of samples at the current node, ``N_t_L`` is the number of samples in the left child, and ``N_t_R`` is the number of samples in the right child. ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum, if ``sample_weight`` is passed. .. versionadded:: 0.19 bootstrap : bool, default=True Whether bootstrap samples are used when building trees. If False, the whole dataset is used to build each tree. oob_score : bool, default=False Whether to use out-of-bag samples to estimate the generalization score. Only available if bootstrap=True. n_jobs : int, default=None The number of jobs to run in parallel. :meth: fit , :meth: predict , :meth: decision_path and :meth: apply are all parallelized over the trees. None means 1 unless in a :obj: joblib.parallel_backend context. -1 means using all processors. See :term: Glossary <n_jobs> for more details. random_state : int, RandomState instance or None, default=None Controls both the randomness of the bootstrapping of the samples used when building trees (if bootstrap=True ) and the sampling of the features to consider when looking for the best split at each node (if max_features < n_features ). See :term: Glossary <random_state> for details. verbose : int, default=0 Controls the verbosity when fitting and predicting. warm_start : bool, default=False When set to True , reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just fit a whole new forest. See :term: the Glossary <warm_start> . ccp_alpha : non-negative float, default=0.0 Complexity parameter used for Minimal Cost-Complexity Pruning. The subtree with the largest cost complexity that is smaller than ccp_alpha will be chosen. By default, no pruning is performed. See :ref: minimal_cost_complexity_pruning for details. .. versionadded:: 0.22 max_samples : int or float, default=None If bootstrap is True, the number of samples to draw from X to train each base estimator. - If None (default), then draw `X.shape[0]` samples. - If int, then draw `max_samples` samples. - If float, then draw `max_samples * X.shape[0]` samples. Thus, `max_samples` should be in the interval `(0.0, 1.0]`. .. versionadded:: 0.22","title":"Parameters"},{"location":"module/config/#giants.config.ModelEstimatorConfig.RandomForestRegressor--attributes","text":"base_estimator_ : DecisionTreeRegressor The child estimator template used to create the collection of fitted sub-estimators. estimators_ : list of DecisionTreeRegressor The collection of fitted sub-estimators. feature_importances_ : ndarray of shape (n_features,) The impurity-based feature importances. The higher, the more important the feature. The importance of a feature is computed as the (normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance. Warning: impurity-based feature importances can be misleading for high cardinality features (many unique values). See :func:`sklearn.inspection.permutation_importance` as an alternative. n_features_ : int The number of features when fit is performed. .. deprecated:: 1.0 Attribute `n_features_` was deprecated in version 1.0 and will be removed in 1.2. Use `n_features_in_` instead. n_features_in_ : int Number of features seen during :term: fit . .. versionadded:: 0.24 feature_names_in_ : ndarray of shape ( n_features_in_ ,) Names of features seen during :term: fit . Defined only when X has feature names that are all strings. .. versionadded:: 1.0 n_outputs_ : int The number of outputs when fit is performed. oob_score_ : float Score of the training dataset obtained using an out-of-bag estimate. This attribute exists only when oob_score is True. oob_prediction_ : ndarray of shape (n_samples,) or (n_samples, n_outputs) Prediction computed with out-of-bag estimate on the training set. This attribute exists only when oob_score is True.","title":"Attributes"},{"location":"module/config/#giants.config.ModelEstimatorConfig.RandomForestRegressor--see-also","text":"sklearn.tree.DecisionTreeRegressor : A decision tree regressor. sklearn.ensemble.ExtraTreesRegressor : Ensemble of extremely randomized tree regressors.","title":"See Also"},{"location":"module/config/#giants.config.ModelEstimatorConfig.RandomForestRegressor--notes","text":"The default values for the parameters controlling the size of the trees (e.g. max_depth , min_samples_leaf , etc.) lead to fully grown and unpruned trees which can potentially be very large on some data sets. To reduce memory consumption, the complexity and size of the trees should be controlled by setting those parameter values. The features are always randomly permuted at each split. Therefore, the best found split may vary, even with the same training data, max_features=n_features and bootstrap=False , if the improvement of the criterion is identical for several splits enumerated during the search of the best split. To obtain a deterministic behaviour during fitting, random_state has to be fixed. The default value max_features=\"auto\" uses n_features rather than n_features / 3 . The latter was originally suggested in [1], whereas the former was more recently justified empirically in [2].","title":"Notes"},{"location":"module/config/#giants.config.ModelEstimatorConfig.RandomForestRegressor--references","text":".. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001. .. [2] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized trees\", Machine Learning, 63(1), 3-42, 2006.","title":"References"},{"location":"module/config/#giants.config.ModelEstimatorConfig.RandomForestRegressor--examples","text":"from sklearn.ensemble import RandomForestRegressor from sklearn.datasets import make_regression X, y = make_regression(n_features=4, n_informative=2, ... random_state=0, shuffle=False) regr = RandomForestRegressor(max_depth=2, random_state=0) regr.fit(X, y) RandomForestRegressor(...) print(regr.predict([[0, 0, 0, 0]])) [-8.32987858]","title":"Examples"},{"location":"module/config/#giants.config.ModelEstimatorConfig.SVC","text":"C-Support Vector Classification. The implementation is based on libsvm. The fit time scales at least quadratically with the number of samples and may be impractical beyond tens of thousands of samples. For large datasets consider using :class: ~sklearn.svm.LinearSVC or :class: ~sklearn.linear_model.SGDClassifier instead, possibly after a :class: ~sklearn.kernel_approximation.Nystroem transformer. The multiclass support is handled according to a one-vs-one scheme. For details on the precise mathematical formulation of the provided kernel functions and how gamma , coef0 and degree affect each other, see the corresponding section in the narrative documentation: :ref: svm_kernels . Read more in the :ref: User Guide <svm_classification> .","title":"SVC"},{"location":"module/config/#giants.config.ModelEstimatorConfig.SVC--parameters","text":"C : float, default=1.0 Regularization parameter. The strength of the regularization is inversely proportional to C. Must be strictly positive. The penalty is a squared l2 penalty. kernel : {'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'}, default='rbf' Specifies the kernel type to be used in the algorithm. It must be one of 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed' or a callable. If none is given, 'rbf' will be used. If a callable is given it is used to pre-compute the kernel matrix from data matrices; that matrix should be an array of shape (n_samples, n_samples) . degree : int, default=3 Degree of the polynomial kernel function ('poly'). Ignored by all other kernels. gamma : {'scale', 'auto'} or float, default='scale' Kernel coefficient for 'rbf', 'poly' and 'sigmoid'. - if ``gamma='scale'`` (default) is passed then it uses 1 / (n_features * X.var()) as value of gamma, - if 'auto', uses 1 / n_features. .. versionchanged:: 0.22 The default value of ``gamma`` changed from 'auto' to 'scale'. coef0 : float, default=0.0 Independent term in kernel function. It is only significant in 'poly' and 'sigmoid'. shrinking : bool, default=True Whether to use the shrinking heuristic. See the :ref: User Guide <shrinking_svm> . probability : bool, default=False Whether to enable probability estimates. This must be enabled prior to calling fit , will slow down that method as it internally uses 5-fold cross-validation, and predict_proba may be inconsistent with predict . Read more in the :ref: User Guide <scores_probabilities> . tol : float, default=1e-3 Tolerance for stopping criterion. cache_size : float, default=200 Specify the size of the kernel cache (in MB). class_weight : dict or 'balanced', default=None Set the parameter C of class i to class_weight[i]*C for SVC. If not given, all classes are supposed to have weight one. The \"balanced\" mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as n_samples / (n_classes * np.bincount(y)) . verbose : bool, default=False Enable verbose output. Note that this setting takes advantage of a per-process runtime setting in libsvm that, if enabled, may not work properly in a multithreaded context. max_iter : int, default=-1 Hard limit on iterations within solver, or -1 for no limit. decision_function_shape : {'ovo', 'ovr'}, default='ovr' Whether to return a one-vs-rest ('ovr') decision function of shape (n_samples, n_classes) as all other classifiers, or the original one-vs-one ('ovo') decision function of libsvm which has shape (n_samples, n_classes * (n_classes - 1) / 2). However, one-vs-one ('ovo') is always used as multi-class strategy. The parameter is ignored for binary classification. .. versionchanged:: 0.19 decision_function_shape is 'ovr' by default. .. versionadded:: 0.17 *decision_function_shape='ovr'* is recommended. .. versionchanged:: 0.17 Deprecated *decision_function_shape='ovo' and None*. break_ties : bool, default=False If true, decision_function_shape='ovr' , and number of classes > 2, :term: predict will break ties according to the confidence values of :term: decision_function ; otherwise the first class among the tied classes is returned. Please note that breaking ties comes at a relatively high computational cost compared to a simple predict. .. versionadded:: 0.22 random_state : int, RandomState instance or None, default=None Controls the pseudo random number generation for shuffling the data for probability estimates. Ignored when probability is False. Pass an int for reproducible output across multiple function calls. See :term: Glossary <random_state> .","title":"Parameters"},{"location":"module/config/#giants.config.ModelEstimatorConfig.SVC--attributes","text":"class_weight_ : ndarray of shape (n_classes,) Multipliers of parameter C for each class. Computed based on the class_weight parameter. classes_ : ndarray of shape (n_classes,) The classes labels. coef_ : ndarray of shape (n_classes * (n_classes - 1) / 2, n_features) Weights assigned to the features (coefficients in the primal problem). This is only available in the case of a linear kernel. `coef_` is a readonly property derived from `dual_coef_` and `support_vectors_`. dual_coef_ : ndarray of shape (n_classes -1, n_SV) Dual coefficients of the support vector in the decision function (see :ref: sgd_mathematical_formulation ), multiplied by their targets. For multiclass, coefficient for all 1-vs-1 classifiers. The layout of the coefficients in the multiclass case is somewhat non-trivial. See the :ref: multi-class section of the User Guide <svm_multi_class> for details. fit_status_ : int 0 if correctly fitted, 1 otherwise (will raise warning) intercept_ : ndarray of shape (n_classes * (n_classes - 1) / 2,) Constants in decision function. n_features_in_ : int Number of features seen during :term: fit . .. versionadded:: 0.24 feature_names_in_ : ndarray of shape ( n_features_in_ ,) Names of features seen during :term: fit . Defined only when X has feature names that are all strings. .. versionadded:: 1.0 support_ : ndarray of shape (n_SV) Indices of support vectors. support_vectors_ : ndarray of shape (n_SV, n_features) Support vectors. n_support_ : ndarray of shape (n_classes,), dtype=int32 Number of support vectors for each class. probA_ : ndarray of shape (n_classes * (n_classes - 1) / 2) probB_ : ndarray of shape (n_classes * (n_classes - 1) / 2) If probability=True , it corresponds to the parameters learned in Platt scaling to produce probability estimates from decision values. If probability=False , it's an empty array. Platt scaling uses the logistic function 1 / (1 + exp(decision_value * probA_ + probB_)) where probA_ and probB_ are learned from the dataset [2] . For more information on the multiclass case and training procedure see section 8 of [1] . shape_fit_ : tuple of int of shape (n_dimensions_of_X,) Array dimensions of training vector X .","title":"Attributes"},{"location":"module/config/#giants.config.ModelEstimatorConfig.SVC--see-also","text":"SVR : Support Vector Machine for Regression implemented using libsvm. LinearSVC : Scalable Linear Support Vector Machine for classification implemented using liblinear. Check the See Also section of LinearSVC for more comparison element.","title":"See Also"},{"location":"module/config/#giants.config.ModelEstimatorConfig.SVC--references","text":".. [1] LIBSVM: A Library for Support Vector Machines <http://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf> _ .. [2] Platt, John (1999). \"Probabilistic outputs for support vector machines and comparison to regularizedlikelihood methods.\" <http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.41.1639> _","title":"References"},{"location":"module/config/#giants.config.ModelEstimatorConfig.SVC--examples","text":"import numpy as np from sklearn.pipeline import make_pipeline from sklearn.preprocessing import StandardScaler X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]]) y = np.array([1, 1, 2, 2]) from sklearn.svm import SVC clf = make_pipeline(StandardScaler(), SVC(gamma='auto')) clf.fit(X, y) Pipeline(steps=[('standardscaler', StandardScaler()), ('svc', SVC(gamma='auto'))]) print(clf.predict([[-0.8, -1]])) [1]","title":"Examples"},{"location":"module/config/#giants.config.ModelEstimatorConfig.SVR","text":"Epsilon-Support Vector Regression. The free parameters in the model are C and epsilon. The implementation is based on libsvm. The fit time complexity is more than quadratic with the number of samples which makes it hard to scale to datasets with more than a couple of 10000 samples. For large datasets consider using :class: ~sklearn.svm.LinearSVR or :class: ~sklearn.linear_model.SGDRegressor instead, possibly after a :class: ~sklearn.kernel_approximation.Nystroem transformer. Read more in the :ref: User Guide <svm_regression> .","title":"SVR"},{"location":"module/config/#giants.config.ModelEstimatorConfig.SVR--parameters","text":"kernel : {'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'}, default='rbf' Specifies the kernel type to be used in the algorithm. It must be one of 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed' or a callable. If none is given, 'rbf' will be used. If a callable is given it is used to precompute the kernel matrix. degree : int, default=3 Degree of the polynomial kernel function ('poly'). Ignored by all other kernels. gamma : {'scale', 'auto'} or float, default='scale' Kernel coefficient for 'rbf', 'poly' and 'sigmoid'. - if ``gamma='scale'`` (default) is passed then it uses 1 / (n_features * X.var()) as value of gamma, - if 'auto', uses 1 / n_features. .. versionchanged:: 0.22 The default value of ``gamma`` changed from 'auto' to 'scale'. coef0 : float, default=0.0 Independent term in kernel function. It is only significant in 'poly' and 'sigmoid'. tol : float, default=1e-3 Tolerance for stopping criterion. C : float, default=1.0 Regularization parameter. The strength of the regularization is inversely proportional to C. Must be strictly positive. The penalty is a squared l2 penalty. epsilon : float, default=0.1 Epsilon in the epsilon-SVR model. It specifies the epsilon-tube within which no penalty is associated in the training loss function with points predicted within a distance epsilon from the actual value. shrinking : bool, default=True Whether to use the shrinking heuristic. See the :ref: User Guide <shrinking_svm> . cache_size : float, default=200 Specify the size of the kernel cache (in MB). verbose : bool, default=False Enable verbose output. Note that this setting takes advantage of a per-process runtime setting in libsvm that, if enabled, may not work properly in a multithreaded context. max_iter : int, default=-1 Hard limit on iterations within solver, or -1 for no limit.","title":"Parameters"},{"location":"module/config/#giants.config.ModelEstimatorConfig.SVR--attributes","text":"class_weight_ : ndarray of shape (n_classes,) Multipliers of parameter C for each class. Computed based on the class_weight parameter. coef_ : ndarray of shape (1, n_features) Weights assigned to the features (coefficients in the primal problem). This is only available in the case of a linear kernel. `coef_` is readonly property derived from `dual_coef_` and `support_vectors_`. dual_coef_ : ndarray of shape (1, n_SV) Coefficients of the support vector in the decision function. fit_status_ : int 0 if correctly fitted, 1 otherwise (will raise warning) intercept_ : ndarray of shape (1,) Constants in decision function. n_features_in_ : int Number of features seen during :term: fit . .. versionadded:: 0.24 feature_names_in_ : ndarray of shape ( n_features_in_ ,) Names of features seen during :term: fit . Defined only when X has feature names that are all strings. .. versionadded:: 1.0 n_support_ : ndarray of shape (n_classes,), dtype=int32 Number of support vectors for each class. shape_fit_ : tuple of int of shape (n_dimensions_of_X,) Array dimensions of training vector X . support_ : ndarray of shape (n_SV,) Indices of support vectors. support_vectors_ : ndarray of shape (n_SV, n_features) Support vectors.","title":"Attributes"},{"location":"module/config/#giants.config.ModelEstimatorConfig.SVR--see-also","text":"NuSVR : Support Vector Machine for regression implemented using libsvm using a parameter to control the number of support vectors. LinearSVR : Scalable Linear Support Vector Machine for regression implemented using liblinear.","title":"See Also"},{"location":"module/config/#giants.config.ModelEstimatorConfig.SVR--references","text":".. [1] LIBSVM: A Library for Support Vector Machines <http://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf> _ .. [2] Platt, John (1999). \"Probabilistic outputs for support vector machines and comparison to regularizedlikelihood methods.\" <http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.41.1639> _","title":"References"},{"location":"module/config/#giants.config.ModelEstimatorConfig.SVR--examples","text":"from sklearn.svm import SVR from sklearn.pipeline import make_pipeline from sklearn.preprocessing import StandardScaler import numpy as np n_samples, n_features = 10, 5 rng = np.random.RandomState(0) y = rng.randn(n_samples) X = rng.randn(n_samples, n_features) regr = make_pipeline(StandardScaler(), SVR(C=1.0, epsilon=0.2)) regr.fit(X, y) Pipeline(steps=[('standardscaler', StandardScaler()), ('svr', SVR(epsilon=0.2))])","title":"Examples"},{"location":"module/config/#giants.config.ParamGridConfig","text":"Stores the default grid search parameters to explore for each model.","title":"ParamGridConfig"},{"location":"module/config/#giants.config.TypeConfig","text":"Stores a series of python type hints for model-specific keywords.","title":"TypeConfig"},{"location":"module/config/#giants.config.TypeConfig.Array","text":"ndarray(shape, dtype=float, buffer=None, offset=0, strides=None, order=None) An array object represents a multidimensional, homogeneous array of fixed-size items. An associated data-type object describes the format of each element in the array (its byte-order, how many bytes it occupies in memory, whether it is an integer, a floating point number, or something else, etc.) Arrays should be constructed using array , zeros or empty (refer to the See Also section below). The parameters given here refer to a low-level method ( ndarray(...) ) for instantiating an array. For more information, refer to the numpy module and examine the methods and attributes of an array.","title":"Array"},{"location":"module/config/#giants.config.TypeConfig.Array--parameters","text":"(for the new method; see Notes below) shape : tuple of ints Shape of created array. dtype : data-type, optional Any object that can be interpreted as a numpy data type. buffer : object exposing buffer interface, optional Used to fill the array with data. offset : int, optional Offset of array data in buffer. strides : tuple of ints, optional Strides of data in memory. order : {'C', 'F'}, optional Row-major (C-style) or column-major (Fortran-style) order.","title":"Parameters"},{"location":"module/config/#giants.config.TypeConfig.Array--attributes","text":"T : ndarray Transpose of the array. data : buffer The array's elements, in memory. dtype : dtype object Describes the format of the elements in the array. flags : dict Dictionary containing information related to memory use, e.g., 'C_CONTIGUOUS', 'OWNDATA', 'WRITEABLE', etc. flat : numpy.flatiter object Flattened version of the array as an iterator. The iterator allows assignments, e.g., x.flat = 3 (See ndarray.flat for assignment examples; TODO). imag : ndarray Imaginary part of the array. real : ndarray Real part of the array. size : int Number of elements in the array. itemsize : int The memory use of each array element in bytes. nbytes : int The total number of bytes required to store the array data, i.e., itemsize * size . ndim : int The array's number of dimensions. shape : tuple of ints Shape of the array. strides : tuple of ints The step-size required to move from one element to the next in memory. For example, a contiguous (3, 4) array of type int16 in C-order has strides (8, 2) . This implies that to move from element to element in memory requires jumps of 2 bytes. To move from row-to-row, one needs to jump 8 bytes at a time ( 2 * 4 ). ctypes : ctypes object Class containing properties of the array needed for interaction with ctypes. base : ndarray If the array is a view into another array, that array is its base (unless that array is also a view). The base array is where the array data is actually stored.","title":"Attributes"},{"location":"module/config/#giants.config.TypeConfig.Array--see-also","text":"array : Construct an array. zeros : Create an array, each element of which is zero. empty : Create an array, but leave its allocated memory unchanged (i.e., it contains \"garbage\"). dtype : Create a data-type. numpy.typing.NDArray : A :term: generic <generic type> version of ndarray.","title":"See Also"},{"location":"module/config/#giants.config.TypeConfig.Array--notes","text":"There are two modes of creating an array using __new__ : If buffer is None, then only shape , dtype , and order are used. If buffer is an object exposing the buffer interface, then all keywords are interpreted. No __init__ method is needed because the array is fully initialized after the __new__ method.","title":"Notes"},{"location":"module/config/#giants.config.TypeConfig.Array--examples","text":"These examples illustrate the low-level ndarray constructor. Refer to the See Also section above for easier ways of constructing an ndarray. First mode, buffer is None: np.ndarray(shape=(2,2), dtype=float, order='F') array([[0.0e+000, 0.0e+000], # random [ nan, 2.5e-323]]) Second mode: np.ndarray((2,), buffer=np.array([1,2,3]), ... offset=np.int_().itemsize, ... dtype=int) # offset = 1*itemsize, i.e. skip first element array([2, 3])","title":"Examples"},{"location":"module/config/#giants.config.TypeConfig.BaseSearchCV","text":"Abstract base class for hyper parameter search with cross-validation.","title":"BaseSearchCV"},{"location":"module/config/#giants.config.TypeConfig.BaseSearchCV.classes_","text":"Class labels. Only available when refit=True and the estimator is a classifier.","title":"classes_"},{"location":"module/config/#giants.config.TypeConfig.BaseSearchCV.n_features_in_","text":"Number of features seen during :term: fit . Only available when refit=True .","title":"n_features_in_"},{"location":"module/config/#giants.config.TypeConfig.BaseSearchCV.decision_function","text":"Call decision_function on the estimator with the best found parameters. Only available if refit=True and the underlying estimator supports decision_function .","title":"decision_function()"},{"location":"module/config/#giants.config.TypeConfig.BaseSearchCV.decision_function--parameters","text":"X : indexable, length n_samples Must fulfill the input assumptions of the underlying estimator.","title":"Parameters"},{"location":"module/config/#giants.config.TypeConfig.BaseSearchCV.decision_function--returns","text":"y_score : ndarray of shape (n_samples,) or (n_samples, n_classes) or (n_samples, n_classes * (n_classes-1) / 2) Result of the decision function for X based on the estimator with the best found parameters. Source code in giants/config.py @available_if ( _estimator_has ( \"decision_function\" )) def decision_function ( self , X ): \"\"\"Call decision_function on the estimator with the best found parameters. Only available if ``refit=True`` and the underlying estimator supports ``decision_function``. Parameters ---------- X : indexable, length n_samples Must fulfill the input assumptions of the underlying estimator. Returns ------- y_score : ndarray of shape (n_samples,) or (n_samples, n_classes) \\ or (n_samples, n_classes * (n_classes-1) / 2) Result of the decision function for `X` based on the estimator with the best found parameters. \"\"\" check_is_fitted ( self ) return self . best_estimator_ . decision_function ( X )","title":"Returns"},{"location":"module/config/#giants.config.TypeConfig.BaseSearchCV.fit","text":"Run fit with all sets of parameters.","title":"fit()"},{"location":"module/config/#giants.config.TypeConfig.BaseSearchCV.fit--parameters","text":"X : array-like of shape (n_samples, n_features) Training vector, where n_samples is the number of samples and n_features is the number of features. y : array-like of shape (n_samples, n_output) or (n_samples,), default=None Target relative to X for classification or regression; None for unsupervised learning. groups : array-like of shape (n_samples,), default=None Group labels for the samples used while splitting the dataset into train/test set. Only used in conjunction with a \"Group\" :term: cv instance (e.g., :class: ~sklearn.model_selection.GroupKFold ). **fit_params : dict of str -> object Parameters passed to the fit method of the estimator.","title":"Parameters"},{"location":"module/config/#giants.config.TypeConfig.BaseSearchCV.fit--returns","text":"self : object Instance of fitted estimator. Source code in giants/config.py def fit ( self , X , y = None , * , groups = None , ** fit_params ): \"\"\"Run fit with all sets of parameters. Parameters ---------- X : array-like of shape (n_samples, n_features) Training vector, where `n_samples` is the number of samples and `n_features` is the number of features. y : array-like of shape (n_samples, n_output) \\ or (n_samples,), default=None Target relative to X for classification or regression; None for unsupervised learning. groups : array-like of shape (n_samples,), default=None Group labels for the samples used while splitting the dataset into train/test set. Only used in conjunction with a \"Group\" :term:`cv` instance (e.g., :class:`~sklearn.model_selection.GroupKFold`). **fit_params : dict of str -> object Parameters passed to the ``fit`` method of the estimator. Returns ------- self : object Instance of fitted estimator. \"\"\" estimator = self . estimator refit_metric = \"score\" if callable ( self . scoring ): scorers = self . scoring elif self . scoring is None or isinstance ( self . scoring , str ): scorers = check_scoring ( self . estimator , self . scoring ) else : scorers = _check_multimetric_scoring ( self . estimator , self . scoring ) self . _check_refit_for_multimetric ( scorers ) refit_metric = self . refit X , y , groups = indexable ( X , y , groups ) fit_params = _check_fit_params ( X , fit_params ) cv_orig = check_cv ( self . cv , y , classifier = is_classifier ( estimator )) n_splits = cv_orig . get_n_splits ( X , y , groups ) base_estimator = clone ( self . estimator ) parallel = Parallel ( n_jobs = self . n_jobs , pre_dispatch = self . pre_dispatch ) fit_and_score_kwargs = dict ( scorer = scorers , fit_params = fit_params , return_train_score = self . return_train_score , return_n_test_samples = True , return_times = True , return_parameters = False , error_score = self . error_score , verbose = self . verbose , ) results = {} with parallel : all_candidate_params = [] all_out = [] all_more_results = defaultdict ( list ) def evaluate_candidates ( candidate_params , cv = None , more_results = None ): cv = cv or cv_orig candidate_params = list ( candidate_params ) n_candidates = len ( candidate_params ) if self . verbose > 0 : print ( \"Fitting {0} folds for each of {1} candidates,\" \" totalling {2} fits\" . format ( n_splits , n_candidates , n_candidates * n_splits ) ) out = parallel ( delayed ( _fit_and_score )( clone ( base_estimator ), X , y , train = train , test = test , parameters = parameters , split_progress = ( split_idx , n_splits ), candidate_progress = ( cand_idx , n_candidates ), ** fit_and_score_kwargs , ) for ( cand_idx , parameters ), ( split_idx , ( train , test )) in product ( enumerate ( candidate_params ), enumerate ( cv . split ( X , y , groups )) ) ) if len ( out ) < 1 : raise ValueError ( \"No fits were performed. \" \"Was the CV iterator empty? \" \"Were there no candidates?\" ) elif len ( out ) != n_candidates * n_splits : raise ValueError ( \"cv.split and cv.get_n_splits returned \" \"inconsistent results. Expected {} \" \"splits, got {} \" . format ( n_splits , len ( out ) // n_candidates ) ) _warn_about_fit_failures ( out , self . error_score ) # For callable self.scoring, the return type is only know after # calling. If the return type is a dictionary, the error scores # can now be inserted with the correct key. The type checking # of out will be done in `_insert_error_scores`. if callable ( self . scoring ): _insert_error_scores ( out , self . error_score ) all_candidate_params . extend ( candidate_params ) all_out . extend ( out ) if more_results is not None : for key , value in more_results . items (): all_more_results [ key ] . extend ( value ) nonlocal results results = self . _format_results ( all_candidate_params , n_splits , all_out , all_more_results ) return results self . _run_search ( evaluate_candidates ) # multimetric is determined here because in the case of a callable # self.scoring the return type is only known after calling first_test_score = all_out [ 0 ][ \"test_scores\" ] self . multimetric_ = isinstance ( first_test_score , dict ) # check refit_metric now for a callabe scorer that is multimetric if callable ( self . scoring ) and self . multimetric_ : self . _check_refit_for_multimetric ( first_test_score ) refit_metric = self . refit # For multi-metric evaluation, store the best_index_, best_params_ and # best_score_ iff refit is one of the scorer names # In single metric evaluation, refit_metric is \"score\" if self . refit or not self . multimetric_ : self . best_index_ = self . _select_best_index ( self . refit , refit_metric , results ) if not callable ( self . refit ): # With a non-custom callable, we can select the best score # based on the best index self . best_score_ = results [ f \"mean_test_ { refit_metric } \" ][ self . best_index_ ] self . best_params_ = results [ \"params\" ][ self . best_index_ ] if self . refit : # we clone again after setting params in case some # of the params are estimators as well. self . best_estimator_ = clone ( clone ( base_estimator ) . set_params ( ** self . best_params_ ) ) refit_start_time = time . time () if y is not None : self . best_estimator_ . fit ( X , y , ** fit_params ) else : self . best_estimator_ . fit ( X , ** fit_params ) refit_end_time = time . time () self . refit_time_ = refit_end_time - refit_start_time if hasattr ( self . best_estimator_ , \"feature_names_in_\" ): self . feature_names_in_ = self . best_estimator_ . feature_names_in_ # Store the only scorer not as a dict for single metric evaluation self . scorer_ = scorers self . cv_results_ = results self . n_splits_ = n_splits return self","title":"Returns"},{"location":"module/config/#giants.config.TypeConfig.BaseSearchCV.inverse_transform","text":"Call inverse_transform on the estimator with the best found params. Only available if the underlying estimator implements inverse_transform and refit=True .","title":"inverse_transform()"},{"location":"module/config/#giants.config.TypeConfig.BaseSearchCV.inverse_transform--parameters","text":"Xt : indexable, length n_samples Must fulfill the input assumptions of the underlying estimator.","title":"Parameters"},{"location":"module/config/#giants.config.TypeConfig.BaseSearchCV.inverse_transform--returns","text":"X : {ndarray, sparse matrix} of shape (n_samples, n_features) Result of the inverse_transform function for Xt based on the estimator with the best found parameters. Source code in giants/config.py @available_if ( _estimator_has ( \"inverse_transform\" )) def inverse_transform ( self , Xt ): \"\"\"Call inverse_transform on the estimator with the best found params. Only available if the underlying estimator implements ``inverse_transform`` and ``refit=True``. Parameters ---------- Xt : indexable, length n_samples Must fulfill the input assumptions of the underlying estimator. Returns ------- X : {ndarray, sparse matrix} of shape (n_samples, n_features) Result of the `inverse_transform` function for `Xt` based on the estimator with the best found parameters. \"\"\" check_is_fitted ( self ) return self . best_estimator_ . inverse_transform ( Xt )","title":"Returns"},{"location":"module/config/#giants.config.TypeConfig.BaseSearchCV.predict","text":"Call predict on the estimator with the best found parameters. Only available if refit=True and the underlying estimator supports predict .","title":"predict()"},{"location":"module/config/#giants.config.TypeConfig.BaseSearchCV.predict--parameters","text":"X : indexable, length n_samples Must fulfill the input assumptions of the underlying estimator.","title":"Parameters"},{"location":"module/config/#giants.config.TypeConfig.BaseSearchCV.predict--returns","text":"y_pred : ndarray of shape (n_samples,) The predicted labels or values for X based on the estimator with the best found parameters. Source code in giants/config.py @available_if ( _estimator_has ( \"predict\" )) def predict ( self , X ): \"\"\"Call predict on the estimator with the best found parameters. Only available if ``refit=True`` and the underlying estimator supports ``predict``. Parameters ---------- X : indexable, length n_samples Must fulfill the input assumptions of the underlying estimator. Returns ------- y_pred : ndarray of shape (n_samples,) The predicted labels or values for `X` based on the estimator with the best found parameters. \"\"\" check_is_fitted ( self ) return self . best_estimator_ . predict ( X )","title":"Returns"},{"location":"module/config/#giants.config.TypeConfig.BaseSearchCV.predict_log_proba","text":"Call predict_log_proba on the estimator with the best found parameters. Only available if refit=True and the underlying estimator supports predict_log_proba .","title":"predict_log_proba()"},{"location":"module/config/#giants.config.TypeConfig.BaseSearchCV.predict_log_proba--parameters","text":"X : indexable, length n_samples Must fulfill the input assumptions of the underlying estimator.","title":"Parameters"},{"location":"module/config/#giants.config.TypeConfig.BaseSearchCV.predict_log_proba--returns","text":"y_pred : ndarray of shape (n_samples,) or (n_samples, n_classes) Predicted class log-probabilities for X based on the estimator with the best found parameters. The order of the classes corresponds to that in the fitted attribute :term: classes_ . Source code in giants/config.py @available_if ( _estimator_has ( \"predict_log_proba\" )) def predict_log_proba ( self , X ): \"\"\"Call predict_log_proba on the estimator with the best found parameters. Only available if ``refit=True`` and the underlying estimator supports ``predict_log_proba``. Parameters ---------- X : indexable, length n_samples Must fulfill the input assumptions of the underlying estimator. Returns ------- y_pred : ndarray of shape (n_samples,) or (n_samples, n_classes) Predicted class log-probabilities for `X` based on the estimator with the best found parameters. The order of the classes corresponds to that in the fitted attribute :term:`classes_`. \"\"\" check_is_fitted ( self ) return self . best_estimator_ . predict_log_proba ( X )","title":"Returns"},{"location":"module/config/#giants.config.TypeConfig.BaseSearchCV.predict_proba","text":"Call predict_proba on the estimator with the best found parameters. Only available if refit=True and the underlying estimator supports predict_proba .","title":"predict_proba()"},{"location":"module/config/#giants.config.TypeConfig.BaseSearchCV.predict_proba--parameters","text":"X : indexable, length n_samples Must fulfill the input assumptions of the underlying estimator.","title":"Parameters"},{"location":"module/config/#giants.config.TypeConfig.BaseSearchCV.predict_proba--returns","text":"y_pred : ndarray of shape (n_samples,) or (n_samples, n_classes) Predicted class probabilities for X based on the estimator with the best found parameters. The order of the classes corresponds to that in the fitted attribute :term: classes_ . Source code in giants/config.py @available_if ( _estimator_has ( \"predict_proba\" )) def predict_proba ( self , X ): \"\"\"Call predict_proba on the estimator with the best found parameters. Only available if ``refit=True`` and the underlying estimator supports ``predict_proba``. Parameters ---------- X : indexable, length n_samples Must fulfill the input assumptions of the underlying estimator. Returns ------- y_pred : ndarray of shape (n_samples,) or (n_samples, n_classes) Predicted class probabilities for `X` based on the estimator with the best found parameters. The order of the classes corresponds to that in the fitted attribute :term:`classes_`. \"\"\" check_is_fitted ( self ) return self . best_estimator_ . predict_proba ( X )","title":"Returns"},{"location":"module/config/#giants.config.TypeConfig.BaseSearchCV.score","text":"Return the score on the given data, if the estimator has been refit. This uses the score defined by scoring where provided, and the best_estimator_.score method otherwise.","title":"score()"},{"location":"module/config/#giants.config.TypeConfig.BaseSearchCV.score--parameters","text":"X : array-like of shape (n_samples, n_features) Input data, where n_samples is the number of samples and n_features is the number of features. y : array-like of shape (n_samples, n_output) or (n_samples,), default=None Target relative to X for classification or regression; None for unsupervised learning.","title":"Parameters"},{"location":"module/config/#giants.config.TypeConfig.BaseSearchCV.score--returns","text":"score : float The score defined by scoring if provided, and the best_estimator_.score method otherwise. Source code in giants/config.py def score ( self , X , y = None ): \"\"\"Return the score on the given data, if the estimator has been refit. This uses the score defined by ``scoring`` where provided, and the ``best_estimator_.score`` method otherwise. Parameters ---------- X : array-like of shape (n_samples, n_features) Input data, where `n_samples` is the number of samples and `n_features` is the number of features. y : array-like of shape (n_samples, n_output) \\ or (n_samples,), default=None Target relative to X for classification or regression; None for unsupervised learning. Returns ------- score : float The score defined by ``scoring`` if provided, and the ``best_estimator_.score`` method otherwise. \"\"\" _check_refit ( self , \"score\" ) check_is_fitted ( self ) if self . scorer_ is None : raise ValueError ( \"No score function explicitly defined, \" \"and the estimator doesn't provide one %s \" % self . best_estimator_ ) if isinstance ( self . scorer_ , dict ): if self . multimetric_ : scorer = self . scorer_ [ self . refit ] else : scorer = self . scorer_ return scorer ( self . best_estimator_ , X , y ) # callable score = self . scorer_ ( self . best_estimator_ , X , y ) if self . multimetric_ : score = score [ self . refit ] return score","title":"Returns"},{"location":"module/config/#giants.config.TypeConfig.BaseSearchCV.score_samples","text":"Call score_samples on the estimator with the best found parameters. Only available if refit=True and the underlying estimator supports score_samples . .. versionadded:: 0.24","title":"score_samples()"},{"location":"module/config/#giants.config.TypeConfig.BaseSearchCV.score_samples--parameters","text":"X : iterable Data to predict on. Must fulfill input requirements of the underlying estimator.","title":"Parameters"},{"location":"module/config/#giants.config.TypeConfig.BaseSearchCV.score_samples--returns","text":"y_score : ndarray of shape (n_samples,) The best_estimator_.score_samples method. Source code in giants/config.py @available_if ( _estimator_has ( \"score_samples\" )) def score_samples ( self , X ): \"\"\"Call score_samples on the estimator with the best found parameters. Only available if ``refit=True`` and the underlying estimator supports ``score_samples``. .. versionadded:: 0.24 Parameters ---------- X : iterable Data to predict on. Must fulfill input requirements of the underlying estimator. Returns ------- y_score : ndarray of shape (n_samples,) The ``best_estimator_.score_samples`` method. \"\"\" check_is_fitted ( self ) return self . best_estimator_ . score_samples ( X )","title":"Returns"},{"location":"module/config/#giants.config.TypeConfig.BaseSearchCV.transform","text":"Call transform on the estimator with the best found parameters. Only available if the underlying estimator supports transform and refit=True .","title":"transform()"},{"location":"module/config/#giants.config.TypeConfig.BaseSearchCV.transform--parameters","text":"X : indexable, length n_samples Must fulfill the input assumptions of the underlying estimator.","title":"Parameters"},{"location":"module/config/#giants.config.TypeConfig.BaseSearchCV.transform--returns","text":"Xt : {ndarray, sparse matrix} of shape (n_samples, n_features) X transformed in the new space based on the estimator with the best found parameters. Source code in giants/config.py @available_if ( _estimator_has ( \"transform\" )) def transform ( self , X ): \"\"\"Call transform on the estimator with the best found parameters. Only available if the underlying estimator supports ``transform`` and ``refit=True``. Parameters ---------- X : indexable, length n_samples Must fulfill the input assumptions of the underlying estimator. Returns ------- Xt : {ndarray, sparse matrix} of shape (n_samples, n_features) `X` transformed in the new space based on the estimator with the best found parameters. \"\"\" check_is_fitted ( self ) return self . best_estimator_ . transform ( X )","title":"Returns"},{"location":"module/config/#giants.config.TypeConfig.Number","text":"All numbers inherit from this class. If you just want to check if an argument x is a number, without caring what kind, use isinstance(x, Number).","title":"Number"},{"location":"module/model/","text":"Tools for running sklearn model hyperparameter searches. Tuner \u00b6 A class for performing hyperparameter searches using sklearn models. AdaBoostClassifier ( self , param_grid = { 'n_estimators' : ( 25 , 50 , 75 , 100 ), 'learning_rate' : ( 0.1 , 0.5 , 1.0 )}, optimizer =< class ' sklearn . model_selection . _search . GridSearchCV '>, scorer=' neg_log_loss ', cv=<class ' sklearn . model_selection . _split . StratifiedKFold '>, fit_params=None) \u00b6 Run hyperparameter tuning for an AdaBoostClassifier model. Parameters: Name Type Description Default param_grid Union[Dict, sklearn.model_selection._search.ParameterGrid] Hyperparameter values to explore in grid searching. {'n_estimators': (25, 50, 75, 100), 'learning_rate': (0.1, 0.5, 1.0)} optimizer BaseSearchCV Method for evaluating hyperparameter combinations. From the sklearn group of hyperparameter optimizers. <class 'sklearn.model_selection._search.GridSearchCV'> scorer str Performance metric to optimize in model training. Get available options with list_scorers() function. 'neg_log_loss' cv Union[int, sklearn.model_selection._split.BaseCrossValidator, Iterable] Determines the cross-validation splitting strategy. Possible inputs for cv are: None, to use the default 5-fold cross validation, integer, to specify the number of folds in a (Stratified)KFold, CV splitter, An iterable yielding (train, test) splits as arrays of indices. <class 'sklearn.model_selection._split.StratifiedKFold'> fit_params dict Parameters passed to the `fit()`` method of the estimator. None Source code in giants/model.py def AdaBoostClassifier ( self , param_grid : Types . ParameterGrid = ParamGrids . AdaBoostClassifier , optimizer : Types . BaseSearchCV = ModelDefaults . Optimizer , scorer : str = ModelDefaults . ClassificationScorer , cv : Types . BaseCrossValidator = ModelDefaults . CVClassification , fit_params : dict = None , ) -> None : \"\"\"Run hyperparameter tuning for an AdaBoostClassifier model. Args: param_grid: Hyperparameter values to explore in grid searching. optimizer: Method for evaluating hyperparameter combinations. From the `sklearn` group of hyperparameter optimizers. scorer: Performance metric to optimize in model training. Get available options with `list_scorers()` function. cv: Determines the cross-validation splitting strategy. Possible inputs for cv are: None, to use the default 5-fold cross validation, integer, to specify the number of folds in a (Stratified)KFold, CV splitter, An iterable yielding (train, test) splits as arrays of indices. fit_params: Parameters passed to the `fit()`` method of the estimator. \"\"\" self . param_grid = param_grid self . optimizer = optimizer self . scorer = scorer self . cv = cv ( n_splits = self . n_splits ) estimator = ModelEstimators . AdaBoostClassifier () self . _run_grid_search ( estimator ) AdaBoostRegressor ( self , param_grid = { 'n_estimators' : ( 25 , 50 , 75 , 100 ), 'learning_rate' : ( 0.1 , 0.5 , 1.0 ), 'loss' : ( 'linear' , 'exponential' , 'square' )}, optimizer =< class ' sklearn . model_selection . _search . GridSearchCV '>, scorer=' neg_root_mean_squared_error ', cv=<class ' sklearn . model_selection . _split . KFold '>, fit_params=None) \u00b6 Run hyperparameter tuning for an AdaBoostRegression model. Parameters: Name Type Description Default param_grid Union[Dict, sklearn.model_selection._search.ParameterGrid] Hyperparameter values to explore in grid searching. {'n_estimators': (25, 50, 75, 100), 'learning_rate': (0.1, 0.5, 1.0), 'loss': ('linear', 'exponential', 'square')} optimizer BaseSearchCV Method for evaluating hyperparameter combinations. From the sklearn group of hyperparameter optimizers. <class 'sklearn.model_selection._search.GridSearchCV'> scorer str Performance metric to optimize in model training. Get available options with list_scorers() function. 'neg_root_mean_squared_error' cv Union[int, sklearn.model_selection._split.BaseCrossValidator, Iterable] Determines the cross-validation splitting strategy. Possible inputs for cv are: None, to use the default 5-fold cross validation, integer, to specify the number of folds in a (Stratified)KFold, CV splitter, An iterable yielding (train, test) splits as arrays of indices. <class 'sklearn.model_selection._split.KFold'> fit_params dict Parameters passed to the fit method of the estimator. None Source code in giants/model.py def AdaBoostRegressor ( self , param_grid : Types . ParameterGrid = ParamGrids . AdaBoostRegressor , optimizer : Types . BaseSearchCV = ModelDefaults . Optimizer , scorer : str = ModelDefaults . RegressionScorer , cv : Types . BaseCrossValidator = ModelDefaults . CVRegression , fit_params : dict = None , ) -> None : \"\"\"Run hyperparameter tuning for an AdaBoostRegression model. Args: param_grid: Hyperparameter values to explore in grid searching. optimizer: Method for evaluating hyperparameter combinations. From the `sklearn` group of hyperparameter optimizers. scorer: Performance metric to optimize in model training. Get available options with `list_scorers()` function. cv: Determines the cross-validation splitting strategy. Possible inputs for cv are: None, to use the default 5-fold cross validation, integer, to specify the number of folds in a (Stratified)KFold, CV splitter, An iterable yielding (train, test) splits as arrays of indices. fit_params: Parameters passed to the fit method of the estimator. \"\"\" self . param_grid = param_grid self . optimizer = optimizer self . scorer = scorer self . cv = cv ( n_splits = self . n_splits ) estimator = ModelEstimators . AdaBoostRegressor () self . _run_grid_search ( estimator ) DecisionTreeClassifier ( self , param_grid = { 'criterion' : ( 'gini' , 'entropy' ), 'splitter' : ( 'best' , 'random' ), 'max_features' : ( 'sqrt' , 'log2' , None ), 'max_depth' : ( 2 , 5 , 10 , None ), 'min_samples_split' : ( 2 , 0.01 , 0.1 )}, optimizer =< class ' sklearn . model_selection . _search . GridSearchCV '>, scorer=' neg_log_loss ', cv=<class ' sklearn . model_selection . _split . StratifiedKFold '>, fit_params=None) \u00b6 Run hyperparameter tuning for a DecisionTreeClassifier model. Parameters: Name Type Description Default param_grid Union[Dict, sklearn.model_selection._search.ParameterGrid] Hyperparameter values to explore in grid searching. {'criterion': ('gini', 'entropy'), 'splitter': ('best', 'random'), 'max_features': ('sqrt', 'log2', None), 'max_depth': (2, 5, 10, None), 'min_samples_split': (2, 0.01, 0.1)} optimizer BaseSearchCV Method for evaluating hyperparameter combinations. From the sklearn group of hyperparameter optimizers. <class 'sklearn.model_selection._search.GridSearchCV'> scorer str Performance metric to optimize in model training. Get available options with list_scorers() function. 'neg_log_loss' cv Union[int, sklearn.model_selection._split.BaseCrossValidator, Iterable] Determines the cross-validation splitting strategy. Possible inputs for cv are: None, to use the default 5-fold cross validation, integer, to specify the number of folds in a (Stratified)KFold, CV splitter, An iterable yielding (train, test) splits as arrays of indices. <class 'sklearn.model_selection._split.StratifiedKFold'> fit_params dict Parameters passed to the fit() method of the estimator. None Source code in giants/model.py def DecisionTreeClassifier ( self , param_grid : Types . ParameterGrid = ParamGrids . DecisionTreeClassifier , optimizer : Types . BaseSearchCV = ModelDefaults . Optimizer , scorer : str = ModelDefaults . ClassificationScorer , cv : Types . BaseCrossValidator = ModelDefaults . CVClassification , fit_params : dict = None , ) -> None : \"\"\"Run hyperparameter tuning for a DecisionTreeClassifier model. Args: param_grid: Hyperparameter values to explore in grid searching. optimizer: Method for evaluating hyperparameter combinations. From the `sklearn` group of hyperparameter optimizers. scorer: Performance metric to optimize in model training. Get available options with `list_scorers()` function. cv: Determines the cross-validation splitting strategy. Possible inputs for cv are: None, to use the default 5-fold cross validation, integer, to specify the number of folds in a (Stratified)KFold, CV splitter, An iterable yielding (train, test) splits as arrays of indices. fit_params: Parameters passed to the `fit()` method of the estimator. \"\"\" self . param_grid = param_grid self . optimizer = optimizer self . scorer = scorer self . cv = cv ( n_splits = self . n_splits ) estimator = ModelEstimators . DecisionTreeClassifier () self . _run_grid_search ( estimator ) GradientBoostingClassifier ( self , param_grid = { 'n_estimators' : ( 10 , 100 , 200 ), 'learning_rate' : ( 0.01 , 0.1 , 0.5 ), 'max_features' : ( 'sqrt' , 'log2' , None ), 'max_depth' : ( 1 , 10 , None ), 'min_samples_split' : ( 2 , 0.1 , 0.01 )}, optimizer =< class ' sklearn . model_selection . _search . GridSearchCV '>, scorer=' neg_log_loss ', cv=<class ' sklearn . model_selection . _split . StratifiedKFold '>, fit_params=None) \u00b6 Run hyperparameter tuning for a GradientBoostingClassifier model. Parameters: Name Type Description Default param_grid Union[Dict, sklearn.model_selection._search.ParameterGrid] Hyperparameter values to explore in grid searching. {'n_estimators': (10, 100, 200), 'learning_rate': (0.01, 0.1, 0.5), 'max_features': ('sqrt', 'log2', None), 'max_depth': (1, 10, None), 'min_samples_split': (2, 0.1, 0.01)} optimizer BaseSearchCV Method for evaluating hyperparameter combinations. From the sklearn group of hyperparameter optimizers. <class 'sklearn.model_selection._search.GridSearchCV'> scorer str Performance metric to optimize in model training. Get available options with list_scorers() function. 'neg_log_loss' cv Union[int, sklearn.model_selection._split.BaseCrossValidator, Iterable] Determines the cross-validation splitting strategy. Possible inputs for cv are: None, to use the default 5-fold cross validation, integer, to specify the number of folds in a (Stratified)KFold, CV splitter, An iterable yielding (train, test) splits as arrays of indices. <class 'sklearn.model_selection._split.StratifiedKFold'> fit_params dict Parameters passed to the fit method of the estimator. None Source code in giants/model.py def GradientBoostingClassifier ( self , param_grid : Types . ParameterGrid = ParamGrids . GradientBoostingClassifier , optimizer : Types . BaseSearchCV = ModelDefaults . Optimizer , scorer : str = ModelDefaults . ClassificationScorer , cv : Types . BaseCrossValidator = ModelDefaults . CVClassification , fit_params : dict = None , ) -> None : \"\"\"Run hyperparameter tuning for a GradientBoostingClassifier model. Args: param_grid: Hyperparameter values to explore in grid searching. optimizer: Method for evaluating hyperparameter combinations. From the `sklearn` group of hyperparameter optimizers. scorer: Performance metric to optimize in model training. Get available options with `list_scorers()` function. cv: Determines the cross-validation splitting strategy. Possible inputs for cv are: None, to use the default 5-fold cross validation, integer, to specify the number of folds in a (Stratified)KFold, CV splitter, An iterable yielding (train, test) splits as arrays of indices. fit_params: Parameters passed to the fit method of the estimator. \"\"\" self . param_grid = param_grid self . optimizer = optimizer self . scorer = scorer self . cv = cv ( n_splits = self . n_splits ) estimator = ModelEstimators . GradientBoostingClassifier () self . _run_grid_search ( estimator ) GradientBoostingRegressor ( self , param_grid = { 'n_estimators' : ( 10 , 100 , 200 ), 'learning_rate' : ( 0.01 , 0.1 , 0.5 ), 'max_features' : ( 'sqrt' , 'log2' , None ), 'max_depth' : ( 1 , 10 , None ), 'min_samples_split' : ( 2 , 0.1 , 0.01 )}, optimizer =< class ' sklearn . model_selection . _search . GridSearchCV '>, scorer=' neg_root_mean_squared_error ', cv=<class ' sklearn . model_selection . _split . KFold '>, fit_params=None) \u00b6 Run hyperparameter tuning for a GradientBoostingRegressor model. Parameters: Name Type Description Default param_grid Union[Dict, sklearn.model_selection._search.ParameterGrid] Hyperparameter values to explore in grid searching. {'n_estimators': (10, 100, 200), 'learning_rate': (0.01, 0.1, 0.5), 'max_features': ('sqrt', 'log2', None), 'max_depth': (1, 10, None), 'min_samples_split': (2, 0.1, 0.01)} optimizer BaseSearchCV Method for evaluating hyperparameter combinations. From the sklearn group of hyperparameter optimizers. <class 'sklearn.model_selection._search.GridSearchCV'> scorer str Performance metric to optimize in model training. Get available options with list_scorers() function. 'neg_root_mean_squared_error' cv Union[int, sklearn.model_selection._split.BaseCrossValidator, Iterable] Determines the cross-validation splitting strategy. Possible inputs for cv are: None, to use the default 5-fold cross validation, integer, to specify the number of folds in a (Stratified)KFold, CV splitter, An iterable yielding (train, test) splits as arrays of indices. <class 'sklearn.model_selection._split.KFold'> fit_params dict Parameters passed to the fit method of the estimator. None Source code in giants/model.py def GradientBoostingRegressor ( self , param_grid : Types . ParameterGrid = ParamGrids . GradientBoostingRegressor , optimizer : Types . BaseSearchCV = ModelDefaults . Optimizer , scorer : str = ModelDefaults . RegressionScorer , cv : Types . BaseCrossValidator = ModelDefaults . CVRegression , fit_params : dict = None , ) -> None : \"\"\"Run hyperparameter tuning for a GradientBoostingRegressor model. Args: param_grid: Hyperparameter values to explore in grid searching. optimizer: Method for evaluating hyperparameter combinations. From the `sklearn` group of hyperparameter optimizers. scorer: Performance metric to optimize in model training. Get available options with `list_scorers()` function. cv: Determines the cross-validation splitting strategy. Possible inputs for cv are: None, to use the default 5-fold cross validation, integer, to specify the number of folds in a (Stratified)KFold, CV splitter, An iterable yielding (train, test) splits as arrays of indices. fit_params: Parameters passed to the fit method of the estimator. \"\"\" self . param_grid = param_grid self . optimizer = optimizer self . scorer = scorer self . cv = cv ( n_splits = self . n_splits ) estimator = ModelEstimators . GradientBoostingRegressor () self . _run_grid_search ( estimator ) LinearRegression ( self , param_grid = { 'fit_intercept' : ( True , False )}, optimizer =< class ' sklearn . model_selection . _search . GridSearchCV '>, scorer=' neg_root_mean_squared_error ', cv=<class ' sklearn . model_selection . _split . KFold '>, fit_params=None) \u00b6 Run hyperparameter tuning for a LinearRegression model. Parameters: Name Type Description Default param_grid Union[Dict, sklearn.model_selection._search.ParameterGrid] Hyperparameter values to explore in grid searching. {'fit_intercept': (True, False)} optimizer BaseSearchCV Method for evaluating hyperparameter combinations. From the sklearn group of hyperparameter optimizers. <class 'sklearn.model_selection._search.GridSearchCV'> scorer str Performance metric to optimize in model training. Get available options with list_scorers() function. 'neg_root_mean_squared_error' cv Union[int, sklearn.model_selection._split.BaseCrossValidator, Iterable] Determines the cross-validation splitting strategy. Possible inputs for cv are: None, to use the default 5-fold cross validation, integer, to specify the number of folds in a (Stratified)KFold, CV splitter, An iterable yielding (train, test) splits as arrays of indices. <class 'sklearn.model_selection._split.KFold'> fit_params dict Parameters passed to the fit() method of the estimator. None Source code in giants/model.py def LinearRegression ( self , param_grid : Types . ParameterGrid = ParamGrids . LinearRegression , optimizer : Types . BaseSearchCV = ModelDefaults . Optimizer , scorer : str = ModelDefaults . RegressionScorer , cv : Types . BaseCrossValidator = ModelDefaults . CVRegression , fit_params : dict = None , ) -> None : \"\"\"Run hyperparameter tuning for a LinearRegression model. Args: param_grid: Hyperparameter values to explore in grid searching. optimizer: Method for evaluating hyperparameter combinations. From the `sklearn` group of hyperparameter optimizers. scorer: Performance metric to optimize in model training. Get available options with `list_scorers()` function. cv: Determines the cross-validation splitting strategy. Possible inputs for cv are: None, to use the default 5-fold cross validation, integer, to specify the number of folds in a (Stratified)KFold, CV splitter, An iterable yielding (train, test) splits as arrays of indices. fit_params: Parameters passed to the `fit()` method of the estimator. \"\"\" self . param_grid = param_grid self . optimizer = optimizer self . scorer = scorer self . cv = cv ( n_splits = self . n_splits ) estimator = ModelEstimators . LinearRegression () self . _run_grid_search ( estimator ) LinearSVC ( self , param_grid = { 'C' : ( 0.01 , 0.1 , 1.0 , 10.0 ), 'loss' : ( 'hinge' , 'squared_hinge' ), 'tol' : ( 0.001 , 0.0001 , 1e-05 ), 'fit_intercept' : ( True , False ), 'class_weight' : ( None , 'balanced' )}, optimizer =< class ' sklearn . model_selection . _search . GridSearchCV '>, scorer=' neg_log_loss ', cv=<class ' sklearn . model_selection . _split . StratifiedKFold '>, fit_params=None) \u00b6 Run hyperparameter tuning for a Linear Support Vector Machine classifier model. Parameters: Name Type Description Default param_grid Union[Dict, sklearn.model_selection._search.ParameterGrid] Hyperparameter values to explore in grid searching. {'C': (0.01, 0.1, 1.0, 10.0), 'loss': ('hinge', 'squared_hinge'), 'tol': (0.001, 0.0001, 1e-05), 'fit_intercept': (True, False), 'class_weight': (None, 'balanced')} optimizer BaseSearchCV Method for evaluating hyperparameter combinations. From the sklearn group of hyperparameter optimizers. <class 'sklearn.model_selection._search.GridSearchCV'> scorer str Performance metric to optimize in model training. Get available options with list_scorers() function. 'neg_log_loss' cv Union[int, sklearn.model_selection._split.BaseCrossValidator, Iterable] Determines the cross-validation splitting strategy. Possible inputs for cv are: None, to use the default 5-fold cross validation, integer, to specify the number of folds in a (Stratified)KFold, CV splitter, An iterable yielding (train, test) splits as arrays of indices. <class 'sklearn.model_selection._split.StratifiedKFold'> fit_params dict Parameters passed to the `fit()`` method of the estimator. None Source code in giants/model.py def LinearSVC ( self , param_grid : Types . ParameterGrid = ParamGrids . LinearSVC , optimizer : Types . BaseSearchCV = ModelDefaults . Optimizer , scorer : str = ModelDefaults . ClassificationScorer , cv : Types . BaseCrossValidator = ModelDefaults . CVClassification , fit_params : dict = None , ) -> None : \"\"\"Run hyperparameter tuning for a Linear Support Vector Machine classifier model. Args: param_grid: Hyperparameter values to explore in grid searching. optimizer: Method for evaluating hyperparameter combinations. From the `sklearn` group of hyperparameter optimizers. scorer: Performance metric to optimize in model training. Get available options with `list_scorers()` function. cv: Determines the cross-validation splitting strategy. Possible inputs for cv are: None, to use the default 5-fold cross validation, integer, to specify the number of folds in a (Stratified)KFold, CV splitter, An iterable yielding (train, test) splits as arrays of indices. fit_params: Parameters passed to the `fit()`` method of the estimator. \"\"\" self . param_grid = param_grid self . optimizer = optimizer self . scorer = scorer self . cv = cv ( n_splits = self . n_splits ) estimator = ModelEstimators . LinearSVC () self . _run_grid_search ( estimator ) LinearSVR ( self , param_grid = { 'C' : ( 0.01 , 0.1 , 1.0 , 10.0 ), 'loss' : ( 'epsilon_insensitive' , 'squared_epsilon_insensitive' ), 'epsilon' : ( 0 , 0.01 , 0.1 ), 'dual' : False , 'tol' : ( 0.001 , 0.0001 , 1e-05 ), 'fit_intercept' : ( True , False )}, optimizer =< class ' sklearn . model_selection . _search . GridSearchCV '>, scorer=' neg_root_mean_squared_error ', cv=<class ' sklearn . model_selection . _split . KFold '>, fit_params=None) \u00b6 Run hyperparameter tuning for a Linear Support Vector Machine regression model. Parameters: Name Type Description Default param_grid Union[Dict, sklearn.model_selection._search.ParameterGrid] Hyperparameter values to explore in grid searching. {'C': (0.01, 0.1, 1.0, 10.0), 'loss': ('epsilon_insensitive', 'squared_epsilon_insensitive'), 'epsilon': (0, 0.01, 0.1), 'dual': False, 'tol': (0.001, 0.0001, 1e-05), 'fit_intercept': (True, False)} optimizer BaseSearchCV Method for evaluating hyperparameter combinations. From the sklearn group of hyperparameter optimizers. <class 'sklearn.model_selection._search.GridSearchCV'> scorer str Performance metric to optimize in model training. Get available options with list_scorers() function. 'neg_root_mean_squared_error' cv Union[int, sklearn.model_selection._split.BaseCrossValidator, Iterable] Determines the cross-validation splitting strategy. Possible inputs for cv are: None, to use the default 5-fold cross validation, integer, to specify the number of folds in a (Stratified)KFold, CV splitter, An iterable yielding (train, test) splits as arrays of indices. <class 'sklearn.model_selection._split.KFold'> fit_params dict Parameters passed to the fit method of the estimator. None Source code in giants/model.py def LinearSVR ( self , param_grid : Types . ParameterGrid = ParamGrids . LinearSVR , optimizer : Types . BaseSearchCV = ModelDefaults . Optimizer , scorer : str = ModelDefaults . RegressionScorer , cv : Types . BaseCrossValidator = ModelDefaults . CVRegression , fit_params : dict = None , ) -> None : \"\"\"Run hyperparameter tuning for a Linear Support Vector Machine regression model. Args: param_grid: Hyperparameter values to explore in grid searching. optimizer: Method for evaluating hyperparameter combinations. From the `sklearn` group of hyperparameter optimizers. scorer: Performance metric to optimize in model training. Get available options with `list_scorers()` function. cv: Determines the cross-validation splitting strategy. Possible inputs for cv are: None, to use the default 5-fold cross validation, integer, to specify the number of folds in a (Stratified)KFold, CV splitter, An iterable yielding (train, test) splits as arrays of indices. fit_params: Parameters passed to the fit method of the estimator. \"\"\" self . param_grid = param_grid self . optimizer = optimizer self . scorer = scorer self . cv = cv ( n_splits = self . n_splits ) estimator = ModelEstimators . LinearSVR () self . _run_grid_search ( estimator ) LogisticRegression ( self , param_grid = { 'C' : ( 0.01 , 0.1 , 1.0 , 10.0 ), 'tol' : ( 0.001 , 0.0001 , 1e-05 ), 'fit_intercept' : ( True , False )}, optimizer =< class ' sklearn . model_selection . _search . GridSearchCV '>, scorer=' neg_root_mean_squared_error ', cv=<class ' sklearn . model_selection . _split . KFold '>, fit_params=None) \u00b6 Run hyperparameter tuning for a LogisticRegression model. Parameters: Name Type Description Default param_grid Union[Dict, sklearn.model_selection._search.ParameterGrid] Hyperparameter values to explore in grid searching. {'C': (0.01, 0.1, 1.0, 10.0), 'tol': (0.001, 0.0001, 1e-05), 'fit_intercept': (True, False)} optimizer BaseSearchCV Method for evaluating hyperparameter combinations. From the sklearn group of hyperparameter optimizers. <class 'sklearn.model_selection._search.GridSearchCV'> scorer str Performance metric to optimize in model training. Get available options with list_scorers() function. 'neg_root_mean_squared_error' cv Union[int, sklearn.model_selection._split.BaseCrossValidator, Iterable] Determines the cross-validation splitting strategy. Possible inputs for cv are: None, to use the default 5-fold cross validation, integer, to specify the number of folds in a (Stratified)KFold, CV splitter, An iterable yielding (train, test) splits as arrays of indices. <class 'sklearn.model_selection._split.KFold'> fit_params dict Parameters passed to the fit() method of the estimator. None Source code in giants/model.py def LogisticRegression ( self , param_grid : Types . ParameterGrid = ParamGrids . LogisticRegression , optimizer : Types . BaseSearchCV = ModelDefaults . Optimizer , scorer : str = ModelDefaults . RegressionScorer , cv : Types . BaseCrossValidator = ModelDefaults . CVRegression , fit_params : dict = None , ) -> None : \"\"\"Run hyperparameter tuning for a LogisticRegression model. Args: param_grid: Hyperparameter values to explore in grid searching. optimizer: Method for evaluating hyperparameter combinations. From the `sklearn` group of hyperparameter optimizers. scorer: Performance metric to optimize in model training. Get available options with `list_scorers()` function. cv: Determines the cross-validation splitting strategy. Possible inputs for cv are: None, to use the default 5-fold cross validation, integer, to specify the number of folds in a (Stratified)KFold, CV splitter, An iterable yielding (train, test) splits as arrays of indices. fit_params: Parameters passed to the `fit()` method of the estimator. \"\"\" self . param_grid = param_grid self . optimizer = optimizer self . scorer = scorer self . cv = cv ( n_splits = self . n_splits ) estimator = ModelEstimators . LogisticRegression () self . _run_grid_search ( estimator ) RandomForestClassifier ( self , param_grid = { 'criterion' : ( 'gini' , 'entropy' ), 'n_estimators' : ( 10 , 100 , 200 ), 'max_features' : ( 'sqrt' , 'log2' , None ), 'max_depth' : ( 1 , 10 , None ), 'min_samples_split' : ( 2 , 0.1 , 0.01 )}, optimizer =< class ' sklearn . model_selection . _search . GridSearchCV '>, scorer=' neg_log_loss ', cv=<class ' sklearn . model_selection . _split . StratifiedKFold '>, fit_params=None) \u00b6 Run hyperparameter tuning for a RandomForestClassifier model. Parameters: Name Type Description Default param_grid Union[Dict, sklearn.model_selection._search.ParameterGrid] Hyperparameter values to explore in grid searching. {'criterion': ('gini', 'entropy'), 'n_estimators': (10, 100, 200), 'max_features': ('sqrt', 'log2', None), 'max_depth': (1, 10, None), 'min_samples_split': (2, 0.1, 0.01)} optimizer BaseSearchCV Method for evaluating hyperparameter combinations. From the sklearn group of hyperparameter optimizers. <class 'sklearn.model_selection._search.GridSearchCV'> scorer str Performance metric to optimize in model training. Get available options with list_scorers() function. 'neg_log_loss' cv Union[int, sklearn.model_selection._split.BaseCrossValidator, Iterable] Determines the cross-validation splitting strategy. Possible inputs for cv are: None, to use the default 5-fold cross validation, integer, to specify the number of folds in a (Stratified)KFold, CV splitter, An iterable yielding (train, test) splits as arrays of indices. <class 'sklearn.model_selection._split.StratifiedKFold'> fit_params dict Parameters passed to the fit method of the estimator. None Source code in giants/model.py def RandomForestClassifier ( self , param_grid : Types . ParameterGrid = ParamGrids . RandomForestClassifier , optimizer : Types . BaseSearchCV = ModelDefaults . Optimizer , scorer : str = ModelDefaults . ClassificationScorer , cv : Types . BaseCrossValidator = ModelDefaults . CVClassification , fit_params : dict = None , ) -> None : \"\"\"Run hyperparameter tuning for a RandomForestClassifier model. Args: param_grid: Hyperparameter values to explore in grid searching. optimizer: Method for evaluating hyperparameter combinations. From the `sklearn` group of hyperparameter optimizers. scorer: Performance metric to optimize in model training. Get available options with `list_scorers()` function. cv: Determines the cross-validation splitting strategy. Possible inputs for cv are: None, to use the default 5-fold cross validation, integer, to specify the number of folds in a (Stratified)KFold, CV splitter, An iterable yielding (train, test) splits as arrays of indices. fit_params: Parameters passed to the fit method of the estimator. \"\"\" self . param_grid = param_grid self . optimizer = optimizer self . scorer = scorer self . cv = cv ( n_splits = self . n_splits ) estimator = ModelEstimators . RandomForestClassifier () self . _run_grid_search ( estimator ) RandomForestRegressor ( self , param_grid = { 'n_estimators' : ( 10 , 100 , 200 ), 'max_features' : ( 'sqrt' , 'log2' , None ), 'max_depth' : ( 1 , 10 , None ), 'min_samples_split' : ( 2 , 0.1 , 0.01 )}, optimizer =< class ' sklearn . model_selection . _search . GridSearchCV '>, scorer=' neg_root_mean_squared_error ', cv=<class ' sklearn . model_selection . _split . KFold '>, fit_params=None) \u00b6 Run hyperparameter tuning for a RandomForestRegressor model. Parameters: Name Type Description Default param_grid Union[Dict, sklearn.model_selection._search.ParameterGrid] Hyperparameter values to explore in grid searching. {'n_estimators': (10, 100, 200), 'max_features': ('sqrt', 'log2', None), 'max_depth': (1, 10, None), 'min_samples_split': (2, 0.1, 0.01)} optimizer BaseSearchCV Method for evaluating hyperparameter combinations. From the sklearn group of hyperparameter optimizers. <class 'sklearn.model_selection._search.GridSearchCV'> scorer str Performance metric to optimize in model training. Get available options with list_scorers() function. 'neg_root_mean_squared_error' cv Union[int, sklearn.model_selection._split.BaseCrossValidator, Iterable] Determines the cross-validation splitting strategy. Possible inputs for cv are: None, to use the default 5-fold cross validation, integer, to specify the number of folds in a (Stratified)KFold, CV splitter, An iterable yielding (train, test) splits as arrays of indices. <class 'sklearn.model_selection._split.KFold'> fit_params dict Parameters passed to the fit method of the estimator. None Source code in giants/model.py def RandomForestRegressor ( self , param_grid : Types . ParameterGrid = ParamGrids . RandomForestRegressor , optimizer : Types . BaseSearchCV = ModelDefaults . Optimizer , scorer : str = ModelDefaults . RegressionScorer , cv : Types . BaseCrossValidator = ModelDefaults . CVRegression , fit_params : dict = None , ) -> None : \"\"\"Run hyperparameter tuning for a RandomForestRegressor model. Args: param_grid: Hyperparameter values to explore in grid searching. optimizer: Method for evaluating hyperparameter combinations. From the `sklearn` group of hyperparameter optimizers. scorer: Performance metric to optimize in model training. Get available options with `list_scorers()` function. cv: Determines the cross-validation splitting strategy. Possible inputs for cv are: None, to use the default 5-fold cross validation, integer, to specify the number of folds in a (Stratified)KFold, CV splitter, An iterable yielding (train, test) splits as arrays of indices. fit_params: Parameters passed to the fit method of the estimator. \"\"\" self . param_grid = param_grid self . optimizer = optimizer self . scorer = scorer self . cv = cv ( n_splits = self . n_splits ) estimator = ModelEstimators . RandomForestRegressor () self . _run_grid_search ( estimator ) SVC ( self , param_grid = { 'C' : ( 0.001 , 0.01 , 0.1 , 1.0 , 10.0 ), 'kernel' : ( 'rbf' , 'linear' ), 'gamma' : ( 0.001 , 0.0001 , 1e-05 , 1e-06 , 1e-07 ), 'class_weight' : ( None , 'balanced' )}, optimizer =< class ' sklearn . model_selection . _search . GridSearchCV '>, scorer=' neg_log_loss ', cv=<class ' sklearn . model_selection . _split . StratifiedKFold '>, fit_params=None) \u00b6 Run hyperparameter tuning for a Support Vector Machine classifier (SVC) model. Parameters: Name Type Description Default param_grid Union[Dict, sklearn.model_selection._search.ParameterGrid] Hyperparameter values to explore in grid searching. {'C': (0.001, 0.01, 0.1, 1.0, 10.0), 'kernel': ('rbf', 'linear'), 'gamma': (0.001, 0.0001, 1e-05, 1e-06, 1e-07), 'class_weight': (None, 'balanced')} optimizer BaseSearchCV Method for evaluating hyperparameter combinations. From the sklearn group of hyperparameter optimizers. <class 'sklearn.model_selection._search.GridSearchCV'> scorer str Performance metric to optimize in model training. Get available options with list_scorers() function. 'neg_log_loss' cv Union[int, sklearn.model_selection._split.BaseCrossValidator, Iterable] Determines the cross-validation splitting strategy. Possible inputs for cv are: None, to use the default 5-fold cross validation, integer, to specify the number of folds in a (Stratified)KFold, CV splitter, An iterable yielding (train, test) splits as arrays of indices. <class 'sklearn.model_selection._split.StratifiedKFold'> fit_params dict Parameters passed to the `fit()`` method of the estimator. None Source code in giants/model.py def SVC ( self , param_grid : Types . ParameterGrid = ParamGrids . SVC , optimizer : Types . BaseSearchCV = ModelDefaults . Optimizer , scorer : str = ModelDefaults . ClassificationScorer , cv : Types . BaseCrossValidator = ModelDefaults . CVClassification , fit_params : dict = None , ) -> None : \"\"\"Run hyperparameter tuning for a Support Vector Machine classifier (SVC) model. Args: param_grid: Hyperparameter values to explore in grid searching. optimizer: Method for evaluating hyperparameter combinations. From the `sklearn` group of hyperparameter optimizers. scorer: Performance metric to optimize in model training. Get available options with `list_scorers()` function. cv: Determines the cross-validation splitting strategy. Possible inputs for cv are: None, to use the default 5-fold cross validation, integer, to specify the number of folds in a (Stratified)KFold, CV splitter, An iterable yielding (train, test) splits as arrays of indices. fit_params: Parameters passed to the `fit()`` method of the estimator. \"\"\" self . param_grid = param_grid self . optimizer = optimizer self . scorer = scorer self . cv = cv ( n_splits = self . n_splits ) estimator = ModelEstimators . SVC () self . _run_grid_search ( estimator ) SVR ( self , param_grid = { 'C' : ( 0.01 , 0.1 , 1.0 , 10.0 ), 'epsilon' : ( 0.01 , 0.1 , 1 ), 'kernel' : ( 'rbf' , 'linear' , 'poly' , 'sigmoid' ), 'gamma' : ( 0.01 , 0.001 , 0.0001 )}, optimizer =< class ' sklearn . model_selection . _search . GridSearchCV '>, scorer=' neg_root_mean_squared_error ', cv=<class ' sklearn . model_selection . _split . KFold '>, fit_params=None) \u00b6 Run hyperparameter tuning for a Support Vector Machine Regression (SVR) model. Parameters: Name Type Description Default param_grid Union[Dict, sklearn.model_selection._search.ParameterGrid] Hyperparameter values to explore in grid searching. {'C': (0.01, 0.1, 1.0, 10.0), 'epsilon': (0.01, 0.1, 1), 'kernel': ('rbf', 'linear', 'poly', 'sigmoid'), 'gamma': (0.01, 0.001, 0.0001)} optimizer BaseSearchCV Method for evaluating hyperparameter combinations. From the sklearn group of hyperparameter optimizers. <class 'sklearn.model_selection._search.GridSearchCV'> scorer str Performance metric to optimize in model training. Get available options with list_scorers() function. 'neg_root_mean_squared_error' cv Union[int, sklearn.model_selection._split.BaseCrossValidator, Iterable] Determines the cross-validation splitting strategy. Possible inputs for cv are: None, to use the default 5-fold cross validation, integer, to specify the number of folds in a (Stratified)KFold, CV splitter, An iterable yielding (train, test) splits as arrays of indices. <class 'sklearn.model_selection._split.KFold'> fit_params dict Parameters passed to the fit method of the estimator. None Source code in giants/model.py def SVR ( self , param_grid : Types . ParameterGrid = ParamGrids . SVR , optimizer : Types . BaseSearchCV = ModelDefaults . Optimizer , scorer : str = ModelDefaults . RegressionScorer , cv : Types . BaseCrossValidator = ModelDefaults . CVRegression , fit_params : dict = None , ) -> None : \"\"\"Run hyperparameter tuning for a Support Vector Machine Regression (SVR) model. Args: param_grid: Hyperparameter values to explore in grid searching. optimizer: Method for evaluating hyperparameter combinations. From the `sklearn` group of hyperparameter optimizers. scorer: Performance metric to optimize in model training. Get available options with `list_scorers()` function. cv: Determines the cross-validation splitting strategy. Possible inputs for cv are: None, to use the default 5-fold cross validation, integer, to specify the number of folds in a (Stratified)KFold, CV splitter, An iterable yielding (train, test) splits as arrays of indices. fit_params: Parameters passed to the fit method of the estimator. \"\"\" self . param_grid = param_grid self . optimizer = optimizer self . scorer = scorer self . cv = cv ( n_splits = self . n_splits ) estimator = ModelEstimators . SVR () self . _run_grid_search ( estimator ) __init__ ( self , x , y , n_splits = 4 , n_jobs = 2 , refit = True , verbose = 1 , error_score = 0 , return_train_score = False ) special \u00b6 Performs hyperparameter searches to optimize model performance using sklearn . Parameters: Name Type Description Default x {array-like, sparse matrix} of shape (n_samples, n_features) The input samples. sklearn will convert this to a dtype=np.float32 array or to a sparse csr_matrix . required y ndarray array-like of shape (n_samples,) Target values (strings or integers in classification, real numbers in regression) For classification, labels correspond to classes. required n_splits int Number of train/test splits to evaluate in cross-validation. 4 n_jobs int Number of simultaneous grid search processes to run. 2 refit bool Whether to fit a new model each time a model class is instantiated. True verbose int Level of model training reporting logged. 1 error_score Union[str, numbers.Number] Value to assign to the score if an error occurs in estimator fitting. If set to \u2018raise\u2019, the error is raised. If a numeric value is given, FitFailedWarning is raised. 0 return_train_score bool Get insights on how different parameter settings impact overfitting/underfitting. But computing scores can be expensive and is not required to select parameters that yield the best generalization performance. False Returns: Type Description object a Tuner object that can perform hyperparameter searches and optimize model performance. Source code in giants/model.py def __init__ ( self , x : Types . Array , y : Types . Array , n_splits : int = ModelDefaults . NumberOfSplits , n_jobs : int = ModelDefaults . NumberOfJobs , refit : bool = ModelDefaults . RefitModels , verbose : int = ModelDefaults . Verbosity , error_score : Types . ErrorScore = ModelDefaults . ErrorScore , return_train_score : bool = ModelDefaults . ReturnTrainScore , ) -> object : \"\"\"Performs hyperparameter searches to optimize model performance using `sklearn`. Args: x : {array-like, sparse matrix} of shape (n_samples, n_features) The input samples. `sklearn` will convert this to a `dtype=np.float32` array or to a sparse `csr_matrix`. y: array-like of shape (n_samples,) Target values (strings or integers in classification, real numbers in regression) For classification, labels correspond to classes. n_splits: Number of train/test splits to evaluate in cross-validation. n_jobs: Number of simultaneous grid search processes to run. refit: Whether to fit a new model each time a model class is instantiated. verbose: Level of model training reporting logged. error_score: Value to assign to the score if an error occurs in estimator fitting. If set to \u2018raise\u2019, the error is raised. If a numeric value is given, FitFailedWarning is raised. return_train_score: Get insights on how different parameter settings impact overfitting/underfitting. But computing scores can be expensive and is not required to select parameters that yield the best generalization performance. Returns: a Tuner object that can perform hyperparameter searches and optimize model performance. \"\"\" self . x = x self . y = y self . n_splits = n_splits self . n_jobs = n_jobs self . refit = refit self . verbose = verbose self . error_score = error_score self . return_train_score = return_train_score list_scorers ( method = None ) \u00b6 Returns a list of viable scorers for grid search optimization. Parameters: Name Type Description Default method str Modeling method. Options: [\"classification\", \"regression\", \"clustering\"]. If None , returns a dictionary with all available scorers. These methods are maintained manually and may become out of date as sklearn implements changes. Running list_scorers() without a method should provide the full list of sklearn scorers via their api. None Returns: Type Description Union[List, Dict] viable model performance scorer metrics. Source code in giants/model.py def list_scorers ( method : str = None ) -> Types . ScorerList : \"\"\"Returns a list of viable scorers for grid search optimization. Args: method: Modeling method. Options: [\"classification\", \"regression\", \"clustering\"]. If `None`, returns a dictionary with all available scorers. These methods are maintained manually and may become out of date as sklearn implements changes. Running `list_scorers()` without a method should provide the full list of `sklearn` scorers via their api. Returns: viable model performance scorer metrics. \"\"\" all_scorers = _scorer . SCORERS classification_scorers = [ \"accuracy\" , \"average_precision\" , \"balanced_accuracy\" , \"f1\" , \"f1_micro\" , \"f1_macro\" , \"f1_weighted\" , \"f1_samples\" , \"jaccard\" , \"neg_log_loss\" , \"neg_brier_score\" , \"precision\" , \"recall\" , \"roc_auc\" , \"roc_auc_ovo\" , \"roc_auc_ovr\" , \"roc_auc_ovo_weighted\" , \"roc_auc_ovr_weighted\" , \"top_k_accuracy\" , ] regression_scorers = [ \"explained_variance\" , \"max_error\" , \"neg_mean_absolute_error\" , \"neg_mean_poisson_deviance\" , \"neg_mean_gamma_deviance\" , \"neg_mean_absolute_percentage_error\" , \"neg_mean_squared_error\" , \"neg_mean_squared_log_error\" , \"neg_median_absolute_error\" , \"neg_root_mean_squared_error\" , \"r2\" , ] clustering_scorers = [ \"adjusted_mutual_info_score\" , \"adjusted_rand_score\" , \"completeness_score\" , \"fowlkes_mallows_score\" , \"homogeneity_score\" , \"mutual_info_score\" , \"normalized_mutual_info_score\" , \"rand_score\" , \"v_measure_score\" , ] if method is None : return all_scorers else : if method . lower () == \"classification\" : return classification_scorers elif method . lower () == \"regression\" : return regression_scorers elif method . lower () == \"clustering\" : return clustering_scorers else : return all_scorers","title":"giants.model"},{"location":"module/model/#giants.model.Tuner","text":"A class for performing hyperparameter searches using sklearn models.","title":"Tuner"},{"location":"module/model/#giants.model.Tuner.AdaBoostClassifier","text":"Run hyperparameter tuning for an AdaBoostClassifier model. Parameters: Name Type Description Default param_grid Union[Dict, sklearn.model_selection._search.ParameterGrid] Hyperparameter values to explore in grid searching. {'n_estimators': (25, 50, 75, 100), 'learning_rate': (0.1, 0.5, 1.0)} optimizer BaseSearchCV Method for evaluating hyperparameter combinations. From the sklearn group of hyperparameter optimizers. <class 'sklearn.model_selection._search.GridSearchCV'> scorer str Performance metric to optimize in model training. Get available options with list_scorers() function. 'neg_log_loss' cv Union[int, sklearn.model_selection._split.BaseCrossValidator, Iterable] Determines the cross-validation splitting strategy. Possible inputs for cv are: None, to use the default 5-fold cross validation, integer, to specify the number of folds in a (Stratified)KFold, CV splitter, An iterable yielding (train, test) splits as arrays of indices. <class 'sklearn.model_selection._split.StratifiedKFold'> fit_params dict Parameters passed to the `fit()`` method of the estimator. None Source code in giants/model.py def AdaBoostClassifier ( self , param_grid : Types . ParameterGrid = ParamGrids . AdaBoostClassifier , optimizer : Types . BaseSearchCV = ModelDefaults . Optimizer , scorer : str = ModelDefaults . ClassificationScorer , cv : Types . BaseCrossValidator = ModelDefaults . CVClassification , fit_params : dict = None , ) -> None : \"\"\"Run hyperparameter tuning for an AdaBoostClassifier model. Args: param_grid: Hyperparameter values to explore in grid searching. optimizer: Method for evaluating hyperparameter combinations. From the `sklearn` group of hyperparameter optimizers. scorer: Performance metric to optimize in model training. Get available options with `list_scorers()` function. cv: Determines the cross-validation splitting strategy. Possible inputs for cv are: None, to use the default 5-fold cross validation, integer, to specify the number of folds in a (Stratified)KFold, CV splitter, An iterable yielding (train, test) splits as arrays of indices. fit_params: Parameters passed to the `fit()`` method of the estimator. \"\"\" self . param_grid = param_grid self . optimizer = optimizer self . scorer = scorer self . cv = cv ( n_splits = self . n_splits ) estimator = ModelEstimators . AdaBoostClassifier () self . _run_grid_search ( estimator )","title":"AdaBoostClassifier()"},{"location":"module/model/#giants.model.Tuner.AdaBoostRegressor","text":"Run hyperparameter tuning for an AdaBoostRegression model. Parameters: Name Type Description Default param_grid Union[Dict, sklearn.model_selection._search.ParameterGrid] Hyperparameter values to explore in grid searching. {'n_estimators': (25, 50, 75, 100), 'learning_rate': (0.1, 0.5, 1.0), 'loss': ('linear', 'exponential', 'square')} optimizer BaseSearchCV Method for evaluating hyperparameter combinations. From the sklearn group of hyperparameter optimizers. <class 'sklearn.model_selection._search.GridSearchCV'> scorer str Performance metric to optimize in model training. Get available options with list_scorers() function. 'neg_root_mean_squared_error' cv Union[int, sklearn.model_selection._split.BaseCrossValidator, Iterable] Determines the cross-validation splitting strategy. Possible inputs for cv are: None, to use the default 5-fold cross validation, integer, to specify the number of folds in a (Stratified)KFold, CV splitter, An iterable yielding (train, test) splits as arrays of indices. <class 'sklearn.model_selection._split.KFold'> fit_params dict Parameters passed to the fit method of the estimator. None Source code in giants/model.py def AdaBoostRegressor ( self , param_grid : Types . ParameterGrid = ParamGrids . AdaBoostRegressor , optimizer : Types . BaseSearchCV = ModelDefaults . Optimizer , scorer : str = ModelDefaults . RegressionScorer , cv : Types . BaseCrossValidator = ModelDefaults . CVRegression , fit_params : dict = None , ) -> None : \"\"\"Run hyperparameter tuning for an AdaBoostRegression model. Args: param_grid: Hyperparameter values to explore in grid searching. optimizer: Method for evaluating hyperparameter combinations. From the `sklearn` group of hyperparameter optimizers. scorer: Performance metric to optimize in model training. Get available options with `list_scorers()` function. cv: Determines the cross-validation splitting strategy. Possible inputs for cv are: None, to use the default 5-fold cross validation, integer, to specify the number of folds in a (Stratified)KFold, CV splitter, An iterable yielding (train, test) splits as arrays of indices. fit_params: Parameters passed to the fit method of the estimator. \"\"\" self . param_grid = param_grid self . optimizer = optimizer self . scorer = scorer self . cv = cv ( n_splits = self . n_splits ) estimator = ModelEstimators . AdaBoostRegressor () self . _run_grid_search ( estimator )","title":"AdaBoostRegressor()"},{"location":"module/model/#giants.model.Tuner.DecisionTreeClassifier","text":"Run hyperparameter tuning for a DecisionTreeClassifier model. Parameters: Name Type Description Default param_grid Union[Dict, sklearn.model_selection._search.ParameterGrid] Hyperparameter values to explore in grid searching. {'criterion': ('gini', 'entropy'), 'splitter': ('best', 'random'), 'max_features': ('sqrt', 'log2', None), 'max_depth': (2, 5, 10, None), 'min_samples_split': (2, 0.01, 0.1)} optimizer BaseSearchCV Method for evaluating hyperparameter combinations. From the sklearn group of hyperparameter optimizers. <class 'sklearn.model_selection._search.GridSearchCV'> scorer str Performance metric to optimize in model training. Get available options with list_scorers() function. 'neg_log_loss' cv Union[int, sklearn.model_selection._split.BaseCrossValidator, Iterable] Determines the cross-validation splitting strategy. Possible inputs for cv are: None, to use the default 5-fold cross validation, integer, to specify the number of folds in a (Stratified)KFold, CV splitter, An iterable yielding (train, test) splits as arrays of indices. <class 'sklearn.model_selection._split.StratifiedKFold'> fit_params dict Parameters passed to the fit() method of the estimator. None Source code in giants/model.py def DecisionTreeClassifier ( self , param_grid : Types . ParameterGrid = ParamGrids . DecisionTreeClassifier , optimizer : Types . BaseSearchCV = ModelDefaults . Optimizer , scorer : str = ModelDefaults . ClassificationScorer , cv : Types . BaseCrossValidator = ModelDefaults . CVClassification , fit_params : dict = None , ) -> None : \"\"\"Run hyperparameter tuning for a DecisionTreeClassifier model. Args: param_grid: Hyperparameter values to explore in grid searching. optimizer: Method for evaluating hyperparameter combinations. From the `sklearn` group of hyperparameter optimizers. scorer: Performance metric to optimize in model training. Get available options with `list_scorers()` function. cv: Determines the cross-validation splitting strategy. Possible inputs for cv are: None, to use the default 5-fold cross validation, integer, to specify the number of folds in a (Stratified)KFold, CV splitter, An iterable yielding (train, test) splits as arrays of indices. fit_params: Parameters passed to the `fit()` method of the estimator. \"\"\" self . param_grid = param_grid self . optimizer = optimizer self . scorer = scorer self . cv = cv ( n_splits = self . n_splits ) estimator = ModelEstimators . DecisionTreeClassifier () self . _run_grid_search ( estimator )","title":"DecisionTreeClassifier()"},{"location":"module/model/#giants.model.Tuner.GradientBoostingClassifier","text":"Run hyperparameter tuning for a GradientBoostingClassifier model. Parameters: Name Type Description Default param_grid Union[Dict, sklearn.model_selection._search.ParameterGrid] Hyperparameter values to explore in grid searching. {'n_estimators': (10, 100, 200), 'learning_rate': (0.01, 0.1, 0.5), 'max_features': ('sqrt', 'log2', None), 'max_depth': (1, 10, None), 'min_samples_split': (2, 0.1, 0.01)} optimizer BaseSearchCV Method for evaluating hyperparameter combinations. From the sklearn group of hyperparameter optimizers. <class 'sklearn.model_selection._search.GridSearchCV'> scorer str Performance metric to optimize in model training. Get available options with list_scorers() function. 'neg_log_loss' cv Union[int, sklearn.model_selection._split.BaseCrossValidator, Iterable] Determines the cross-validation splitting strategy. Possible inputs for cv are: None, to use the default 5-fold cross validation, integer, to specify the number of folds in a (Stratified)KFold, CV splitter, An iterable yielding (train, test) splits as arrays of indices. <class 'sklearn.model_selection._split.StratifiedKFold'> fit_params dict Parameters passed to the fit method of the estimator. None Source code in giants/model.py def GradientBoostingClassifier ( self , param_grid : Types . ParameterGrid = ParamGrids . GradientBoostingClassifier , optimizer : Types . BaseSearchCV = ModelDefaults . Optimizer , scorer : str = ModelDefaults . ClassificationScorer , cv : Types . BaseCrossValidator = ModelDefaults . CVClassification , fit_params : dict = None , ) -> None : \"\"\"Run hyperparameter tuning for a GradientBoostingClassifier model. Args: param_grid: Hyperparameter values to explore in grid searching. optimizer: Method for evaluating hyperparameter combinations. From the `sklearn` group of hyperparameter optimizers. scorer: Performance metric to optimize in model training. Get available options with `list_scorers()` function. cv: Determines the cross-validation splitting strategy. Possible inputs for cv are: None, to use the default 5-fold cross validation, integer, to specify the number of folds in a (Stratified)KFold, CV splitter, An iterable yielding (train, test) splits as arrays of indices. fit_params: Parameters passed to the fit method of the estimator. \"\"\" self . param_grid = param_grid self . optimizer = optimizer self . scorer = scorer self . cv = cv ( n_splits = self . n_splits ) estimator = ModelEstimators . GradientBoostingClassifier () self . _run_grid_search ( estimator )","title":"GradientBoostingClassifier()"},{"location":"module/model/#giants.model.Tuner.GradientBoostingRegressor","text":"Run hyperparameter tuning for a GradientBoostingRegressor model. Parameters: Name Type Description Default param_grid Union[Dict, sklearn.model_selection._search.ParameterGrid] Hyperparameter values to explore in grid searching. {'n_estimators': (10, 100, 200), 'learning_rate': (0.01, 0.1, 0.5), 'max_features': ('sqrt', 'log2', None), 'max_depth': (1, 10, None), 'min_samples_split': (2, 0.1, 0.01)} optimizer BaseSearchCV Method for evaluating hyperparameter combinations. From the sklearn group of hyperparameter optimizers. <class 'sklearn.model_selection._search.GridSearchCV'> scorer str Performance metric to optimize in model training. Get available options with list_scorers() function. 'neg_root_mean_squared_error' cv Union[int, sklearn.model_selection._split.BaseCrossValidator, Iterable] Determines the cross-validation splitting strategy. Possible inputs for cv are: None, to use the default 5-fold cross validation, integer, to specify the number of folds in a (Stratified)KFold, CV splitter, An iterable yielding (train, test) splits as arrays of indices. <class 'sklearn.model_selection._split.KFold'> fit_params dict Parameters passed to the fit method of the estimator. None Source code in giants/model.py def GradientBoostingRegressor ( self , param_grid : Types . ParameterGrid = ParamGrids . GradientBoostingRegressor , optimizer : Types . BaseSearchCV = ModelDefaults . Optimizer , scorer : str = ModelDefaults . RegressionScorer , cv : Types . BaseCrossValidator = ModelDefaults . CVRegression , fit_params : dict = None , ) -> None : \"\"\"Run hyperparameter tuning for a GradientBoostingRegressor model. Args: param_grid: Hyperparameter values to explore in grid searching. optimizer: Method for evaluating hyperparameter combinations. From the `sklearn` group of hyperparameter optimizers. scorer: Performance metric to optimize in model training. Get available options with `list_scorers()` function. cv: Determines the cross-validation splitting strategy. Possible inputs for cv are: None, to use the default 5-fold cross validation, integer, to specify the number of folds in a (Stratified)KFold, CV splitter, An iterable yielding (train, test) splits as arrays of indices. fit_params: Parameters passed to the fit method of the estimator. \"\"\" self . param_grid = param_grid self . optimizer = optimizer self . scorer = scorer self . cv = cv ( n_splits = self . n_splits ) estimator = ModelEstimators . GradientBoostingRegressor () self . _run_grid_search ( estimator )","title":"GradientBoostingRegressor()"},{"location":"module/model/#giants.model.Tuner.LinearRegression","text":"Run hyperparameter tuning for a LinearRegression model. Parameters: Name Type Description Default param_grid Union[Dict, sklearn.model_selection._search.ParameterGrid] Hyperparameter values to explore in grid searching. {'fit_intercept': (True, False)} optimizer BaseSearchCV Method for evaluating hyperparameter combinations. From the sklearn group of hyperparameter optimizers. <class 'sklearn.model_selection._search.GridSearchCV'> scorer str Performance metric to optimize in model training. Get available options with list_scorers() function. 'neg_root_mean_squared_error' cv Union[int, sklearn.model_selection._split.BaseCrossValidator, Iterable] Determines the cross-validation splitting strategy. Possible inputs for cv are: None, to use the default 5-fold cross validation, integer, to specify the number of folds in a (Stratified)KFold, CV splitter, An iterable yielding (train, test) splits as arrays of indices. <class 'sklearn.model_selection._split.KFold'> fit_params dict Parameters passed to the fit() method of the estimator. None Source code in giants/model.py def LinearRegression ( self , param_grid : Types . ParameterGrid = ParamGrids . LinearRegression , optimizer : Types . BaseSearchCV = ModelDefaults . Optimizer , scorer : str = ModelDefaults . RegressionScorer , cv : Types . BaseCrossValidator = ModelDefaults . CVRegression , fit_params : dict = None , ) -> None : \"\"\"Run hyperparameter tuning for a LinearRegression model. Args: param_grid: Hyperparameter values to explore in grid searching. optimizer: Method for evaluating hyperparameter combinations. From the `sklearn` group of hyperparameter optimizers. scorer: Performance metric to optimize in model training. Get available options with `list_scorers()` function. cv: Determines the cross-validation splitting strategy. Possible inputs for cv are: None, to use the default 5-fold cross validation, integer, to specify the number of folds in a (Stratified)KFold, CV splitter, An iterable yielding (train, test) splits as arrays of indices. fit_params: Parameters passed to the `fit()` method of the estimator. \"\"\" self . param_grid = param_grid self . optimizer = optimizer self . scorer = scorer self . cv = cv ( n_splits = self . n_splits ) estimator = ModelEstimators . LinearRegression () self . _run_grid_search ( estimator )","title":"LinearRegression()"},{"location":"module/model/#giants.model.Tuner.LinearSVC","text":"Run hyperparameter tuning for a Linear Support Vector Machine classifier model. Parameters: Name Type Description Default param_grid Union[Dict, sklearn.model_selection._search.ParameterGrid] Hyperparameter values to explore in grid searching. {'C': (0.01, 0.1, 1.0, 10.0), 'loss': ('hinge', 'squared_hinge'), 'tol': (0.001, 0.0001, 1e-05), 'fit_intercept': (True, False), 'class_weight': (None, 'balanced')} optimizer BaseSearchCV Method for evaluating hyperparameter combinations. From the sklearn group of hyperparameter optimizers. <class 'sklearn.model_selection._search.GridSearchCV'> scorer str Performance metric to optimize in model training. Get available options with list_scorers() function. 'neg_log_loss' cv Union[int, sklearn.model_selection._split.BaseCrossValidator, Iterable] Determines the cross-validation splitting strategy. Possible inputs for cv are: None, to use the default 5-fold cross validation, integer, to specify the number of folds in a (Stratified)KFold, CV splitter, An iterable yielding (train, test) splits as arrays of indices. <class 'sklearn.model_selection._split.StratifiedKFold'> fit_params dict Parameters passed to the `fit()`` method of the estimator. None Source code in giants/model.py def LinearSVC ( self , param_grid : Types . ParameterGrid = ParamGrids . LinearSVC , optimizer : Types . BaseSearchCV = ModelDefaults . Optimizer , scorer : str = ModelDefaults . ClassificationScorer , cv : Types . BaseCrossValidator = ModelDefaults . CVClassification , fit_params : dict = None , ) -> None : \"\"\"Run hyperparameter tuning for a Linear Support Vector Machine classifier model. Args: param_grid: Hyperparameter values to explore in grid searching. optimizer: Method for evaluating hyperparameter combinations. From the `sklearn` group of hyperparameter optimizers. scorer: Performance metric to optimize in model training. Get available options with `list_scorers()` function. cv: Determines the cross-validation splitting strategy. Possible inputs for cv are: None, to use the default 5-fold cross validation, integer, to specify the number of folds in a (Stratified)KFold, CV splitter, An iterable yielding (train, test) splits as arrays of indices. fit_params: Parameters passed to the `fit()`` method of the estimator. \"\"\" self . param_grid = param_grid self . optimizer = optimizer self . scorer = scorer self . cv = cv ( n_splits = self . n_splits ) estimator = ModelEstimators . LinearSVC () self . _run_grid_search ( estimator )","title":"LinearSVC()"},{"location":"module/model/#giants.model.Tuner.LinearSVR","text":"Run hyperparameter tuning for a Linear Support Vector Machine regression model. Parameters: Name Type Description Default param_grid Union[Dict, sklearn.model_selection._search.ParameterGrid] Hyperparameter values to explore in grid searching. {'C': (0.01, 0.1, 1.0, 10.0), 'loss': ('epsilon_insensitive', 'squared_epsilon_insensitive'), 'epsilon': (0, 0.01, 0.1), 'dual': False, 'tol': (0.001, 0.0001, 1e-05), 'fit_intercept': (True, False)} optimizer BaseSearchCV Method for evaluating hyperparameter combinations. From the sklearn group of hyperparameter optimizers. <class 'sklearn.model_selection._search.GridSearchCV'> scorer str Performance metric to optimize in model training. Get available options with list_scorers() function. 'neg_root_mean_squared_error' cv Union[int, sklearn.model_selection._split.BaseCrossValidator, Iterable] Determines the cross-validation splitting strategy. Possible inputs for cv are: None, to use the default 5-fold cross validation, integer, to specify the number of folds in a (Stratified)KFold, CV splitter, An iterable yielding (train, test) splits as arrays of indices. <class 'sklearn.model_selection._split.KFold'> fit_params dict Parameters passed to the fit method of the estimator. None Source code in giants/model.py def LinearSVR ( self , param_grid : Types . ParameterGrid = ParamGrids . LinearSVR , optimizer : Types . BaseSearchCV = ModelDefaults . Optimizer , scorer : str = ModelDefaults . RegressionScorer , cv : Types . BaseCrossValidator = ModelDefaults . CVRegression , fit_params : dict = None , ) -> None : \"\"\"Run hyperparameter tuning for a Linear Support Vector Machine regression model. Args: param_grid: Hyperparameter values to explore in grid searching. optimizer: Method for evaluating hyperparameter combinations. From the `sklearn` group of hyperparameter optimizers. scorer: Performance metric to optimize in model training. Get available options with `list_scorers()` function. cv: Determines the cross-validation splitting strategy. Possible inputs for cv are: None, to use the default 5-fold cross validation, integer, to specify the number of folds in a (Stratified)KFold, CV splitter, An iterable yielding (train, test) splits as arrays of indices. fit_params: Parameters passed to the fit method of the estimator. \"\"\" self . param_grid = param_grid self . optimizer = optimizer self . scorer = scorer self . cv = cv ( n_splits = self . n_splits ) estimator = ModelEstimators . LinearSVR () self . _run_grid_search ( estimator )","title":"LinearSVR()"},{"location":"module/model/#giants.model.Tuner.LogisticRegression","text":"Run hyperparameter tuning for a LogisticRegression model. Parameters: Name Type Description Default param_grid Union[Dict, sklearn.model_selection._search.ParameterGrid] Hyperparameter values to explore in grid searching. {'C': (0.01, 0.1, 1.0, 10.0), 'tol': (0.001, 0.0001, 1e-05), 'fit_intercept': (True, False)} optimizer BaseSearchCV Method for evaluating hyperparameter combinations. From the sklearn group of hyperparameter optimizers. <class 'sklearn.model_selection._search.GridSearchCV'> scorer str Performance metric to optimize in model training. Get available options with list_scorers() function. 'neg_root_mean_squared_error' cv Union[int, sklearn.model_selection._split.BaseCrossValidator, Iterable] Determines the cross-validation splitting strategy. Possible inputs for cv are: None, to use the default 5-fold cross validation, integer, to specify the number of folds in a (Stratified)KFold, CV splitter, An iterable yielding (train, test) splits as arrays of indices. <class 'sklearn.model_selection._split.KFold'> fit_params dict Parameters passed to the fit() method of the estimator. None Source code in giants/model.py def LogisticRegression ( self , param_grid : Types . ParameterGrid = ParamGrids . LogisticRegression , optimizer : Types . BaseSearchCV = ModelDefaults . Optimizer , scorer : str = ModelDefaults . RegressionScorer , cv : Types . BaseCrossValidator = ModelDefaults . CVRegression , fit_params : dict = None , ) -> None : \"\"\"Run hyperparameter tuning for a LogisticRegression model. Args: param_grid: Hyperparameter values to explore in grid searching. optimizer: Method for evaluating hyperparameter combinations. From the `sklearn` group of hyperparameter optimizers. scorer: Performance metric to optimize in model training. Get available options with `list_scorers()` function. cv: Determines the cross-validation splitting strategy. Possible inputs for cv are: None, to use the default 5-fold cross validation, integer, to specify the number of folds in a (Stratified)KFold, CV splitter, An iterable yielding (train, test) splits as arrays of indices. fit_params: Parameters passed to the `fit()` method of the estimator. \"\"\" self . param_grid = param_grid self . optimizer = optimizer self . scorer = scorer self . cv = cv ( n_splits = self . n_splits ) estimator = ModelEstimators . LogisticRegression () self . _run_grid_search ( estimator )","title":"LogisticRegression()"},{"location":"module/model/#giants.model.Tuner.RandomForestClassifier","text":"Run hyperparameter tuning for a RandomForestClassifier model. Parameters: Name Type Description Default param_grid Union[Dict, sklearn.model_selection._search.ParameterGrid] Hyperparameter values to explore in grid searching. {'criterion': ('gini', 'entropy'), 'n_estimators': (10, 100, 200), 'max_features': ('sqrt', 'log2', None), 'max_depth': (1, 10, None), 'min_samples_split': (2, 0.1, 0.01)} optimizer BaseSearchCV Method for evaluating hyperparameter combinations. From the sklearn group of hyperparameter optimizers. <class 'sklearn.model_selection._search.GridSearchCV'> scorer str Performance metric to optimize in model training. Get available options with list_scorers() function. 'neg_log_loss' cv Union[int, sklearn.model_selection._split.BaseCrossValidator, Iterable] Determines the cross-validation splitting strategy. Possible inputs for cv are: None, to use the default 5-fold cross validation, integer, to specify the number of folds in a (Stratified)KFold, CV splitter, An iterable yielding (train, test) splits as arrays of indices. <class 'sklearn.model_selection._split.StratifiedKFold'> fit_params dict Parameters passed to the fit method of the estimator. None Source code in giants/model.py def RandomForestClassifier ( self , param_grid : Types . ParameterGrid = ParamGrids . RandomForestClassifier , optimizer : Types . BaseSearchCV = ModelDefaults . Optimizer , scorer : str = ModelDefaults . ClassificationScorer , cv : Types . BaseCrossValidator = ModelDefaults . CVClassification , fit_params : dict = None , ) -> None : \"\"\"Run hyperparameter tuning for a RandomForestClassifier model. Args: param_grid: Hyperparameter values to explore in grid searching. optimizer: Method for evaluating hyperparameter combinations. From the `sklearn` group of hyperparameter optimizers. scorer: Performance metric to optimize in model training. Get available options with `list_scorers()` function. cv: Determines the cross-validation splitting strategy. Possible inputs for cv are: None, to use the default 5-fold cross validation, integer, to specify the number of folds in a (Stratified)KFold, CV splitter, An iterable yielding (train, test) splits as arrays of indices. fit_params: Parameters passed to the fit method of the estimator. \"\"\" self . param_grid = param_grid self . optimizer = optimizer self . scorer = scorer self . cv = cv ( n_splits = self . n_splits ) estimator = ModelEstimators . RandomForestClassifier () self . _run_grid_search ( estimator )","title":"RandomForestClassifier()"},{"location":"module/model/#giants.model.Tuner.RandomForestRegressor","text":"Run hyperparameter tuning for a RandomForestRegressor model. Parameters: Name Type Description Default param_grid Union[Dict, sklearn.model_selection._search.ParameterGrid] Hyperparameter values to explore in grid searching. {'n_estimators': (10, 100, 200), 'max_features': ('sqrt', 'log2', None), 'max_depth': (1, 10, None), 'min_samples_split': (2, 0.1, 0.01)} optimizer BaseSearchCV Method for evaluating hyperparameter combinations. From the sklearn group of hyperparameter optimizers. <class 'sklearn.model_selection._search.GridSearchCV'> scorer str Performance metric to optimize in model training. Get available options with list_scorers() function. 'neg_root_mean_squared_error' cv Union[int, sklearn.model_selection._split.BaseCrossValidator, Iterable] Determines the cross-validation splitting strategy. Possible inputs for cv are: None, to use the default 5-fold cross validation, integer, to specify the number of folds in a (Stratified)KFold, CV splitter, An iterable yielding (train, test) splits as arrays of indices. <class 'sklearn.model_selection._split.KFold'> fit_params dict Parameters passed to the fit method of the estimator. None Source code in giants/model.py def RandomForestRegressor ( self , param_grid : Types . ParameterGrid = ParamGrids . RandomForestRegressor , optimizer : Types . BaseSearchCV = ModelDefaults . Optimizer , scorer : str = ModelDefaults . RegressionScorer , cv : Types . BaseCrossValidator = ModelDefaults . CVRegression , fit_params : dict = None , ) -> None : \"\"\"Run hyperparameter tuning for a RandomForestRegressor model. Args: param_grid: Hyperparameter values to explore in grid searching. optimizer: Method for evaluating hyperparameter combinations. From the `sklearn` group of hyperparameter optimizers. scorer: Performance metric to optimize in model training. Get available options with `list_scorers()` function. cv: Determines the cross-validation splitting strategy. Possible inputs for cv are: None, to use the default 5-fold cross validation, integer, to specify the number of folds in a (Stratified)KFold, CV splitter, An iterable yielding (train, test) splits as arrays of indices. fit_params: Parameters passed to the fit method of the estimator. \"\"\" self . param_grid = param_grid self . optimizer = optimizer self . scorer = scorer self . cv = cv ( n_splits = self . n_splits ) estimator = ModelEstimators . RandomForestRegressor () self . _run_grid_search ( estimator )","title":"RandomForestRegressor()"},{"location":"module/model/#giants.model.Tuner.SVC","text":"Run hyperparameter tuning for a Support Vector Machine classifier (SVC) model. Parameters: Name Type Description Default param_grid Union[Dict, sklearn.model_selection._search.ParameterGrid] Hyperparameter values to explore in grid searching. {'C': (0.001, 0.01, 0.1, 1.0, 10.0), 'kernel': ('rbf', 'linear'), 'gamma': (0.001, 0.0001, 1e-05, 1e-06, 1e-07), 'class_weight': (None, 'balanced')} optimizer BaseSearchCV Method for evaluating hyperparameter combinations. From the sklearn group of hyperparameter optimizers. <class 'sklearn.model_selection._search.GridSearchCV'> scorer str Performance metric to optimize in model training. Get available options with list_scorers() function. 'neg_log_loss' cv Union[int, sklearn.model_selection._split.BaseCrossValidator, Iterable] Determines the cross-validation splitting strategy. Possible inputs for cv are: None, to use the default 5-fold cross validation, integer, to specify the number of folds in a (Stratified)KFold, CV splitter, An iterable yielding (train, test) splits as arrays of indices. <class 'sklearn.model_selection._split.StratifiedKFold'> fit_params dict Parameters passed to the `fit()`` method of the estimator. None Source code in giants/model.py def SVC ( self , param_grid : Types . ParameterGrid = ParamGrids . SVC , optimizer : Types . BaseSearchCV = ModelDefaults . Optimizer , scorer : str = ModelDefaults . ClassificationScorer , cv : Types . BaseCrossValidator = ModelDefaults . CVClassification , fit_params : dict = None , ) -> None : \"\"\"Run hyperparameter tuning for a Support Vector Machine classifier (SVC) model. Args: param_grid: Hyperparameter values to explore in grid searching. optimizer: Method for evaluating hyperparameter combinations. From the `sklearn` group of hyperparameter optimizers. scorer: Performance metric to optimize in model training. Get available options with `list_scorers()` function. cv: Determines the cross-validation splitting strategy. Possible inputs for cv are: None, to use the default 5-fold cross validation, integer, to specify the number of folds in a (Stratified)KFold, CV splitter, An iterable yielding (train, test) splits as arrays of indices. fit_params: Parameters passed to the `fit()`` method of the estimator. \"\"\" self . param_grid = param_grid self . optimizer = optimizer self . scorer = scorer self . cv = cv ( n_splits = self . n_splits ) estimator = ModelEstimators . SVC () self . _run_grid_search ( estimator )","title":"SVC()"},{"location":"module/model/#giants.model.Tuner.SVR","text":"Run hyperparameter tuning for a Support Vector Machine Regression (SVR) model. Parameters: Name Type Description Default param_grid Union[Dict, sklearn.model_selection._search.ParameterGrid] Hyperparameter values to explore in grid searching. {'C': (0.01, 0.1, 1.0, 10.0), 'epsilon': (0.01, 0.1, 1), 'kernel': ('rbf', 'linear', 'poly', 'sigmoid'), 'gamma': (0.01, 0.001, 0.0001)} optimizer BaseSearchCV Method for evaluating hyperparameter combinations. From the sklearn group of hyperparameter optimizers. <class 'sklearn.model_selection._search.GridSearchCV'> scorer str Performance metric to optimize in model training. Get available options with list_scorers() function. 'neg_root_mean_squared_error' cv Union[int, sklearn.model_selection._split.BaseCrossValidator, Iterable] Determines the cross-validation splitting strategy. Possible inputs for cv are: None, to use the default 5-fold cross validation, integer, to specify the number of folds in a (Stratified)KFold, CV splitter, An iterable yielding (train, test) splits as arrays of indices. <class 'sklearn.model_selection._split.KFold'> fit_params dict Parameters passed to the fit method of the estimator. None Source code in giants/model.py def SVR ( self , param_grid : Types . ParameterGrid = ParamGrids . SVR , optimizer : Types . BaseSearchCV = ModelDefaults . Optimizer , scorer : str = ModelDefaults . RegressionScorer , cv : Types . BaseCrossValidator = ModelDefaults . CVRegression , fit_params : dict = None , ) -> None : \"\"\"Run hyperparameter tuning for a Support Vector Machine Regression (SVR) model. Args: param_grid: Hyperparameter values to explore in grid searching. optimizer: Method for evaluating hyperparameter combinations. From the `sklearn` group of hyperparameter optimizers. scorer: Performance metric to optimize in model training. Get available options with `list_scorers()` function. cv: Determines the cross-validation splitting strategy. Possible inputs for cv are: None, to use the default 5-fold cross validation, integer, to specify the number of folds in a (Stratified)KFold, CV splitter, An iterable yielding (train, test) splits as arrays of indices. fit_params: Parameters passed to the fit method of the estimator. \"\"\" self . param_grid = param_grid self . optimizer = optimizer self . scorer = scorer self . cv = cv ( n_splits = self . n_splits ) estimator = ModelEstimators . SVR () self . _run_grid_search ( estimator )","title":"SVR()"},{"location":"module/model/#giants.model.Tuner.__init__","text":"Performs hyperparameter searches to optimize model performance using sklearn . Parameters: Name Type Description Default x {array-like, sparse matrix} of shape (n_samples, n_features) The input samples. sklearn will convert this to a dtype=np.float32 array or to a sparse csr_matrix . required y ndarray array-like of shape (n_samples,) Target values (strings or integers in classification, real numbers in regression) For classification, labels correspond to classes. required n_splits int Number of train/test splits to evaluate in cross-validation. 4 n_jobs int Number of simultaneous grid search processes to run. 2 refit bool Whether to fit a new model each time a model class is instantiated. True verbose int Level of model training reporting logged. 1 error_score Union[str, numbers.Number] Value to assign to the score if an error occurs in estimator fitting. If set to \u2018raise\u2019, the error is raised. If a numeric value is given, FitFailedWarning is raised. 0 return_train_score bool Get insights on how different parameter settings impact overfitting/underfitting. But computing scores can be expensive and is not required to select parameters that yield the best generalization performance. False Returns: Type Description object a Tuner object that can perform hyperparameter searches and optimize model performance. Source code in giants/model.py def __init__ ( self , x : Types . Array , y : Types . Array , n_splits : int = ModelDefaults . NumberOfSplits , n_jobs : int = ModelDefaults . NumberOfJobs , refit : bool = ModelDefaults . RefitModels , verbose : int = ModelDefaults . Verbosity , error_score : Types . ErrorScore = ModelDefaults . ErrorScore , return_train_score : bool = ModelDefaults . ReturnTrainScore , ) -> object : \"\"\"Performs hyperparameter searches to optimize model performance using `sklearn`. Args: x : {array-like, sparse matrix} of shape (n_samples, n_features) The input samples. `sklearn` will convert this to a `dtype=np.float32` array or to a sparse `csr_matrix`. y: array-like of shape (n_samples,) Target values (strings or integers in classification, real numbers in regression) For classification, labels correspond to classes. n_splits: Number of train/test splits to evaluate in cross-validation. n_jobs: Number of simultaneous grid search processes to run. refit: Whether to fit a new model each time a model class is instantiated. verbose: Level of model training reporting logged. error_score: Value to assign to the score if an error occurs in estimator fitting. If set to \u2018raise\u2019, the error is raised. If a numeric value is given, FitFailedWarning is raised. return_train_score: Get insights on how different parameter settings impact overfitting/underfitting. But computing scores can be expensive and is not required to select parameters that yield the best generalization performance. Returns: a Tuner object that can perform hyperparameter searches and optimize model performance. \"\"\" self . x = x self . y = y self . n_splits = n_splits self . n_jobs = n_jobs self . refit = refit self . verbose = verbose self . error_score = error_score self . return_train_score = return_train_score","title":"__init__()"},{"location":"module/model/#giants.model.list_scorers","text":"Returns a list of viable scorers for grid search optimization. Parameters: Name Type Description Default method str Modeling method. Options: [\"classification\", \"regression\", \"clustering\"]. If None , returns a dictionary with all available scorers. These methods are maintained manually and may become out of date as sklearn implements changes. Running list_scorers() without a method should provide the full list of sklearn scorers via their api. None Returns: Type Description Union[List, Dict] viable model performance scorer metrics. Source code in giants/model.py def list_scorers ( method : str = None ) -> Types . ScorerList : \"\"\"Returns a list of viable scorers for grid search optimization. Args: method: Modeling method. Options: [\"classification\", \"regression\", \"clustering\"]. If `None`, returns a dictionary with all available scorers. These methods are maintained manually and may become out of date as sklearn implements changes. Running `list_scorers()` without a method should provide the full list of `sklearn` scorers via their api. Returns: viable model performance scorer metrics. \"\"\" all_scorers = _scorer . SCORERS classification_scorers = [ \"accuracy\" , \"average_precision\" , \"balanced_accuracy\" , \"f1\" , \"f1_micro\" , \"f1_macro\" , \"f1_weighted\" , \"f1_samples\" , \"jaccard\" , \"neg_log_loss\" , \"neg_brier_score\" , \"precision\" , \"recall\" , \"roc_auc\" , \"roc_auc_ovo\" , \"roc_auc_ovr\" , \"roc_auc_ovo_weighted\" , \"roc_auc_ovr_weighted\" , \"top_k_accuracy\" , ] regression_scorers = [ \"explained_variance\" , \"max_error\" , \"neg_mean_absolute_error\" , \"neg_mean_poisson_deviance\" , \"neg_mean_gamma_deviance\" , \"neg_mean_absolute_percentage_error\" , \"neg_mean_squared_error\" , \"neg_mean_squared_log_error\" , \"neg_median_absolute_error\" , \"neg_root_mean_squared_error\" , \"r2\" , ] clustering_scorers = [ \"adjusted_mutual_info_score\" , \"adjusted_rand_score\" , \"completeness_score\" , \"fowlkes_mallows_score\" , \"homogeneity_score\" , \"mutual_info_score\" , \"normalized_mutual_info_score\" , \"rand_score\" , \"v_measure_score\" , ] if method is None : return all_scorers else : if method . lower () == \"classification\" : return classification_scorers elif method . lower () == \"regression\" : return regression_scorers elif method . lower () == \"clustering\" : return clustering_scorers else : return all_scorers","title":"list_scorers()"}]}