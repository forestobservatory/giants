
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Basic machine learning optimization support, developed to identify big trees.">
      
      
      
        <link rel="canonical" href="https://the.forestobservatory.com/giants/module/config/">
      
      <link rel="icon" href="../../img/favicon.png">
      <meta name="generator" content="mkdocs-1.2.3, mkdocs-material-8.1.3">
    
    
      
        <title>giants.config - giants</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.edf004c2.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.e6a45f82.min.css">
        
      
    
    
    
      
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Poppins:300,400,400i,700%7CSource+Code+Pro&display=fallback">
        <style>:root{--md-text-font:"Poppins";--md-code-font:"Source Code Pro"}</style>
      
    
    
      <link rel="stylesheet" href="../../assets/_mkdocstrings.css">
    
      <link rel="stylesheet" href="https://the.forestobservatory.com/assets/shared/colors.css">
    
    <script>__md_scope=new URL("../..",location),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="forestobs-light" data-md-color-primary="" data-md-color-accent="">
  
    
    
      <script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#giants.config" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="giants" class="md-header__button md-logo" aria-label="giants" data-md-component="logo">
      
  <img src="../../img/giants.svg" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            giants
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              giants.config
            
          </span>
        </div>
      </div>
    </div>
    
      <form class="md-header__option" data-md-component="palette">
        
          
          
          <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="forestobs-light" data-md-color-primary="" data-md-color-accent=""  aria-label="Go dark"  type="radio" name="__palette" id="__palette_1">
          
            <label class="md-header__button md-icon" title="Go dark" for="__palette_2" hidden>
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3 3.19.09m3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95 2.06.05m-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31z"/></svg>
            </label>
          
        
          
          
          <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="" data-md-color-accent=""  aria-label="Go light"  type="radio" name="__palette" id="__palette_2">
          
            <label class="md-header__button md-icon" title="Go light" for="__palette_1" hidden>
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5c-.84 0-1.65.15-2.39.42L12 2M3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29L3.34 7m.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14L3.36 17M20.65 7l-1.77 3.79a7.023 7.023 0 0 0-2.38-4.15l4.15.36m-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29L20.64 17M12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44L12 22z"/></svg>
            </label>
          
        
      </form>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/forestobservatory/giants/" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
  </div>
  <div class="md-source__repository">
    forestobservatory/giants
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="giants" class="md-nav__button md-logo" aria-label="giants" data-md-component="logo">
      
  <img src="../../img/giants.svg" alt="logo">

    </a>
    giants
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/forestobservatory/giants/" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
  </div>
  <div class="md-source__repository">
    forestobservatory/giants
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        Home
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../user-guide/" class="md-nav__link">
        User Guide
      </a>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3" type="checkbox" id="__nav_3" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_3">
          Code Documenation
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Code Documenation" data-md-level="1">
        <label class="md-nav__title" for="__nav_3">
          <span class="md-nav__icon md-icon"></span>
          Code Documenation
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../model/" class="md-nav__link">
        giants.model
      </a>
    </li>
  

            
          
            
              
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          giants.config
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        giants.config
      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#giants.config" class="md-nav__link">
    giants.config
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#giants.config.ModelConfig" class="md-nav__link">
    ModelConfig
  </a>
  
    <nav class="md-nav" aria-label="ModelConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelConfig.CVClassification" class="md-nav__link">
    CVClassification
  </a>
  
    <nav class="md-nav" aria-label="CVClassification">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelConfig.CVClassification--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelConfig.CVClassification--examples" class="md-nav__link">
    Examples
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelConfig.CVClassification--notes" class="md-nav__link">
    Notes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelConfig.CVClassification--see-also" class="md-nav__link">
    See Also
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelConfig.CVClassification.split" class="md-nav__link">
    split()
  </a>
  
    <nav class="md-nav" aria-label="split()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelConfig.CVClassification.split--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelConfig.CVClassification.split--yields" class="md-nav__link">
    Yields
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelConfig.CVClassification.split--notes" class="md-nav__link">
    Notes
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelConfig.CVRegression" class="md-nav__link">
    CVRegression
  </a>
  
    <nav class="md-nav" aria-label="CVRegression">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelConfig.CVRegression--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelConfig.CVRegression--examples" class="md-nav__link">
    Examples
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelConfig.CVRegression--notes" class="md-nav__link">
    Notes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelConfig.CVRegression--see-also" class="md-nav__link">
    See Also
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelConfig.Optimizer" class="md-nav__link">
    Optimizer
  </a>
  
    <nav class="md-nav" aria-label="Optimizer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelConfig.Optimizer--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelConfig.Optimizer--attributes" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelConfig.Optimizer--notes" class="md-nav__link">
    Notes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelConfig.Optimizer--see-also" class="md-nav__link">
    See Also
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelConfig.Optimizer--examples" class="md-nav__link">
    Examples
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig" class="md-nav__link">
    ModelEstimatorConfig
  </a>
  
    <nav class="md-nav" aria-label="ModelEstimatorConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.AdaBoostClassifier" class="md-nav__link">
    AdaBoostClassifier
  </a>
  
    <nav class="md-nav" aria-label="AdaBoostClassifier">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.AdaBoostClassifier--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.AdaBoostClassifier--attributes" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.AdaBoostClassifier--see-also" class="md-nav__link">
    See Also
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.AdaBoostClassifier--references" class="md-nav__link">
    References
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.AdaBoostClassifier--examples" class="md-nav__link">
    Examples
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.AdaBoostClassifier.decision_function" class="md-nav__link">
    decision_function()
  </a>
  
    <nav class="md-nav" aria-label="decision_function()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.AdaBoostClassifier.decision_function--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.AdaBoostClassifier.decision_function--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.AdaBoostClassifier.fit" class="md-nav__link">
    fit()
  </a>
  
    <nav class="md-nav" aria-label="fit()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.AdaBoostClassifier.fit--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.AdaBoostClassifier.fit--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.AdaBoostClassifier.predict" class="md-nav__link">
    predict()
  </a>
  
    <nav class="md-nav" aria-label="predict()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.AdaBoostClassifier.predict--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.AdaBoostClassifier.predict--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.AdaBoostClassifier.predict_log_proba" class="md-nav__link">
    predict_log_proba()
  </a>
  
    <nav class="md-nav" aria-label="predict_log_proba()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.AdaBoostClassifier.predict_log_proba--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.AdaBoostClassifier.predict_log_proba--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.AdaBoostClassifier.predict_proba" class="md-nav__link">
    predict_proba()
  </a>
  
    <nav class="md-nav" aria-label="predict_proba()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.AdaBoostClassifier.predict_proba--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.AdaBoostClassifier.predict_proba--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.AdaBoostClassifier.staged_decision_function" class="md-nav__link">
    staged_decision_function()
  </a>
  
    <nav class="md-nav" aria-label="staged_decision_function()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.AdaBoostClassifier.staged_decision_function--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.AdaBoostClassifier.staged_decision_function--yields" class="md-nav__link">
    Yields
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.AdaBoostClassifier.staged_predict" class="md-nav__link">
    staged_predict()
  </a>
  
    <nav class="md-nav" aria-label="staged_predict()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.AdaBoostClassifier.staged_predict--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.AdaBoostClassifier.staged_predict--yields" class="md-nav__link">
    Yields
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.AdaBoostClassifier.staged_predict_proba" class="md-nav__link">
    staged_predict_proba()
  </a>
  
    <nav class="md-nav" aria-label="staged_predict_proba()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.AdaBoostClassifier.staged_predict_proba--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.AdaBoostClassifier.staged_predict_proba--yields" class="md-nav__link">
    Yields
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.AdaBoostRegressor" class="md-nav__link">
    AdaBoostRegressor
  </a>
  
    <nav class="md-nav" aria-label="AdaBoostRegressor">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.AdaBoostRegressor--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.AdaBoostRegressor--attributes" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.AdaBoostRegressor--see-also" class="md-nav__link">
    See Also
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.AdaBoostRegressor--references" class="md-nav__link">
    References
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.AdaBoostRegressor--examples" class="md-nav__link">
    Examples
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.AdaBoostRegressor.fit" class="md-nav__link">
    fit()
  </a>
  
    <nav class="md-nav" aria-label="fit()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.AdaBoostRegressor.fit--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.AdaBoostRegressor.fit--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.AdaBoostRegressor.predict" class="md-nav__link">
    predict()
  </a>
  
    <nav class="md-nav" aria-label="predict()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.AdaBoostRegressor.predict--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.AdaBoostRegressor.predict--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.AdaBoostRegressor.staged_predict" class="md-nav__link">
    staged_predict()
  </a>
  
    <nav class="md-nav" aria-label="staged_predict()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.AdaBoostRegressor.staged_predict--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.AdaBoostRegressor.staged_predict--yields" class="md-nav__link">
    Yields
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.DecisionTreeClassifier" class="md-nav__link">
    DecisionTreeClassifier
  </a>
  
    <nav class="md-nav" aria-label="DecisionTreeClassifier">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.DecisionTreeClassifier--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.DecisionTreeClassifier--attributes" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.DecisionTreeClassifier--see-also" class="md-nav__link">
    See Also
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.DecisionTreeClassifier--notes" class="md-nav__link">
    Notes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.DecisionTreeClassifier--references" class="md-nav__link">
    References
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.DecisionTreeClassifier--examples" class="md-nav__link">
    Examples
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.DecisionTreeClassifier.n_features_" class="md-nav__link">
    n_features_
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.DecisionTreeClassifier.fit" class="md-nav__link">
    fit()
  </a>
  
    <nav class="md-nav" aria-label="fit()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.DecisionTreeClassifier.fit--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.DecisionTreeClassifier.fit--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.DecisionTreeClassifier.predict_log_proba" class="md-nav__link">
    predict_log_proba()
  </a>
  
    <nav class="md-nav" aria-label="predict_log_proba()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.DecisionTreeClassifier.predict_log_proba--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.DecisionTreeClassifier.predict_log_proba--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.DecisionTreeClassifier.predict_proba" class="md-nav__link">
    predict_proba()
  </a>
  
    <nav class="md-nav" aria-label="predict_proba()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.DecisionTreeClassifier.predict_proba--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.DecisionTreeClassifier.predict_proba--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.GradientBoostingClassifier" class="md-nav__link">
    GradientBoostingClassifier
  </a>
  
    <nav class="md-nav" aria-label="GradientBoostingClassifier">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.GradientBoostingClassifier--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.GradientBoostingClassifier--attributes" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.GradientBoostingClassifier--see-also" class="md-nav__link">
    See Also
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.GradientBoostingClassifier--notes" class="md-nav__link">
    Notes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.GradientBoostingClassifier--references" class="md-nav__link">
    References
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.GradientBoostingClassifier--examples" class="md-nav__link">
    Examples
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.GradientBoostingClassifier.decision_function" class="md-nav__link">
    decision_function()
  </a>
  
    <nav class="md-nav" aria-label="decision_function()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.GradientBoostingClassifier.decision_function--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.GradientBoostingClassifier.decision_function--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.GradientBoostingClassifier.predict" class="md-nav__link">
    predict()
  </a>
  
    <nav class="md-nav" aria-label="predict()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.GradientBoostingClassifier.predict--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.GradientBoostingClassifier.predict--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.GradientBoostingClassifier.predict_log_proba" class="md-nav__link">
    predict_log_proba()
  </a>
  
    <nav class="md-nav" aria-label="predict_log_proba()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.GradientBoostingClassifier.predict_log_proba--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.GradientBoostingClassifier.predict_log_proba--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.GradientBoostingClassifier.predict_log_proba--raises" class="md-nav__link">
    Raises
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.GradientBoostingClassifier.predict_proba" class="md-nav__link">
    predict_proba()
  </a>
  
    <nav class="md-nav" aria-label="predict_proba()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.GradientBoostingClassifier.predict_proba--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.GradientBoostingClassifier.predict_proba--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.GradientBoostingClassifier.predict_proba--raises" class="md-nav__link">
    Raises
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.GradientBoostingClassifier.staged_decision_function" class="md-nav__link">
    staged_decision_function()
  </a>
  
    <nav class="md-nav" aria-label="staged_decision_function()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.GradientBoostingClassifier.staged_decision_function--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.GradientBoostingClassifier.staged_decision_function--yields" class="md-nav__link">
    Yields
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.GradientBoostingClassifier.staged_predict" class="md-nav__link">
    staged_predict()
  </a>
  
    <nav class="md-nav" aria-label="staged_predict()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.GradientBoostingClassifier.staged_predict--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.GradientBoostingClassifier.staged_predict--yields" class="md-nav__link">
    Yields
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.GradientBoostingClassifier.staged_predict_proba" class="md-nav__link">
    staged_predict_proba()
  </a>
  
    <nav class="md-nav" aria-label="staged_predict_proba()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.GradientBoostingClassifier.staged_predict_proba--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.GradientBoostingClassifier.staged_predict_proba--yields" class="md-nav__link">
    Yields
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.GradientBoostingRegressor" class="md-nav__link">
    GradientBoostingRegressor
  </a>
  
    <nav class="md-nav" aria-label="GradientBoostingRegressor">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.GradientBoostingRegressor--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.GradientBoostingRegressor--attributes" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.GradientBoostingRegressor--see-also" class="md-nav__link">
    See Also
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.GradientBoostingRegressor--notes" class="md-nav__link">
    Notes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.GradientBoostingRegressor--references" class="md-nav__link">
    References
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.GradientBoostingRegressor--examples" class="md-nav__link">
    Examples
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.GradientBoostingRegressor.n_classes_" class="md-nav__link">
    n_classes_
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.GradientBoostingRegressor.apply" class="md-nav__link">
    apply()
  </a>
  
    <nav class="md-nav" aria-label="apply()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.GradientBoostingRegressor.apply--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.GradientBoostingRegressor.apply--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.GradientBoostingRegressor.predict" class="md-nav__link">
    predict()
  </a>
  
    <nav class="md-nav" aria-label="predict()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.GradientBoostingRegressor.predict--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.GradientBoostingRegressor.predict--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.GradientBoostingRegressor.staged_predict" class="md-nav__link">
    staged_predict()
  </a>
  
    <nav class="md-nav" aria-label="staged_predict()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.GradientBoostingRegressor.staged_predict--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.GradientBoostingRegressor.staged_predict--yields" class="md-nav__link">
    Yields
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.LinearRegression" class="md-nav__link">
    LinearRegression
  </a>
  
    <nav class="md-nav" aria-label="LinearRegression">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.LinearRegression--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.LinearRegression--attributes" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.LinearRegression--see-also" class="md-nav__link">
    See Also
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.LinearRegression--notes" class="md-nav__link">
    Notes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.LinearRegression--examples" class="md-nav__link">
    Examples
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.LinearRegression--y-1-x_0-2-x_1-3" class="md-nav__link">
    y = 1 * x_0 + 2 * x_1 + 3
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.LinearRegression.fit" class="md-nav__link">
    fit()
  </a>
  
    <nav class="md-nav" aria-label="fit()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.LinearRegression.fit--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.LinearRegression.fit--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.LinearSVC" class="md-nav__link">
    LinearSVC
  </a>
  
    <nav class="md-nav" aria-label="LinearSVC">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.LinearSVC--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.LinearSVC--attributes" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.LinearSVC--see-also" class="md-nav__link">
    See Also
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.LinearSVC--notes" class="md-nav__link">
    Notes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.LinearSVC--references" class="md-nav__link">
    References
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.LinearSVC--examples" class="md-nav__link">
    Examples
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.LinearSVC.fit" class="md-nav__link">
    fit()
  </a>
  
    <nav class="md-nav" aria-label="fit()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.LinearSVC.fit--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.LinearSVC.fit--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.LinearSVR" class="md-nav__link">
    LinearSVR
  </a>
  
    <nav class="md-nav" aria-label="LinearSVR">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.LinearSVR--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.LinearSVR--attributes" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.LinearSVR--see-also" class="md-nav__link">
    See Also
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.LinearSVR--examples" class="md-nav__link">
    Examples
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.LinearSVR.fit" class="md-nav__link">
    fit()
  </a>
  
    <nav class="md-nav" aria-label="fit()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.LinearSVR.fit--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.LinearSVR.fit--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.LogisticRegression" class="md-nav__link">
    LogisticRegression
  </a>
  
    <nav class="md-nav" aria-label="LogisticRegression">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.LogisticRegression--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.LogisticRegression--attributes" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.LogisticRegression--see-also" class="md-nav__link">
    See Also
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.LogisticRegression--notes" class="md-nav__link">
    Notes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.LogisticRegression--references" class="md-nav__link">
    References
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.LogisticRegression--examples" class="md-nav__link">
    Examples
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.LogisticRegression.fit" class="md-nav__link">
    fit()
  </a>
  
    <nav class="md-nav" aria-label="fit()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.LogisticRegression.fit--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.LogisticRegression.fit--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.LogisticRegression.fit--notes" class="md-nav__link">
    Notes
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.LogisticRegression.predict_log_proba" class="md-nav__link">
    predict_log_proba()
  </a>
  
    <nav class="md-nav" aria-label="predict_log_proba()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.LogisticRegression.predict_log_proba--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.LogisticRegression.predict_log_proba--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.LogisticRegression.predict_proba" class="md-nav__link">
    predict_proba()
  </a>
  
    <nav class="md-nav" aria-label="predict_proba()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.LogisticRegression.predict_proba--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.LogisticRegression.predict_proba--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.RandomForestClassifier" class="md-nav__link">
    RandomForestClassifier
  </a>
  
    <nav class="md-nav" aria-label="RandomForestClassifier">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.RandomForestClassifier--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.RandomForestClassifier--attributes" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.RandomForestClassifier--see-also" class="md-nav__link">
    See Also
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.RandomForestClassifier--notes" class="md-nav__link">
    Notes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.RandomForestClassifier--references" class="md-nav__link">
    References
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.RandomForestClassifier--examples" class="md-nav__link">
    Examples
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.RandomForestRegressor" class="md-nav__link">
    RandomForestRegressor
  </a>
  
    <nav class="md-nav" aria-label="RandomForestRegressor">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.RandomForestRegressor--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.RandomForestRegressor--attributes" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.RandomForestRegressor--see-also" class="md-nav__link">
    See Also
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.RandomForestRegressor--notes" class="md-nav__link">
    Notes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.RandomForestRegressor--references" class="md-nav__link">
    References
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.RandomForestRegressor--examples" class="md-nav__link">
    Examples
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.SVC" class="md-nav__link">
    SVC
  </a>
  
    <nav class="md-nav" aria-label="SVC">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.SVC--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.SVC--attributes" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.SVC--see-also" class="md-nav__link">
    See Also
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.SVC--references" class="md-nav__link">
    References
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.SVC--examples" class="md-nav__link">
    Examples
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.SVR" class="md-nav__link">
    SVR
  </a>
  
    <nav class="md-nav" aria-label="SVR">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.SVR--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.SVR--attributes" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.SVR--see-also" class="md-nav__link">
    See Also
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.SVR--references" class="md-nav__link">
    References
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.SVR--examples" class="md-nav__link">
    Examples
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#giants.config.ParamGridConfig" class="md-nav__link">
    ParamGridConfig
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#giants.config.TypeConfig" class="md-nav__link">
    TypeConfig
  </a>
  
    <nav class="md-nav" aria-label="TypeConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.TypeConfig.Array" class="md-nav__link">
    Array
  </a>
  
    <nav class="md-nav" aria-label="Array">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.TypeConfig.Array--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.TypeConfig.Array--attributes" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.TypeConfig.Array--see-also" class="md-nav__link">
    See Also
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.TypeConfig.Array--notes" class="md-nav__link">
    Notes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.TypeConfig.Array--examples" class="md-nav__link">
    Examples
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.TypeConfig.BaseSearchCV" class="md-nav__link">
    BaseSearchCV
  </a>
  
    <nav class="md-nav" aria-label="BaseSearchCV">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.TypeConfig.BaseSearchCV.classes_" class="md-nav__link">
    classes_
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.TypeConfig.BaseSearchCV.n_features_in_" class="md-nav__link">
    n_features_in_
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.TypeConfig.BaseSearchCV.decision_function" class="md-nav__link">
    decision_function()
  </a>
  
    <nav class="md-nav" aria-label="decision_function()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.TypeConfig.BaseSearchCV.decision_function--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.TypeConfig.BaseSearchCV.decision_function--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.TypeConfig.BaseSearchCV.fit" class="md-nav__link">
    fit()
  </a>
  
    <nav class="md-nav" aria-label="fit()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.TypeConfig.BaseSearchCV.fit--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.TypeConfig.BaseSearchCV.fit--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.TypeConfig.BaseSearchCV.inverse_transform" class="md-nav__link">
    inverse_transform()
  </a>
  
    <nav class="md-nav" aria-label="inverse_transform()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.TypeConfig.BaseSearchCV.inverse_transform--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.TypeConfig.BaseSearchCV.inverse_transform--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.TypeConfig.BaseSearchCV.predict" class="md-nav__link">
    predict()
  </a>
  
    <nav class="md-nav" aria-label="predict()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.TypeConfig.BaseSearchCV.predict--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.TypeConfig.BaseSearchCV.predict--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.TypeConfig.BaseSearchCV.predict_log_proba" class="md-nav__link">
    predict_log_proba()
  </a>
  
    <nav class="md-nav" aria-label="predict_log_proba()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.TypeConfig.BaseSearchCV.predict_log_proba--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.TypeConfig.BaseSearchCV.predict_log_proba--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.TypeConfig.BaseSearchCV.predict_proba" class="md-nav__link">
    predict_proba()
  </a>
  
    <nav class="md-nav" aria-label="predict_proba()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.TypeConfig.BaseSearchCV.predict_proba--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.TypeConfig.BaseSearchCV.predict_proba--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.TypeConfig.BaseSearchCV.score" class="md-nav__link">
    score()
  </a>
  
    <nav class="md-nav" aria-label="score()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.TypeConfig.BaseSearchCV.score--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.TypeConfig.BaseSearchCV.score--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.TypeConfig.BaseSearchCV.score_samples" class="md-nav__link">
    score_samples()
  </a>
  
    <nav class="md-nav" aria-label="score_samples()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.TypeConfig.BaseSearchCV.score_samples--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.TypeConfig.BaseSearchCV.score_samples--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.TypeConfig.BaseSearchCV.transform" class="md-nav__link">
    transform()
  </a>
  
    <nav class="md-nav" aria-label="transform()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.TypeConfig.BaseSearchCV.transform--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.TypeConfig.BaseSearchCV.transform--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.TypeConfig.Number" class="md-nav__link">
    Number
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#giants.config" class="md-nav__link">
    giants.config
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#giants.config.ModelConfig" class="md-nav__link">
    ModelConfig
  </a>
  
    <nav class="md-nav" aria-label="ModelConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelConfig.CVClassification" class="md-nav__link">
    CVClassification
  </a>
  
    <nav class="md-nav" aria-label="CVClassification">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelConfig.CVClassification--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelConfig.CVClassification--examples" class="md-nav__link">
    Examples
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelConfig.CVClassification--notes" class="md-nav__link">
    Notes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelConfig.CVClassification--see-also" class="md-nav__link">
    See Also
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelConfig.CVClassification.split" class="md-nav__link">
    split()
  </a>
  
    <nav class="md-nav" aria-label="split()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelConfig.CVClassification.split--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelConfig.CVClassification.split--yields" class="md-nav__link">
    Yields
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelConfig.CVClassification.split--notes" class="md-nav__link">
    Notes
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelConfig.CVRegression" class="md-nav__link">
    CVRegression
  </a>
  
    <nav class="md-nav" aria-label="CVRegression">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelConfig.CVRegression--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelConfig.CVRegression--examples" class="md-nav__link">
    Examples
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelConfig.CVRegression--notes" class="md-nav__link">
    Notes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelConfig.CVRegression--see-also" class="md-nav__link">
    See Also
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelConfig.Optimizer" class="md-nav__link">
    Optimizer
  </a>
  
    <nav class="md-nav" aria-label="Optimizer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelConfig.Optimizer--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelConfig.Optimizer--attributes" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelConfig.Optimizer--notes" class="md-nav__link">
    Notes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelConfig.Optimizer--see-also" class="md-nav__link">
    See Also
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelConfig.Optimizer--examples" class="md-nav__link">
    Examples
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig" class="md-nav__link">
    ModelEstimatorConfig
  </a>
  
    <nav class="md-nav" aria-label="ModelEstimatorConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.AdaBoostClassifier" class="md-nav__link">
    AdaBoostClassifier
  </a>
  
    <nav class="md-nav" aria-label="AdaBoostClassifier">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.AdaBoostClassifier--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.AdaBoostClassifier--attributes" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.AdaBoostClassifier--see-also" class="md-nav__link">
    See Also
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.AdaBoostClassifier--references" class="md-nav__link">
    References
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.AdaBoostClassifier--examples" class="md-nav__link">
    Examples
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.AdaBoostClassifier.decision_function" class="md-nav__link">
    decision_function()
  </a>
  
    <nav class="md-nav" aria-label="decision_function()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.AdaBoostClassifier.decision_function--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.AdaBoostClassifier.decision_function--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.AdaBoostClassifier.fit" class="md-nav__link">
    fit()
  </a>
  
    <nav class="md-nav" aria-label="fit()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.AdaBoostClassifier.fit--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.AdaBoostClassifier.fit--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.AdaBoostClassifier.predict" class="md-nav__link">
    predict()
  </a>
  
    <nav class="md-nav" aria-label="predict()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.AdaBoostClassifier.predict--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.AdaBoostClassifier.predict--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.AdaBoostClassifier.predict_log_proba" class="md-nav__link">
    predict_log_proba()
  </a>
  
    <nav class="md-nav" aria-label="predict_log_proba()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.AdaBoostClassifier.predict_log_proba--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.AdaBoostClassifier.predict_log_proba--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.AdaBoostClassifier.predict_proba" class="md-nav__link">
    predict_proba()
  </a>
  
    <nav class="md-nav" aria-label="predict_proba()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.AdaBoostClassifier.predict_proba--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.AdaBoostClassifier.predict_proba--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.AdaBoostClassifier.staged_decision_function" class="md-nav__link">
    staged_decision_function()
  </a>
  
    <nav class="md-nav" aria-label="staged_decision_function()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.AdaBoostClassifier.staged_decision_function--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.AdaBoostClassifier.staged_decision_function--yields" class="md-nav__link">
    Yields
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.AdaBoostClassifier.staged_predict" class="md-nav__link">
    staged_predict()
  </a>
  
    <nav class="md-nav" aria-label="staged_predict()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.AdaBoostClassifier.staged_predict--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.AdaBoostClassifier.staged_predict--yields" class="md-nav__link">
    Yields
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.AdaBoostClassifier.staged_predict_proba" class="md-nav__link">
    staged_predict_proba()
  </a>
  
    <nav class="md-nav" aria-label="staged_predict_proba()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.AdaBoostClassifier.staged_predict_proba--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.AdaBoostClassifier.staged_predict_proba--yields" class="md-nav__link">
    Yields
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.AdaBoostRegressor" class="md-nav__link">
    AdaBoostRegressor
  </a>
  
    <nav class="md-nav" aria-label="AdaBoostRegressor">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.AdaBoostRegressor--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.AdaBoostRegressor--attributes" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.AdaBoostRegressor--see-also" class="md-nav__link">
    See Also
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.AdaBoostRegressor--references" class="md-nav__link">
    References
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.AdaBoostRegressor--examples" class="md-nav__link">
    Examples
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.AdaBoostRegressor.fit" class="md-nav__link">
    fit()
  </a>
  
    <nav class="md-nav" aria-label="fit()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.AdaBoostRegressor.fit--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.AdaBoostRegressor.fit--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.AdaBoostRegressor.predict" class="md-nav__link">
    predict()
  </a>
  
    <nav class="md-nav" aria-label="predict()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.AdaBoostRegressor.predict--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.AdaBoostRegressor.predict--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.AdaBoostRegressor.staged_predict" class="md-nav__link">
    staged_predict()
  </a>
  
    <nav class="md-nav" aria-label="staged_predict()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.AdaBoostRegressor.staged_predict--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.AdaBoostRegressor.staged_predict--yields" class="md-nav__link">
    Yields
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.DecisionTreeClassifier" class="md-nav__link">
    DecisionTreeClassifier
  </a>
  
    <nav class="md-nav" aria-label="DecisionTreeClassifier">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.DecisionTreeClassifier--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.DecisionTreeClassifier--attributes" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.DecisionTreeClassifier--see-also" class="md-nav__link">
    See Also
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.DecisionTreeClassifier--notes" class="md-nav__link">
    Notes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.DecisionTreeClassifier--references" class="md-nav__link">
    References
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.DecisionTreeClassifier--examples" class="md-nav__link">
    Examples
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.DecisionTreeClassifier.n_features_" class="md-nav__link">
    n_features_
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.DecisionTreeClassifier.fit" class="md-nav__link">
    fit()
  </a>
  
    <nav class="md-nav" aria-label="fit()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.DecisionTreeClassifier.fit--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.DecisionTreeClassifier.fit--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.DecisionTreeClassifier.predict_log_proba" class="md-nav__link">
    predict_log_proba()
  </a>
  
    <nav class="md-nav" aria-label="predict_log_proba()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.DecisionTreeClassifier.predict_log_proba--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.DecisionTreeClassifier.predict_log_proba--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.DecisionTreeClassifier.predict_proba" class="md-nav__link">
    predict_proba()
  </a>
  
    <nav class="md-nav" aria-label="predict_proba()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.DecisionTreeClassifier.predict_proba--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.DecisionTreeClassifier.predict_proba--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.GradientBoostingClassifier" class="md-nav__link">
    GradientBoostingClassifier
  </a>
  
    <nav class="md-nav" aria-label="GradientBoostingClassifier">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.GradientBoostingClassifier--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.GradientBoostingClassifier--attributes" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.GradientBoostingClassifier--see-also" class="md-nav__link">
    See Also
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.GradientBoostingClassifier--notes" class="md-nav__link">
    Notes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.GradientBoostingClassifier--references" class="md-nav__link">
    References
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.GradientBoostingClassifier--examples" class="md-nav__link">
    Examples
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.GradientBoostingClassifier.decision_function" class="md-nav__link">
    decision_function()
  </a>
  
    <nav class="md-nav" aria-label="decision_function()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.GradientBoostingClassifier.decision_function--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.GradientBoostingClassifier.decision_function--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.GradientBoostingClassifier.predict" class="md-nav__link">
    predict()
  </a>
  
    <nav class="md-nav" aria-label="predict()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.GradientBoostingClassifier.predict--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.GradientBoostingClassifier.predict--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.GradientBoostingClassifier.predict_log_proba" class="md-nav__link">
    predict_log_proba()
  </a>
  
    <nav class="md-nav" aria-label="predict_log_proba()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.GradientBoostingClassifier.predict_log_proba--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.GradientBoostingClassifier.predict_log_proba--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.GradientBoostingClassifier.predict_log_proba--raises" class="md-nav__link">
    Raises
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.GradientBoostingClassifier.predict_proba" class="md-nav__link">
    predict_proba()
  </a>
  
    <nav class="md-nav" aria-label="predict_proba()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.GradientBoostingClassifier.predict_proba--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.GradientBoostingClassifier.predict_proba--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.GradientBoostingClassifier.predict_proba--raises" class="md-nav__link">
    Raises
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.GradientBoostingClassifier.staged_decision_function" class="md-nav__link">
    staged_decision_function()
  </a>
  
    <nav class="md-nav" aria-label="staged_decision_function()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.GradientBoostingClassifier.staged_decision_function--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.GradientBoostingClassifier.staged_decision_function--yields" class="md-nav__link">
    Yields
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.GradientBoostingClassifier.staged_predict" class="md-nav__link">
    staged_predict()
  </a>
  
    <nav class="md-nav" aria-label="staged_predict()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.GradientBoostingClassifier.staged_predict--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.GradientBoostingClassifier.staged_predict--yields" class="md-nav__link">
    Yields
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.GradientBoostingClassifier.staged_predict_proba" class="md-nav__link">
    staged_predict_proba()
  </a>
  
    <nav class="md-nav" aria-label="staged_predict_proba()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.GradientBoostingClassifier.staged_predict_proba--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.GradientBoostingClassifier.staged_predict_proba--yields" class="md-nav__link">
    Yields
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.GradientBoostingRegressor" class="md-nav__link">
    GradientBoostingRegressor
  </a>
  
    <nav class="md-nav" aria-label="GradientBoostingRegressor">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.GradientBoostingRegressor--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.GradientBoostingRegressor--attributes" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.GradientBoostingRegressor--see-also" class="md-nav__link">
    See Also
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.GradientBoostingRegressor--notes" class="md-nav__link">
    Notes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.GradientBoostingRegressor--references" class="md-nav__link">
    References
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.GradientBoostingRegressor--examples" class="md-nav__link">
    Examples
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.GradientBoostingRegressor.n_classes_" class="md-nav__link">
    n_classes_
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.GradientBoostingRegressor.apply" class="md-nav__link">
    apply()
  </a>
  
    <nav class="md-nav" aria-label="apply()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.GradientBoostingRegressor.apply--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.GradientBoostingRegressor.apply--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.GradientBoostingRegressor.predict" class="md-nav__link">
    predict()
  </a>
  
    <nav class="md-nav" aria-label="predict()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.GradientBoostingRegressor.predict--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.GradientBoostingRegressor.predict--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.GradientBoostingRegressor.staged_predict" class="md-nav__link">
    staged_predict()
  </a>
  
    <nav class="md-nav" aria-label="staged_predict()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.GradientBoostingRegressor.staged_predict--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.GradientBoostingRegressor.staged_predict--yields" class="md-nav__link">
    Yields
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.LinearRegression" class="md-nav__link">
    LinearRegression
  </a>
  
    <nav class="md-nav" aria-label="LinearRegression">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.LinearRegression--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.LinearRegression--attributes" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.LinearRegression--see-also" class="md-nav__link">
    See Also
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.LinearRegression--notes" class="md-nav__link">
    Notes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.LinearRegression--examples" class="md-nav__link">
    Examples
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.LinearRegression--y-1-x_0-2-x_1-3" class="md-nav__link">
    y = 1 * x_0 + 2 * x_1 + 3
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.LinearRegression.fit" class="md-nav__link">
    fit()
  </a>
  
    <nav class="md-nav" aria-label="fit()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.LinearRegression.fit--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.LinearRegression.fit--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.LinearSVC" class="md-nav__link">
    LinearSVC
  </a>
  
    <nav class="md-nav" aria-label="LinearSVC">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.LinearSVC--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.LinearSVC--attributes" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.LinearSVC--see-also" class="md-nav__link">
    See Also
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.LinearSVC--notes" class="md-nav__link">
    Notes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.LinearSVC--references" class="md-nav__link">
    References
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.LinearSVC--examples" class="md-nav__link">
    Examples
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.LinearSVC.fit" class="md-nav__link">
    fit()
  </a>
  
    <nav class="md-nav" aria-label="fit()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.LinearSVC.fit--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.LinearSVC.fit--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.LinearSVR" class="md-nav__link">
    LinearSVR
  </a>
  
    <nav class="md-nav" aria-label="LinearSVR">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.LinearSVR--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.LinearSVR--attributes" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.LinearSVR--see-also" class="md-nav__link">
    See Also
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.LinearSVR--examples" class="md-nav__link">
    Examples
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.LinearSVR.fit" class="md-nav__link">
    fit()
  </a>
  
    <nav class="md-nav" aria-label="fit()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.LinearSVR.fit--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.LinearSVR.fit--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.LogisticRegression" class="md-nav__link">
    LogisticRegression
  </a>
  
    <nav class="md-nav" aria-label="LogisticRegression">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.LogisticRegression--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.LogisticRegression--attributes" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.LogisticRegression--see-also" class="md-nav__link">
    See Also
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.LogisticRegression--notes" class="md-nav__link">
    Notes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.LogisticRegression--references" class="md-nav__link">
    References
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.LogisticRegression--examples" class="md-nav__link">
    Examples
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.LogisticRegression.fit" class="md-nav__link">
    fit()
  </a>
  
    <nav class="md-nav" aria-label="fit()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.LogisticRegression.fit--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.LogisticRegression.fit--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.LogisticRegression.fit--notes" class="md-nav__link">
    Notes
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.LogisticRegression.predict_log_proba" class="md-nav__link">
    predict_log_proba()
  </a>
  
    <nav class="md-nav" aria-label="predict_log_proba()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.LogisticRegression.predict_log_proba--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.LogisticRegression.predict_log_proba--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.LogisticRegression.predict_proba" class="md-nav__link">
    predict_proba()
  </a>
  
    <nav class="md-nav" aria-label="predict_proba()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.LogisticRegression.predict_proba--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.LogisticRegression.predict_proba--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.RandomForestClassifier" class="md-nav__link">
    RandomForestClassifier
  </a>
  
    <nav class="md-nav" aria-label="RandomForestClassifier">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.RandomForestClassifier--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.RandomForestClassifier--attributes" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.RandomForestClassifier--see-also" class="md-nav__link">
    See Also
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.RandomForestClassifier--notes" class="md-nav__link">
    Notes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.RandomForestClassifier--references" class="md-nav__link">
    References
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.RandomForestClassifier--examples" class="md-nav__link">
    Examples
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.RandomForestRegressor" class="md-nav__link">
    RandomForestRegressor
  </a>
  
    <nav class="md-nav" aria-label="RandomForestRegressor">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.RandomForestRegressor--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.RandomForestRegressor--attributes" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.RandomForestRegressor--see-also" class="md-nav__link">
    See Also
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.RandomForestRegressor--notes" class="md-nav__link">
    Notes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.RandomForestRegressor--references" class="md-nav__link">
    References
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.RandomForestRegressor--examples" class="md-nav__link">
    Examples
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.SVC" class="md-nav__link">
    SVC
  </a>
  
    <nav class="md-nav" aria-label="SVC">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.SVC--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.SVC--attributes" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.SVC--see-also" class="md-nav__link">
    See Also
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.SVC--references" class="md-nav__link">
    References
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.SVC--examples" class="md-nav__link">
    Examples
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.SVR" class="md-nav__link">
    SVR
  </a>
  
    <nav class="md-nav" aria-label="SVR">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.SVR--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.SVR--attributes" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.SVR--see-also" class="md-nav__link">
    See Also
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.SVR--references" class="md-nav__link">
    References
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.ModelEstimatorConfig.SVR--examples" class="md-nav__link">
    Examples
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#giants.config.ParamGridConfig" class="md-nav__link">
    ParamGridConfig
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#giants.config.TypeConfig" class="md-nav__link">
    TypeConfig
  </a>
  
    <nav class="md-nav" aria-label="TypeConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.TypeConfig.Array" class="md-nav__link">
    Array
  </a>
  
    <nav class="md-nav" aria-label="Array">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.TypeConfig.Array--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.TypeConfig.Array--attributes" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.TypeConfig.Array--see-also" class="md-nav__link">
    See Also
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.TypeConfig.Array--notes" class="md-nav__link">
    Notes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.TypeConfig.Array--examples" class="md-nav__link">
    Examples
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.TypeConfig.BaseSearchCV" class="md-nav__link">
    BaseSearchCV
  </a>
  
    <nav class="md-nav" aria-label="BaseSearchCV">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.TypeConfig.BaseSearchCV.classes_" class="md-nav__link">
    classes_
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.TypeConfig.BaseSearchCV.n_features_in_" class="md-nav__link">
    n_features_in_
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.TypeConfig.BaseSearchCV.decision_function" class="md-nav__link">
    decision_function()
  </a>
  
    <nav class="md-nav" aria-label="decision_function()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.TypeConfig.BaseSearchCV.decision_function--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.TypeConfig.BaseSearchCV.decision_function--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.TypeConfig.BaseSearchCV.fit" class="md-nav__link">
    fit()
  </a>
  
    <nav class="md-nav" aria-label="fit()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.TypeConfig.BaseSearchCV.fit--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.TypeConfig.BaseSearchCV.fit--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.TypeConfig.BaseSearchCV.inverse_transform" class="md-nav__link">
    inverse_transform()
  </a>
  
    <nav class="md-nav" aria-label="inverse_transform()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.TypeConfig.BaseSearchCV.inverse_transform--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.TypeConfig.BaseSearchCV.inverse_transform--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.TypeConfig.BaseSearchCV.predict" class="md-nav__link">
    predict()
  </a>
  
    <nav class="md-nav" aria-label="predict()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.TypeConfig.BaseSearchCV.predict--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.TypeConfig.BaseSearchCV.predict--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.TypeConfig.BaseSearchCV.predict_log_proba" class="md-nav__link">
    predict_log_proba()
  </a>
  
    <nav class="md-nav" aria-label="predict_log_proba()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.TypeConfig.BaseSearchCV.predict_log_proba--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.TypeConfig.BaseSearchCV.predict_log_proba--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.TypeConfig.BaseSearchCV.predict_proba" class="md-nav__link">
    predict_proba()
  </a>
  
    <nav class="md-nav" aria-label="predict_proba()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.TypeConfig.BaseSearchCV.predict_proba--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.TypeConfig.BaseSearchCV.predict_proba--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.TypeConfig.BaseSearchCV.score" class="md-nav__link">
    score()
  </a>
  
    <nav class="md-nav" aria-label="score()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.TypeConfig.BaseSearchCV.score--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.TypeConfig.BaseSearchCV.score--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.TypeConfig.BaseSearchCV.score_samples" class="md-nav__link">
    score_samples()
  </a>
  
    <nav class="md-nav" aria-label="score_samples()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.TypeConfig.BaseSearchCV.score_samples--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.TypeConfig.BaseSearchCV.score_samples--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.TypeConfig.BaseSearchCV.transform" class="md-nav__link">
    transform()
  </a>
  
    <nav class="md-nav" aria-label="transform()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#giants.config.TypeConfig.BaseSearchCV.transform--parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.TypeConfig.BaseSearchCV.transform--returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#giants.config.TypeConfig.Number" class="md-nav__link">
    Number
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content" data-md-component="content">
            <article class="md-content__inner md-typeset">
              
                
<a href="https://github.com/forestobservatory/giants/edit/master/docs/module/config.md" title="Edit this page" class="md-content__button md-icon">
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25z"/></svg>
</a>


  <h1>giants.config</h1>

<div class="doc doc-object doc-module">

<a id="giants.config"></a>
    <div class="doc doc-contents first">

      <p>Default configuration options for model hyperparameter searches.</p>



  <div class="doc doc-children">











  <div class="doc doc-object doc-class">



<h2 class="doc doc-heading" id="giants.config.ModelConfig">
        <code>
ModelConfig        </code>



<a class="headerlink" href="#giants.config.ModelConfig" title="Permanent link">&para;</a></h2>

    <div class="doc doc-contents ">

      <p>Stores default model tuning parameters</p>




  <div class="doc doc-children">















  <div class="doc doc-object doc-class">



<h3 class="doc doc-heading" id="giants.config.ModelConfig.CVClassification">
        <code>
CVClassification            (<span title="sklearn.model_selection._split._BaseKFold">_BaseKFold</span>)
        </code>



<a class="headerlink" href="#giants.config.ModelConfig.CVClassification" title="Permanent link">&para;</a></h3>

    <div class="doc doc-contents ">

      <p>Stratified K-Folds cross-validator.</p>
<p>Provides train/test indices to split data in train/test sets.</p>
<p>This cross-validation object is a variation of KFold that returns
stratified folds. The folds are made by preserving the percentage of
samples for each class.</p>
<p>Read more in the :ref:<code>User Guide &lt;stratified_k_fold&gt;</code>.</p>
<h5 id="giants.config.ModelConfig.CVClassification--parameters">Parameters<a class="headerlink" href="#giants.config.ModelConfig.CVClassification--parameters" title="Permanent link">&para;</a></h5>
<p>n_splits : int, default=5
    Number of folds. Must be at least 2.</p>
<div class="codehilite"><pre><span></span><code>.. versionchanged:: 0.22
    ``n_splits`` default value changed from 3 to 5.
</code></pre></div>

<p>shuffle : bool, default=False
    Whether to shuffle each class's samples before splitting into batches.
    Note that the samples within each split will not be shuffled.</p>
<p>random_state : int, RandomState instance or None, default=None
    When <code>shuffle</code> is True, <code>random_state</code> affects the ordering of the
    indices, which controls the randomness of each fold for each class.
    Otherwise, leave <code>random_state</code> as <code>None</code>.
    Pass an int for reproducible output across multiple function calls.
    See :term:<code>Glossary &lt;random_state&gt;</code>.</p>
<h5 id="giants.config.ModelConfig.CVClassification--examples">Examples<a class="headerlink" href="#giants.config.ModelConfig.CVClassification--examples" title="Permanent link">&para;</a></h5>
<blockquote>
<blockquote>
<blockquote>
<p>import numpy as np
from sklearn.model_selection import StratifiedKFold
X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])
y = np.array([0, 0, 1, 1])
skf = StratifiedKFold(n_splits=2)
skf.get_n_splits(X, y)
2
print(skf)
StratifiedKFold(n_splits=2, random_state=None, shuffle=False)
for train_index, test_index in skf.split(X, y):
...     print("TRAIN:", train_index, "TEST:", test_index)
...     X_train, X_test = X[train_index], X[test_index]
...     y_train, y_test = y[train_index], y[test_index]
TRAIN: [1 3] TEST: [0 2]
TRAIN: [0 2] TEST: [1 3]</p>
</blockquote>
</blockquote>
</blockquote>
<h5 id="giants.config.ModelConfig.CVClassification--notes">Notes<a class="headerlink" href="#giants.config.ModelConfig.CVClassification--notes" title="Permanent link">&para;</a></h5>
<p>The implementation is designed to:</p>
<ul>
<li>Generate test sets such that all contain the same distribution of
  classes, or as close as possible.</li>
<li>Be invariant to class label: relabelling <code>y = ["Happy", "Sad"]</code> to
  <code>y = [1, 0]</code> should not change the indices generated.</li>
<li>Preserve order dependencies in the dataset ordering, when
  <code>shuffle=False</code>: all samples from class k in some test set were
  contiguous in y, or separated in y by samples from classes other than k.</li>
<li>Generate test sets where the smallest and largest differ by at most one
  sample.</li>
</ul>
<p>.. versionchanged:: 0.22
    The previous implementation did not follow the last constraint.</p>
<h5 id="giants.config.ModelConfig.CVClassification--see-also">See Also<a class="headerlink" href="#giants.config.ModelConfig.CVClassification--see-also" title="Permanent link">&para;</a></h5>
<p>RepeatedStratifiedKFold : Repeats Stratified K-Fold n times.</p>




  <div class="doc doc-children">










  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="giants.config.ModelConfig.CVClassification.split">
<code class="codehilite language-python"><span class="n">split</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>


<a class="headerlink" href="#giants.config.ModelConfig.CVClassification.split" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Generate indices to split data into training and test set.</p>
<h6 id="giants.config.ModelConfig.CVClassification.split--parameters">Parameters<a class="headerlink" href="#giants.config.ModelConfig.CVClassification.split--parameters" title="Permanent link">&para;</a></h6>
<p>X : array-like of shape (n_samples, n_features)
    Training data, where <code>n_samples</code> is the number of samples
    and <code>n_features</code> is the number of features.</p>
<div class="codehilite"><pre><span></span><code>Note that providing ``y`` is sufficient to generate the splits and
hence ``np.zeros(n_samples)`` may be used as a placeholder for
``X`` instead of actual training data.
</code></pre></div>

<p>y : array-like of shape (n_samples,)
    The target variable for supervised learning problems.
    Stratification is done based on the y labels.</p>
<p>groups : object
    Always ignored, exists for compatibility.</p>
<h6 id="giants.config.ModelConfig.CVClassification.split--yields">Yields<a class="headerlink" href="#giants.config.ModelConfig.CVClassification.split--yields" title="Permanent link">&para;</a></h6>
<p>train : ndarray
    The training set indices for that split.</p>
<p>test : ndarray
    The testing set indices for that split.</p>
<h6 id="giants.config.ModelConfig.CVClassification.split--notes">Notes<a class="headerlink" href="#giants.config.ModelConfig.CVClassification.split--notes" title="Permanent link">&para;</a></h6>
<p>Randomized CV splitters may return different results for each call of
split. You can make the results identical by setting <code>random_state</code>
to an integer.</p>

        <details class="quote">
          <summary>Source code in <code>giants/config.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">split</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Generate indices to split data into training and test set.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : array-like of shape (n_samples, n_features)</span>
<span class="sd">        Training data, where `n_samples` is the number of samples</span>
<span class="sd">        and `n_features` is the number of features.</span>

<span class="sd">        Note that providing ``y`` is sufficient to generate the splits and</span>
<span class="sd">        hence ``np.zeros(n_samples)`` may be used as a placeholder for</span>
<span class="sd">        ``X`` instead of actual training data.</span>

<span class="sd">    y : array-like of shape (n_samples,)</span>
<span class="sd">        The target variable for supervised learning problems.</span>
<span class="sd">        Stratification is done based on the y labels.</span>

<span class="sd">    groups : object</span>
<span class="sd">        Always ignored, exists for compatibility.</span>

<span class="sd">    Yields</span>
<span class="sd">    ------</span>
<span class="sd">    train : ndarray</span>
<span class="sd">        The training set indices for that split.</span>

<span class="sd">    test : ndarray</span>
<span class="sd">        The testing set indices for that split.</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    Randomized CV splitters may return different results for each call of</span>
<span class="sd">    split. You can make the results identical by setting `random_state`</span>
<span class="sd">    to an integer.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">ensure_2d</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">groups</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h3 class="doc doc-heading" id="giants.config.ModelConfig.CVRegression">
        <code>
CVRegression            (<span title="sklearn.model_selection._split._BaseKFold">_BaseKFold</span>)
        </code>



<a class="headerlink" href="#giants.config.ModelConfig.CVRegression" title="Permanent link">&para;</a></h3>

    <div class="doc doc-contents ">

      <p>K-Folds cross-validator</p>
<p>Provides train/test indices to split data in train/test sets. Split
dataset into k consecutive folds (without shuffling by default).</p>
<p>Each fold is then used once as a validation while the k - 1 remaining
folds form the training set.</p>
<p>Read more in the :ref:<code>User Guide &lt;k_fold&gt;</code>.</p>
<h5 id="giants.config.ModelConfig.CVRegression--parameters">Parameters<a class="headerlink" href="#giants.config.ModelConfig.CVRegression--parameters" title="Permanent link">&para;</a></h5>
<p>n_splits : int, default=5
    Number of folds. Must be at least 2.</p>
<div class="codehilite"><pre><span></span><code>.. versionchanged:: 0.22
    ``n_splits`` default value changed from 3 to 5.
</code></pre></div>

<p>shuffle : bool, default=False
    Whether to shuffle the data before splitting into batches.
    Note that the samples within each split will not be shuffled.</p>
<p>random_state : int, RandomState instance or None, default=None
    When <code>shuffle</code> is True, <code>random_state</code> affects the ordering of the
    indices, which controls the randomness of each fold. Otherwise, this
    parameter has no effect.
    Pass an int for reproducible output across multiple function calls.
    See :term:<code>Glossary &lt;random_state&gt;</code>.</p>
<h5 id="giants.config.ModelConfig.CVRegression--examples">Examples<a class="headerlink" href="#giants.config.ModelConfig.CVRegression--examples" title="Permanent link">&para;</a></h5>
<blockquote>
<blockquote>
<blockquote>
<p>import numpy as np
from sklearn.model_selection import KFold
X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])
y = np.array([1, 2, 3, 4])
kf = KFold(n_splits=2)
kf.get_n_splits(X)
2
print(kf)
KFold(n_splits=2, random_state=None, shuffle=False)
for train_index, test_index in kf.split(X):
...     print("TRAIN:", train_index, "TEST:", test_index)
...     X_train, X_test = X[train_index], X[test_index]
...     y_train, y_test = y[train_index], y[test_index]
TRAIN: [2 3] TEST: [0 1]
TRAIN: [0 1] TEST: [2 3]</p>
</blockquote>
</blockquote>
</blockquote>
<h5 id="giants.config.ModelConfig.CVRegression--notes">Notes<a class="headerlink" href="#giants.config.ModelConfig.CVRegression--notes" title="Permanent link">&para;</a></h5>
<p>The first <code>n_samples % n_splits</code> folds have size
<code>n_samples // n_splits + 1</code>, other folds have size
<code>n_samples // n_splits</code>, where <code>n_samples</code> is the number of samples.</p>
<p>Randomized CV splitters may return different results for each call of
split. You can make the results identical by setting <code>random_state</code>
to an integer.</p>
<h5 id="giants.config.ModelConfig.CVRegression--see-also">See Also<a class="headerlink" href="#giants.config.ModelConfig.CVRegression--see-also" title="Permanent link">&para;</a></h5>
<p>StratifiedKFold : Takes group information into account to avoid building
    folds with imbalanced class distributions (for binary or multiclass
    classification tasks).</p>
<p>GroupKFold : K-fold iterator variant with non-overlapping groups.</p>
<p>RepeatedKFold : Repeats K-Fold n times.</p>




  <div class="doc doc-children">












  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h3 class="doc doc-heading" id="giants.config.ModelConfig.Optimizer">
        <code>
Optimizer            (<span title="sklearn.model_selection._search.BaseSearchCV">BaseSearchCV</span>)
        </code>



<a class="headerlink" href="#giants.config.ModelConfig.Optimizer" title="Permanent link">&para;</a></h3>

    <div class="doc doc-contents ">

      <p>Exhaustive search over specified parameter values for an estimator.</p>
<p>Important members are fit, predict.</p>
<p>GridSearchCV implements a "fit" and a "score" method.
It also implements "score_samples", "predict", "predict_proba",
"decision_function", "transform" and "inverse_transform" if they are
implemented in the estimator used.</p>
<p>The parameters of the estimator used to apply these methods are optimized
by cross-validated grid-search over a parameter grid.</p>
<p>Read more in the :ref:<code>User Guide &lt;grid_search&gt;</code>.</p>
<h5 id="giants.config.ModelConfig.Optimizer--parameters">Parameters<a class="headerlink" href="#giants.config.ModelConfig.Optimizer--parameters" title="Permanent link">&para;</a></h5>
<p>estimator : estimator object
    This is assumed to implement the scikit-learn estimator interface.
    Either estimator needs to provide a <code>score</code> function,
    or <code>scoring</code> must be passed.</p>
<p>param_grid : dict or list of dictionaries
    Dictionary with parameters names (<code>str</code>) as keys and lists of
    parameter settings to try as values, or a list of such
    dictionaries, in which case the grids spanned by each dictionary
    in the list are explored. This enables searching over any sequence
    of parameter settings.</p>
<p>scoring : str, callable, list, tuple or dict, default=None
    Strategy to evaluate the performance of the cross-validated model on
    the test set.</p>
<div class="codehilite"><pre><span></span><code>If `scoring` represents a single score, one can use:

- a single string (see :ref:`scoring_parameter`);
- a callable (see :ref:`scoring`) that returns a single value.

If `scoring` represents multiple scores, one can use:

- a list or tuple of unique strings;
- a callable returning a dictionary where the keys are the metric
  names and the values are the metric scores;
- a dictionary with metric names as keys and callables a values.

See :ref:`multimetric_grid_search` for an example.
</code></pre></div>

<p>n_jobs : int, default=None
    Number of jobs to run in parallel.
    <code>None</code> means 1 unless in a :obj:<code>joblib.parallel_backend</code> context.
    <code>-1</code> means using all processors. See :term:<code>Glossary &lt;n_jobs&gt;</code>
    for more details.</p>
<div class="codehilite"><pre><span></span><code>.. versionchanged:: v0.20
   `n_jobs` default changed from 1 to None
</code></pre></div>

<p>refit : bool, str, or callable, default=True
    Refit an estimator using the best found parameters on the whole
    dataset.</p>
<div class="codehilite"><pre><span></span><code>For multiple metric evaluation, this needs to be a `str` denoting the
scorer that would be used to find the best parameters for refitting
the estimator at the end.

Where there are considerations other than maximum score in
choosing a best estimator, ``refit`` can be set to a function which
returns the selected ``best_index_`` given ``cv_results_``. In that
case, the ``best_estimator_`` and ``best_params_`` will be set
according to the returned ``best_index_`` while the ``best_score_``
attribute will not be available.

The refitted estimator is made available at the ``best_estimator_``
attribute and permits using ``predict`` directly on this
``GridSearchCV`` instance.

Also for multiple metric evaluation, the attributes ``best_index_``,
``best_score_`` and ``best_params_`` will only be available if
``refit`` is set and all of them will be determined w.r.t this specific
scorer.

See ``scoring`` parameter to know more about multiple metric
evaluation.

.. versionchanged:: 0.20
    Support for callable added.
</code></pre></div>

<p>cv : int, cross-validation generator or an iterable, default=None
    Determines the cross-validation splitting strategy.
    Possible inputs for cv are:</p>
<div class="codehilite"><pre><span></span><code>- None, to use the default 5-fold cross validation,
- integer, to specify the number of folds in a `(Stratified)KFold`,
- :term:`CV splitter`,
- An iterable yielding (train, test) splits as arrays of indices.

For integer/None inputs, if the estimator is a classifier and ``y`` is
either binary or multiclass, :class:`StratifiedKFold` is used. In all
other cases, :class:`KFold` is used. These splitters are instantiated
with `shuffle=False` so the splits will be the same across calls.

Refer :ref:`User Guide &lt;cross_validation&gt;` for the various
cross-validation strategies that can be used here.

.. versionchanged:: 0.22
    ``cv`` default value if None changed from 3-fold to 5-fold.
</code></pre></div>

<p>verbose : int
    Controls the verbosity: the higher, the more messages.</p>
<div class="codehilite"><pre><span></span><code>- &gt;1 : the computation time for each fold and parameter candidate is
  displayed;
- &gt;2 : the score is also displayed;
- &gt;3 : the fold and candidate parameter indexes are also displayed
  together with the starting time of the computation.
</code></pre></div>

<p>pre_dispatch : int, or str, default='2*n_jobs'
    Controls the number of jobs that get dispatched during parallel
    execution. Reducing this number can be useful to avoid an
    explosion of memory consumption when more jobs get dispatched
    than CPUs can process. This parameter can be:</p>
<div class="codehilite"><pre><span></span><code>    - None, in which case all the jobs are immediately
      created and spawned. Use this for lightweight and
      fast-running jobs, to avoid delays due to on-demand
      spawning of the jobs

    - An int, giving the exact number of total jobs that are
      spawned

    - A str, giving an expression as a function of n_jobs,
      as in &#39;2*n_jobs&#39;
</code></pre></div>

<p>error_score : 'raise' or numeric, default=np.nan
    Value to assign to the score if an error occurs in estimator fitting.
    If set to 'raise', the error is raised. If a numeric value is given,
    FitFailedWarning is raised. This parameter does not affect the refit
    step, which will always raise the error.</p>
<p>return_train_score : bool, default=False
    If <code>False</code>, the <code>cv_results_</code> attribute will not include training
    scores.
    Computing training scores is used to get insights on how different
    parameter settings impact the overfitting/underfitting trade-off.
    However computing the scores on the training set can be computationally
    expensive and is not strictly required to select the parameters that
    yield the best generalization performance.</p>
<div class="codehilite"><pre><span></span><code>.. versionadded:: 0.19

.. versionchanged:: 0.21
    Default value was changed from ``True`` to ``False``
</code></pre></div>

<h5 id="giants.config.ModelConfig.Optimizer--attributes">Attributes<a class="headerlink" href="#giants.config.ModelConfig.Optimizer--attributes" title="Permanent link">&para;</a></h5>
<p>cv_results_ : dict of numpy (masked) ndarrays
    A dict with keys as column headers and values as columns, that can be
    imported into a pandas <code>DataFrame</code>.</p>
<div class="codehilite"><pre><span></span><code>For instance the below given table

+------------+-----------+------------+-----------------+---+---------+
|param_kernel|param_gamma|param_degree|split0_test_score|...|rank_t...|
+============+===========+============+=================+===+=========+
|  &#39;poly&#39;    |     --    |      2     |       0.80      |...|    2    |
+------------+-----------+------------+-----------------+---+---------+
|  &#39;poly&#39;    |     --    |      3     |       0.70      |...|    4    |
+------------+-----------+------------+-----------------+---+---------+
|  &#39;rbf&#39;     |     0.1   |     --     |       0.80      |...|    3    |
+------------+-----------+------------+-----------------+---+---------+
|  &#39;rbf&#39;     |     0.2   |     --     |       0.93      |...|    1    |
+------------+-----------+------------+-----------------+---+---------+

will be represented by a ``cv_results_`` dict of::

    {
    &#39;param_kernel&#39;: masked_array(data = [&#39;poly&#39;, &#39;poly&#39;, &#39;rbf&#39;, &#39;rbf&#39;],
                                 mask = [False False False False]...)
    &#39;param_gamma&#39;: masked_array(data = [-- -- 0.1 0.2],
                                mask = [ True  True False False]...),
    &#39;param_degree&#39;: masked_array(data = [2.0 3.0 -- --],
                                 mask = [False False  True  True]...),
    &#39;split0_test_score&#39;  : [0.80, 0.70, 0.80, 0.93],
    &#39;split1_test_score&#39;  : [0.82, 0.50, 0.70, 0.78],
    &#39;mean_test_score&#39;    : [0.81, 0.60, 0.75, 0.85],
    &#39;std_test_score&#39;     : [0.01, 0.10, 0.05, 0.08],
    &#39;rank_test_score&#39;    : [2, 4, 3, 1],
    &#39;split0_train_score&#39; : [0.80, 0.92, 0.70, 0.93],
    &#39;split1_train_score&#39; : [0.82, 0.55, 0.70, 0.87],
    &#39;mean_train_score&#39;   : [0.81, 0.74, 0.70, 0.90],
    &#39;std_train_score&#39;    : [0.01, 0.19, 0.00, 0.03],
    &#39;mean_fit_time&#39;      : [0.73, 0.63, 0.43, 0.49],
    &#39;std_fit_time&#39;       : [0.01, 0.02, 0.01, 0.01],
    &#39;mean_score_time&#39;    : [0.01, 0.06, 0.04, 0.04],
    &#39;std_score_time&#39;     : [0.00, 0.00, 0.00, 0.01],
    &#39;params&#39;             : [{&#39;kernel&#39;: &#39;poly&#39;, &#39;degree&#39;: 2}, ...],
    }

NOTE

The key ``&#39;params&#39;`` is used to store a list of parameter
settings dicts for all the parameter candidates.

The ``mean_fit_time``, ``std_fit_time``, ``mean_score_time`` and
``std_score_time`` are all in seconds.

For multi-metric evaluation, the scores for all the scorers are
available in the ``cv_results_`` dict at the keys ending with that
scorer&#39;s name (``&#39;_&lt;scorer_name&gt;&#39;``) instead of ``&#39;_score&#39;`` shown
above. (&#39;split0_test_precision&#39;, &#39;mean_train_precision&#39; etc.)
</code></pre></div>

<p>best_estimator_ : estimator
    Estimator that was chosen by the search, i.e. estimator
    which gave highest score (or smallest loss if specified)
    on the left out data. Not available if <code>refit=False</code>.</p>
<div class="codehilite"><pre><span></span><code>See ``refit`` parameter for more information on allowed values.
</code></pre></div>

<p>best_score_ : float
    Mean cross-validated score of the best_estimator</p>
<div class="codehilite"><pre><span></span><code>For multi-metric evaluation, this is present only if ``refit`` is
specified.

This attribute is not available if ``refit`` is a function.
</code></pre></div>

<p>best_params_ : dict
    Parameter setting that gave the best results on the hold out data.</p>
<div class="codehilite"><pre><span></span><code>For multi-metric evaluation, this is present only if ``refit`` is
specified.
</code></pre></div>

<p>best_index_ : int
    The index (of the <code>cv_results_</code> arrays) which corresponds to the best
    candidate parameter setting.</p>
<div class="codehilite"><pre><span></span><code>The dict at ``search.cv_results_[&#39;params&#39;][search.best_index_]`` gives
the parameter setting for the best model, that gives the highest
mean score (``search.best_score_``).

For multi-metric evaluation, this is present only if ``refit`` is
specified.
</code></pre></div>

<p>scorer_ : function or a dict
    Scorer function used on the held out data to choose the best
    parameters for the model.</p>
<div class="codehilite"><pre><span></span><code>For multi-metric evaluation, this attribute holds the validated
``scoring`` dict which maps the scorer key to the scorer callable.
</code></pre></div>

<p>n_splits_ : int
    The number of cross-validation splits (folds/iterations).</p>
<p>refit_time_ : float
    Seconds used for refitting the best model on the whole dataset.</p>
<div class="codehilite"><pre><span></span><code>This is present only if ``refit`` is not False.

.. versionadded:: 0.20
</code></pre></div>

<p>multimetric_ : bool
    Whether or not the scorers compute several metrics.</p>
<p>classes_ : ndarray of shape (n_classes,)
    The classes labels. This is present only if <code>refit</code> is specified and
    the underlying estimator is a classifier.</p>
<p>n_features_in_ : int
    Number of features seen during :term:<code>fit</code>. Only defined if
    <code>best_estimator_</code> is defined (see the documentation for the <code>refit</code>
    parameter for more details) and that <code>best_estimator_</code> exposes
    <code>n_features_in_</code> when fit.</p>
<div class="codehilite"><pre><span></span><code>.. versionadded:: 0.24
</code></pre></div>

<p>feature_names_in_ : ndarray of shape (<code>n_features_in_</code>,)
    Names of features seen during :term:<code>fit</code>. Only defined if
    <code>best_estimator_</code> is defined (see the documentation for the <code>refit</code>
    parameter for more details) and that <code>best_estimator_</code> exposes
    <code>feature_names_in_</code> when fit.</p>
<div class="codehilite"><pre><span></span><code>.. versionadded:: 1.0
</code></pre></div>

<h5 id="giants.config.ModelConfig.Optimizer--notes">Notes<a class="headerlink" href="#giants.config.ModelConfig.Optimizer--notes" title="Permanent link">&para;</a></h5>
<p>The parameters selected are those that maximize the score of the left out
data, unless an explicit score is passed in which case it is used instead.</p>
<p>If <code>n_jobs</code> was set to a value higher than one, the data is copied for each
point in the grid (and not <code>n_jobs</code> times). This is done for efficiency
reasons if individual jobs take very little time, but may raise errors if
the dataset is large and not enough memory is available.  A workaround in
this case is to set <code>pre_dispatch</code>. Then, the memory is copied only
<code>pre_dispatch</code> many times. A reasonable value for <code>pre_dispatch</code> is <code>2 *
n_jobs</code>.</p>
<h5 id="giants.config.ModelConfig.Optimizer--see-also">See Also<a class="headerlink" href="#giants.config.ModelConfig.Optimizer--see-also" title="Permanent link">&para;</a></h5>
<p>ParameterGrid : Generates all the combinations of a hyperparameter grid.
train_test_split : Utility function to split the data into a development
    set usable for fitting a GridSearchCV instance and an evaluation set
    for its final evaluation.
sklearn.metrics.make_scorer : Make a scorer from a performance metric or
    loss function.</p>
<h5 id="giants.config.ModelConfig.Optimizer--examples">Examples<a class="headerlink" href="#giants.config.ModelConfig.Optimizer--examples" title="Permanent link">&para;</a></h5>
<blockquote>
<blockquote>
<blockquote>
<p>from sklearn import svm, datasets
from sklearn.model_selection import GridSearchCV
iris = datasets.load_iris()
parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}
svc = svm.SVC()
clf = GridSearchCV(svc, parameters)
clf.fit(iris.data, iris.target)
GridSearchCV(estimator=SVC(),
             param_grid={'C': [1, 10], 'kernel': ('linear', 'rbf')})
sorted(clf.cv_results_.keys())
['mean_fit_time', 'mean_score_time', 'mean_test_score',...
 'param_C', 'param_kernel', 'params',...
 'rank_test_score', 'split0_test_score',...
 'split2_test_score', ...
 'std_fit_time', 'std_score_time', 'std_test_score']</p>
</blockquote>
</blockquote>
</blockquote>




  <div class="doc doc-children">












  </div>

    </div>

  </div>







  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 class="doc doc-heading" id="giants.config.ModelEstimatorConfig">
        <code>
ModelEstimatorConfig        </code>



<a class="headerlink" href="#giants.config.ModelEstimatorConfig" title="Permanent link">&para;</a></h2>

    <div class="doc doc-contents ">

      <p>Stores sklearn model estimators</p>




  <div class="doc doc-children">







  <div class="doc doc-object doc-class">



<h3 class="doc doc-heading" id="giants.config.ModelEstimatorConfig.AdaBoostClassifier">
        <code>
AdaBoostClassifier            (<span title="sklearn.base.ClassifierMixin">ClassifierMixin</span>, <span title="sklearn.ensemble._weight_boosting.BaseWeightBoosting">BaseWeightBoosting</span>)
        </code>



<a class="headerlink" href="#giants.config.ModelEstimatorConfig.AdaBoostClassifier" title="Permanent link">&para;</a></h3>

    <div class="doc doc-contents ">

      <p>An AdaBoost classifier.</p>
<p>An AdaBoost [1] classifier is a meta-estimator that begins by fitting a
classifier on the original dataset and then fits additional copies of the
classifier on the same dataset but where the weights of incorrectly
classified instances are adjusted such that subsequent classifiers focus
more on difficult cases.</p>
<p>This class implements the algorithm known as AdaBoost-SAMME [2].</p>
<p>Read more in the :ref:<code>User Guide &lt;adaboost&gt;</code>.</p>
<p>.. versionadded:: 0.14</p>
<h5 id="giants.config.ModelEstimatorConfig.AdaBoostClassifier--parameters">Parameters<a class="headerlink" href="#giants.config.ModelEstimatorConfig.AdaBoostClassifier--parameters" title="Permanent link">&para;</a></h5>
<p>base_estimator : object, default=None
    The base estimator from which the boosted ensemble is built.
    Support for sample weighting is required, as well as proper
    <code>classes_</code> and <code>n_classes_</code> attributes. If <code>None</code>, then
    the base estimator is :class:<code>~sklearn.tree.DecisionTreeClassifier</code>
    initialized with <code>max_depth=1</code>.</p>
<p>n_estimators : int, default=50
    The maximum number of estimators at which boosting is terminated.
    In case of perfect fit, the learning procedure is stopped early.</p>
<p>learning_rate : float, default=1.0
    Weight applied to each classifier at each boosting iteration. A higher
    learning rate increases the contribution of each classifier. There is
    a trade-off between the <code>learning_rate</code> and <code>n_estimators</code> parameters.</p>
<p>algorithm : {'SAMME', 'SAMME.R'}, default='SAMME.R'
    If 'SAMME.R' then use the SAMME.R real boosting algorithm.
    <code>base_estimator</code> must support calculation of class probabilities.
    If 'SAMME' then use the SAMME discrete boosting algorithm.
    The SAMME.R algorithm typically converges faster than SAMME,
    achieving a lower test error with fewer boosting iterations.</p>
<p>random_state : int, RandomState instance or None, default=None
    Controls the random seed given at each <code>base_estimator</code> at each
    boosting iteration.
    Thus, it is only used when <code>base_estimator</code> exposes a <code>random_state</code>.
    Pass an int for reproducible output across multiple function calls.
    See :term:<code>Glossary &lt;random_state&gt;</code>.</p>
<h5 id="giants.config.ModelEstimatorConfig.AdaBoostClassifier--attributes">Attributes<a class="headerlink" href="#giants.config.ModelEstimatorConfig.AdaBoostClassifier--attributes" title="Permanent link">&para;</a></h5>
<p>base_estimator_ : estimator
    The base estimator from which the ensemble is grown.</p>
<p>estimators_ : list of classifiers
    The collection of fitted sub-estimators.</p>
<p>classes_ : ndarray of shape (n_classes,)
    The classes labels.</p>
<p>n_classes_ : int
    The number of classes.</p>
<p>estimator_weights_ : ndarray of floats
    Weights for each estimator in the boosted ensemble.</p>
<p>estimator_errors_ : ndarray of floats
    Classification error for each estimator in the boosted
    ensemble.</p>
<p>feature_importances_ : ndarray of shape (n_features,)
    The impurity-based feature importances if supported by the
    <code>base_estimator</code> (when based on decision trees).</p>
<div class="codehilite"><pre><span></span><code>Warning: impurity-based feature importances can be misleading for
high cardinality features (many unique values). See
:func:`sklearn.inspection.permutation_importance` as an alternative.
</code></pre></div>

<p>n_features_in_ : int
    Number of features seen during :term:<code>fit</code>.</p>
<div class="codehilite"><pre><span></span><code>.. versionadded:: 0.24
</code></pre></div>

<p>feature_names_in_ : ndarray of shape (<code>n_features_in_</code>,)
    Names of features seen during :term:<code>fit</code>. Defined only when <code>X</code>
    has feature names that are all strings.</p>
<div class="codehilite"><pre><span></span><code>.. versionadded:: 1.0
</code></pre></div>

<h5 id="giants.config.ModelEstimatorConfig.AdaBoostClassifier--see-also">See Also<a class="headerlink" href="#giants.config.ModelEstimatorConfig.AdaBoostClassifier--see-also" title="Permanent link">&para;</a></h5>
<p>AdaBoostRegressor : An AdaBoost regressor that begins by fitting a
    regressor on the original dataset and then fits additional copies of
    the regressor on the same dataset but where the weights of instances
    are adjusted according to the error of the current prediction.</p>
<p>GradientBoostingClassifier : GB builds an additive model in a forward
    stage-wise fashion. Regression trees are fit on the negative gradient
    of the binomial or multinomial deviance loss function. Binary
    classification is a special case where only a single regression tree is
    induced.</p>
<p>sklearn.tree.DecisionTreeClassifier : A non-parametric supervised learning
    method used for classification.
    Creates a model that predicts the value of a target variable by
    learning simple decision rules inferred from the data features.</p>
<h5 id="giants.config.ModelEstimatorConfig.AdaBoostClassifier--references">References<a class="headerlink" href="#giants.config.ModelEstimatorConfig.AdaBoostClassifier--references" title="Permanent link">&para;</a></h5>
<p>.. [1] Y. Freund, R. Schapire, "A Decision-Theoretic Generalization of
       on-Line Learning and an Application to Boosting", 1995.</p>
<p>.. [2] J. Zhu, H. Zou, S. Rosset, T. Hastie, "Multi-class AdaBoost", 2009.</p>
<h5 id="giants.config.ModelEstimatorConfig.AdaBoostClassifier--examples">Examples<a class="headerlink" href="#giants.config.ModelEstimatorConfig.AdaBoostClassifier--examples" title="Permanent link">&para;</a></h5>
<blockquote>
<blockquote>
<blockquote>
<p>from sklearn.ensemble import AdaBoostClassifier
from sklearn.datasets import make_classification
X, y = make_classification(n_samples=1000, n_features=4,
...                            n_informative=2, n_redundant=0,
...                            random_state=0, shuffle=False)
clf = AdaBoostClassifier(n_estimators=100, random_state=0)
clf.fit(X, y)
AdaBoostClassifier(n_estimators=100, random_state=0)
clf.predict([[0, 0, 0, 0]])
array([1])
clf.score(X, y)
0.983...</p>
</blockquote>
</blockquote>
</blockquote>




  <div class="doc doc-children">










  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="giants.config.ModelEstimatorConfig.AdaBoostClassifier.decision_function">
<code class="codehilite language-python"><span class="n">decision_function</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span></code>


<a class="headerlink" href="#giants.config.ModelEstimatorConfig.AdaBoostClassifier.decision_function" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Compute the decision function of <code>X</code>.</p>
<h6 id="giants.config.ModelEstimatorConfig.AdaBoostClassifier.decision_function--parameters">Parameters<a class="headerlink" href="#giants.config.ModelEstimatorConfig.AdaBoostClassifier.decision_function--parameters" title="Permanent link">&para;</a></h6>
<p>X : {array-like, sparse matrix} of shape (n_samples, n_features)
    The training input samples. Sparse matrix can be CSC, CSR, COO,
    DOK, or LIL. COO, DOK, and LIL are converted to CSR.</p>
<h6 id="giants.config.ModelEstimatorConfig.AdaBoostClassifier.decision_function--returns">Returns<a class="headerlink" href="#giants.config.ModelEstimatorConfig.AdaBoostClassifier.decision_function--returns" title="Permanent link">&para;</a></h6>
<p>score : ndarray of shape of (n_samples, k)
    The decision function of the input samples. The order of
    outputs is the same of that of the :term:<code>classes_</code> attribute.
    Binary classification is a special cases with <code>k == 1</code>,
    otherwise <code>k==n_classes</code>. For binary classification,
    values closer to -1 or 1 mean more like the first or second
    class in <code>classes_</code>, respectively.</p>

        <details class="quote">
          <summary>Source code in <code>giants/config.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">decision_function</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Compute the decision function of ``X``.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">        The training input samples. Sparse matrix can be CSC, CSR, COO,</span>
<span class="sd">        DOK, or LIL. COO, DOK, and LIL are converted to CSR.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    score : ndarray of shape of (n_samples, k)</span>
<span class="sd">        The decision function of the input samples. The order of</span>
<span class="sd">        outputs is the same of that of the :term:`classes_` attribute.</span>
<span class="sd">        Binary classification is a special cases with ``k == 1``,</span>
<span class="sd">        otherwise ``k==n_classes``. For binary classification,</span>
<span class="sd">        values closer to -1 or 1 mean more like the first or second</span>
<span class="sd">        class in ``classes_``, respectively.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
    <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_check_X</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

    <span class="n">n_classes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_classes_</span>
    <span class="n">classes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">algorithm</span> <span class="o">==</span> <span class="s2">&quot;SAMME.R&quot;</span><span class="p">:</span>
        <span class="c1"># The weights are all 1. for SAMME.R</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span>
            <span class="n">_samme_proba</span><span class="p">(</span><span class="n">estimator</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span> <span class="k">for</span> <span class="n">estimator</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>  <span class="c1"># self.algorithm == &quot;SAMME&quot;</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span>
            <span class="p">(</span><span class="n">estimator</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">==</span> <span class="n">classes</span><span class="p">)</span><span class="o">.</span><span class="n">T</span> <span class="o">*</span> <span class="n">w</span>
            <span class="k">for</span> <span class="n">estimator</span><span class="p">,</span> <span class="n">w</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimator_weights_</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="n">pred</span> <span class="o">/=</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimator_weights_</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">n_classes</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="n">pred</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">*=</span> <span class="o">-</span><span class="mi">1</span>
        <span class="k">return</span> <span class="n">pred</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">pred</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="giants.config.ModelEstimatorConfig.AdaBoostClassifier.fit">
<code class="codehilite language-python"><span class="n">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>


<a class="headerlink" href="#giants.config.ModelEstimatorConfig.AdaBoostClassifier.fit" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Build a boosted classifier from the training set (X, y).</p>
<h6 id="giants.config.ModelEstimatorConfig.AdaBoostClassifier.fit--parameters">Parameters<a class="headerlink" href="#giants.config.ModelEstimatorConfig.AdaBoostClassifier.fit--parameters" title="Permanent link">&para;</a></h6>
<p>X : {array-like, sparse matrix} of shape (n_samples, n_features)
    The training input samples. Sparse matrix can be CSC, CSR, COO,
    DOK, or LIL. COO, DOK, and LIL are converted to CSR.</p>
<p>y : array-like of shape (n_samples,)
    The target values (class labels).</p>
<p>sample_weight : array-like of shape (n_samples,), default=None
    Sample weights. If None, the sample weights are initialized to
    <code>1 / n_samples</code>.</p>
<h6 id="giants.config.ModelEstimatorConfig.AdaBoostClassifier.fit--returns">Returns<a class="headerlink" href="#giants.config.ModelEstimatorConfig.AdaBoostClassifier.fit--returns" title="Permanent link">&para;</a></h6>
<p>self : object
    Fitted estimator.</p>

        <details class="quote">
          <summary>Source code in <code>giants/config.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Build a boosted classifier from the training set (X, y).</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">        The training input samples. Sparse matrix can be CSC, CSR, COO,</span>
<span class="sd">        DOK, or LIL. COO, DOK, and LIL are converted to CSR.</span>

<span class="sd">    y : array-like of shape (n_samples,)</span>
<span class="sd">        The target values (class labels).</span>

<span class="sd">    sample_weight : array-like of shape (n_samples,), default=None</span>
<span class="sd">        Sample weights. If None, the sample weights are initialized to</span>
<span class="sd">        ``1 / n_samples``.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    self : object</span>
<span class="sd">        Fitted estimator.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Check that algorithm is supported</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">algorithm</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="s2">&quot;SAMME&quot;</span><span class="p">,</span> <span class="s2">&quot;SAMME.R&quot;</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;algorithm </span><span class="si">%s</span><span class="s2"> is not supported&quot;</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">algorithm</span><span class="p">)</span>

    <span class="c1"># Fit</span>
    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="giants.config.ModelEstimatorConfig.AdaBoostClassifier.predict">
<code class="codehilite language-python"><span class="n">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span></code>


<a class="headerlink" href="#giants.config.ModelEstimatorConfig.AdaBoostClassifier.predict" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Predict classes for X.</p>
<p>The predicted class of an input sample is computed as the weighted mean
prediction of the classifiers in the ensemble.</p>
<h6 id="giants.config.ModelEstimatorConfig.AdaBoostClassifier.predict--parameters">Parameters<a class="headerlink" href="#giants.config.ModelEstimatorConfig.AdaBoostClassifier.predict--parameters" title="Permanent link">&para;</a></h6>
<p>X : {array-like, sparse matrix} of shape (n_samples, n_features)
    The training input samples. Sparse matrix can be CSC, CSR, COO,
    DOK, or LIL. COO, DOK, and LIL are converted to CSR.</p>
<h6 id="giants.config.ModelEstimatorConfig.AdaBoostClassifier.predict--returns">Returns<a class="headerlink" href="#giants.config.ModelEstimatorConfig.AdaBoostClassifier.predict--returns" title="Permanent link">&para;</a></h6>
<p>y : ndarray of shape (n_samples,)
    The predicted classes.</p>

        <details class="quote">
          <summary>Source code in <code>giants/config.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Predict classes for X.</span>

<span class="sd">    The predicted class of an input sample is computed as the weighted mean</span>
<span class="sd">    prediction of the classifiers in the ensemble.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">        The training input samples. Sparse matrix can be CSC, CSR, COO,</span>
<span class="sd">        DOK, or LIL. COO, DOK, and LIL are converted to CSR.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    y : ndarray of shape (n_samples,)</span>
<span class="sd">        The predicted classes.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decision_function</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_classes_</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">pred</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="giants.config.ModelEstimatorConfig.AdaBoostClassifier.predict_log_proba">
<code class="codehilite language-python"><span class="n">predict_log_proba</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span></code>


<a class="headerlink" href="#giants.config.ModelEstimatorConfig.AdaBoostClassifier.predict_log_proba" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Predict class log-probabilities for X.</p>
<p>The predicted class log-probabilities of an input sample is computed as
the weighted mean predicted class log-probabilities of the classifiers
in the ensemble.</p>
<h6 id="giants.config.ModelEstimatorConfig.AdaBoostClassifier.predict_log_proba--parameters">Parameters<a class="headerlink" href="#giants.config.ModelEstimatorConfig.AdaBoostClassifier.predict_log_proba--parameters" title="Permanent link">&para;</a></h6>
<p>X : {array-like, sparse matrix} of shape (n_samples, n_features)
    The training input samples. Sparse matrix can be CSC, CSR, COO,
    DOK, or LIL. COO, DOK, and LIL are converted to CSR.</p>
<h6 id="giants.config.ModelEstimatorConfig.AdaBoostClassifier.predict_log_proba--returns">Returns<a class="headerlink" href="#giants.config.ModelEstimatorConfig.AdaBoostClassifier.predict_log_proba--returns" title="Permanent link">&para;</a></h6>
<p>p : ndarray of shape (n_samples, n_classes)
    The class probabilities of the input samples. The order of
    outputs is the same of that of the :term:<code>classes_</code> attribute.</p>

        <details class="quote">
          <summary>Source code in <code>giants/config.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">predict_log_proba</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Predict class log-probabilities for X.</span>

<span class="sd">    The predicted class log-probabilities of an input sample is computed as</span>
<span class="sd">    the weighted mean predicted class log-probabilities of the classifiers</span>
<span class="sd">    in the ensemble.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">        The training input samples. Sparse matrix can be CSC, CSR, COO,</span>
<span class="sd">        DOK, or LIL. COO, DOK, and LIL are converted to CSR.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    p : ndarray of shape (n_samples, n_classes)</span>
<span class="sd">        The class probabilities of the input samples. The order of</span>
<span class="sd">        outputs is the same of that of the :term:`classes_` attribute.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="giants.config.ModelEstimatorConfig.AdaBoostClassifier.predict_proba">
<code class="codehilite language-python"><span class="n">predict_proba</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span></code>


<a class="headerlink" href="#giants.config.ModelEstimatorConfig.AdaBoostClassifier.predict_proba" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Predict class probabilities for X.</p>
<p>The predicted class probabilities of an input sample is computed as
the weighted mean predicted class probabilities of the classifiers
in the ensemble.</p>
<h6 id="giants.config.ModelEstimatorConfig.AdaBoostClassifier.predict_proba--parameters">Parameters<a class="headerlink" href="#giants.config.ModelEstimatorConfig.AdaBoostClassifier.predict_proba--parameters" title="Permanent link">&para;</a></h6>
<p>X : {array-like, sparse matrix} of shape (n_samples, n_features)
    The training input samples. Sparse matrix can be CSC, CSR, COO,
    DOK, or LIL. COO, DOK, and LIL are converted to CSR.</p>
<h6 id="giants.config.ModelEstimatorConfig.AdaBoostClassifier.predict_proba--returns">Returns<a class="headerlink" href="#giants.config.ModelEstimatorConfig.AdaBoostClassifier.predict_proba--returns" title="Permanent link">&para;</a></h6>
<p>p : ndarray of shape (n_samples, n_classes)
    The class probabilities of the input samples. The order of
    outputs is the same of that of the :term:<code>classes_</code> attribute.</p>

        <details class="quote">
          <summary>Source code in <code>giants/config.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">predict_proba</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Predict class probabilities for X.</span>

<span class="sd">    The predicted class probabilities of an input sample is computed as</span>
<span class="sd">    the weighted mean predicted class probabilities of the classifiers</span>
<span class="sd">    in the ensemble.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">        The training input samples. Sparse matrix can be CSC, CSR, COO,</span>
<span class="sd">        DOK, or LIL. COO, DOK, and LIL are converted to CSR.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    p : ndarray of shape (n_samples, n_classes)</span>
<span class="sd">        The class probabilities of the input samples. The order of</span>
<span class="sd">        outputs is the same of that of the :term:`classes_` attribute.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
    <span class="n">n_classes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_classes_</span>

    <span class="k">if</span> <span class="n">n_classes</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">_num_samples</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="mi">1</span><span class="p">))</span>

    <span class="n">decision</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decision_function</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compute_proba_from_decision</span><span class="p">(</span><span class="n">decision</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="giants.config.ModelEstimatorConfig.AdaBoostClassifier.staged_decision_function">
<code class="codehilite language-python"><span class="n">staged_decision_function</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span></code>


<a class="headerlink" href="#giants.config.ModelEstimatorConfig.AdaBoostClassifier.staged_decision_function" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Compute decision function of <code>X</code> for each boosting iteration.</p>
<p>This method allows monitoring (i.e. determine error on testing set)
after each boosting iteration.</p>
<h6 id="giants.config.ModelEstimatorConfig.AdaBoostClassifier.staged_decision_function--parameters">Parameters<a class="headerlink" href="#giants.config.ModelEstimatorConfig.AdaBoostClassifier.staged_decision_function--parameters" title="Permanent link">&para;</a></h6>
<p>X : {array-like, sparse matrix} of shape (n_samples, n_features)
    The training input samples. Sparse matrix can be CSC, CSR, COO,
    DOK, or LIL. COO, DOK, and LIL are converted to CSR.</p>
<h6 id="giants.config.ModelEstimatorConfig.AdaBoostClassifier.staged_decision_function--yields">Yields<a class="headerlink" href="#giants.config.ModelEstimatorConfig.AdaBoostClassifier.staged_decision_function--yields" title="Permanent link">&para;</a></h6>
<p>score : generator of ndarray of shape (n_samples, k)
    The decision function of the input samples. The order of
    outputs is the same of that of the :term:<code>classes_</code> attribute.
    Binary classification is a special cases with <code>k == 1</code>,
    otherwise <code>k==n_classes</code>. For binary classification,
    values closer to -1 or 1 mean more like the first or second
    class in <code>classes_</code>, respectively.</p>

        <details class="quote">
          <summary>Source code in <code>giants/config.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">staged_decision_function</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Compute decision function of ``X`` for each boosting iteration.</span>

<span class="sd">    This method allows monitoring (i.e. determine error on testing set)</span>
<span class="sd">    after each boosting iteration.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">        The training input samples. Sparse matrix can be CSC, CSR, COO,</span>
<span class="sd">        DOK, or LIL. COO, DOK, and LIL are converted to CSR.</span>

<span class="sd">    Yields</span>
<span class="sd">    ------</span>
<span class="sd">    score : generator of ndarray of shape (n_samples, k)</span>
<span class="sd">        The decision function of the input samples. The order of</span>
<span class="sd">        outputs is the same of that of the :term:`classes_` attribute.</span>
<span class="sd">        Binary classification is a special cases with ``k == 1``,</span>
<span class="sd">        otherwise ``k==n_classes``. For binary classification,</span>
<span class="sd">        values closer to -1 or 1 mean more like the first or second</span>
<span class="sd">        class in ``classes_``, respectively.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
    <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_check_X</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

    <span class="n">n_classes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_classes_</span>
    <span class="n">classes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>
    <span class="n">pred</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">norm</span> <span class="o">=</span> <span class="mf">0.0</span>

    <span class="k">for</span> <span class="n">weight</span><span class="p">,</span> <span class="n">estimator</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimator_weights_</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">):</span>
        <span class="n">norm</span> <span class="o">+=</span> <span class="n">weight</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">algorithm</span> <span class="o">==</span> <span class="s2">&quot;SAMME.R&quot;</span><span class="p">:</span>
            <span class="c1"># The weights are all 1. for SAMME.R</span>
            <span class="n">current_pred</span> <span class="o">=</span> <span class="n">_samme_proba</span><span class="p">(</span><span class="n">estimator</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>  <span class="c1"># elif self.algorithm == &quot;SAMME&quot;:</span>
            <span class="n">current_pred</span> <span class="o">=</span> <span class="n">estimator</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
            <span class="n">current_pred</span> <span class="o">=</span> <span class="p">(</span><span class="n">current_pred</span> <span class="o">==</span> <span class="n">classes</span><span class="p">)</span><span class="o">.</span><span class="n">T</span> <span class="o">*</span> <span class="n">weight</span>

        <span class="k">if</span> <span class="n">pred</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">pred</span> <span class="o">=</span> <span class="n">current_pred</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">pred</span> <span class="o">+=</span> <span class="n">current_pred</span>

        <span class="k">if</span> <span class="n">n_classes</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">tmp_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">pred</span><span class="p">)</span>
            <span class="n">tmp_pred</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">*=</span> <span class="o">-</span><span class="mi">1</span>
            <span class="k">yield</span> <span class="p">(</span><span class="n">tmp_pred</span> <span class="o">/</span> <span class="n">norm</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">yield</span> <span class="n">pred</span> <span class="o">/</span> <span class="n">norm</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="giants.config.ModelEstimatorConfig.AdaBoostClassifier.staged_predict">
<code class="codehilite language-python"><span class="n">staged_predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span></code>


<a class="headerlink" href="#giants.config.ModelEstimatorConfig.AdaBoostClassifier.staged_predict" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Return staged predictions for X.</p>
<p>The predicted class of an input sample is computed as the weighted mean
prediction of the classifiers in the ensemble.</p>
<p>This generator method yields the ensemble prediction after each
iteration of boosting and therefore allows monitoring, such as to
determine the prediction on a test set after each boost.</p>
<h6 id="giants.config.ModelEstimatorConfig.AdaBoostClassifier.staged_predict--parameters">Parameters<a class="headerlink" href="#giants.config.ModelEstimatorConfig.AdaBoostClassifier.staged_predict--parameters" title="Permanent link">&para;</a></h6>
<p>X : array-like of shape (n_samples, n_features)
    The input samples. Sparse matrix can be CSC, CSR, COO,
    DOK, or LIL. COO, DOK, and LIL are converted to CSR.</p>
<h6 id="giants.config.ModelEstimatorConfig.AdaBoostClassifier.staged_predict--yields">Yields<a class="headerlink" href="#giants.config.ModelEstimatorConfig.AdaBoostClassifier.staged_predict--yields" title="Permanent link">&para;</a></h6>
<p>y : generator of ndarray of shape (n_samples,)
    The predicted classes.</p>

        <details class="quote">
          <summary>Source code in <code>giants/config.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">staged_predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Return staged predictions for X.</span>

<span class="sd">    The predicted class of an input sample is computed as the weighted mean</span>
<span class="sd">    prediction of the classifiers in the ensemble.</span>

<span class="sd">    This generator method yields the ensemble prediction after each</span>
<span class="sd">    iteration of boosting and therefore allows monitoring, such as to</span>
<span class="sd">    determine the prediction on a test set after each boost.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : array-like of shape (n_samples, n_features)</span>
<span class="sd">        The input samples. Sparse matrix can be CSC, CSR, COO,</span>
<span class="sd">        DOK, or LIL. COO, DOK, and LIL are converted to CSR.</span>

<span class="sd">    Yields</span>
<span class="sd">    ------</span>
<span class="sd">    y : generator of ndarray of shape (n_samples,)</span>
<span class="sd">        The predicted classes.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_check_X</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

    <span class="n">n_classes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_classes_</span>
    <span class="n">classes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span>

    <span class="k">if</span> <span class="n">n_classes</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">pred</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">staged_decision_function</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
            <span class="k">yield</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">classes</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">pred</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>

    <span class="k">else</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">pred</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">staged_decision_function</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
            <span class="k">yield</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">classes</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="giants.config.ModelEstimatorConfig.AdaBoostClassifier.staged_predict_proba">
<code class="codehilite language-python"><span class="n">staged_predict_proba</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span></code>


<a class="headerlink" href="#giants.config.ModelEstimatorConfig.AdaBoostClassifier.staged_predict_proba" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Predict class probabilities for X.</p>
<p>The predicted class probabilities of an input sample is computed as
the weighted mean predicted class probabilities of the classifiers
in the ensemble.</p>
<p>This generator method yields the ensemble predicted class probabilities
after each iteration of boosting and therefore allows monitoring, such
as to determine the predicted class probabilities on a test set after
each boost.</p>
<h6 id="giants.config.ModelEstimatorConfig.AdaBoostClassifier.staged_predict_proba--parameters">Parameters<a class="headerlink" href="#giants.config.ModelEstimatorConfig.AdaBoostClassifier.staged_predict_proba--parameters" title="Permanent link">&para;</a></h6>
<p>X : {array-like, sparse matrix} of shape (n_samples, n_features)
    The training input samples. Sparse matrix can be CSC, CSR, COO,
    DOK, or LIL. COO, DOK, and LIL are converted to CSR.</p>
<h6 id="giants.config.ModelEstimatorConfig.AdaBoostClassifier.staged_predict_proba--yields">Yields<a class="headerlink" href="#giants.config.ModelEstimatorConfig.AdaBoostClassifier.staged_predict_proba--yields" title="Permanent link">&para;</a></h6>
<p>p : generator of ndarray of shape (n_samples,)
    The class probabilities of the input samples. The order of
    outputs is the same of that of the :term:<code>classes_</code> attribute.</p>

        <details class="quote">
          <summary>Source code in <code>giants/config.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">staged_predict_proba</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Predict class probabilities for X.</span>

<span class="sd">    The predicted class probabilities of an input sample is computed as</span>
<span class="sd">    the weighted mean predicted class probabilities of the classifiers</span>
<span class="sd">    in the ensemble.</span>

<span class="sd">    This generator method yields the ensemble predicted class probabilities</span>
<span class="sd">    after each iteration of boosting and therefore allows monitoring, such</span>
<span class="sd">    as to determine the predicted class probabilities on a test set after</span>
<span class="sd">    each boost.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">        The training input samples. Sparse matrix can be CSC, CSR, COO,</span>
<span class="sd">        DOK, or LIL. COO, DOK, and LIL are converted to CSR.</span>

<span class="sd">    Yields</span>
<span class="sd">    ------</span>
<span class="sd">    p : generator of ndarray of shape (n_samples,)</span>
<span class="sd">        The class probabilities of the input samples. The order of</span>
<span class="sd">        outputs is the same of that of the :term:`classes_` attribute.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">n_classes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_classes_</span>

    <span class="k">for</span> <span class="n">decision</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">staged_decision_function</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
        <span class="k">yield</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compute_proba_from_decision</span><span class="p">(</span><span class="n">decision</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h3 class="doc doc-heading" id="giants.config.ModelEstimatorConfig.AdaBoostRegressor">
        <code>
AdaBoostRegressor            (<span title="sklearn.base.RegressorMixin">RegressorMixin</span>, <span title="sklearn.ensemble._weight_boosting.BaseWeightBoosting">BaseWeightBoosting</span>)
        </code>



<a class="headerlink" href="#giants.config.ModelEstimatorConfig.AdaBoostRegressor" title="Permanent link">&para;</a></h3>

    <div class="doc doc-contents ">

      <p>An AdaBoost regressor.</p>
<p>An AdaBoost [1] regressor is a meta-estimator that begins by fitting a
regressor on the original dataset and then fits additional copies of the
regressor on the same dataset but where the weights of instances are
adjusted according to the error of the current prediction. As such,
subsequent regressors focus more on difficult cases.</p>
<p>This class implements the algorithm known as AdaBoost.R2 [2].</p>
<p>Read more in the :ref:<code>User Guide &lt;adaboost&gt;</code>.</p>
<p>.. versionadded:: 0.14</p>
<h5 id="giants.config.ModelEstimatorConfig.AdaBoostRegressor--parameters">Parameters<a class="headerlink" href="#giants.config.ModelEstimatorConfig.AdaBoostRegressor--parameters" title="Permanent link">&para;</a></h5>
<p>base_estimator : object, default=None
    The base estimator from which the boosted ensemble is built.
    If <code>None</code>, then the base estimator is
    :class:<code>~sklearn.tree.DecisionTreeRegressor</code> initialized with
    <code>max_depth=3</code>.</p>
<p>n_estimators : int, default=50
    The maximum number of estimators at which boosting is terminated.
    In case of perfect fit, the learning procedure is stopped early.</p>
<p>learning_rate : float, default=1.0
    Weight applied to each regressor at each boosting iteration. A higher
    learning rate increases the contribution of each regressor. There is
    a trade-off between the <code>learning_rate</code> and <code>n_estimators</code> parameters.</p>
<p>loss : {'linear', 'square', 'exponential'}, default='linear'
    The loss function to use when updating the weights after each
    boosting iteration.</p>
<p>random_state : int, RandomState instance or None, default=None
    Controls the random seed given at each <code>base_estimator</code> at each
    boosting iteration.
    Thus, it is only used when <code>base_estimator</code> exposes a <code>random_state</code>.
    In addition, it controls the bootstrap of the weights used to train the
    <code>base_estimator</code> at each boosting iteration.
    Pass an int for reproducible output across multiple function calls.
    See :term:<code>Glossary &lt;random_state&gt;</code>.</p>
<h5 id="giants.config.ModelEstimatorConfig.AdaBoostRegressor--attributes">Attributes<a class="headerlink" href="#giants.config.ModelEstimatorConfig.AdaBoostRegressor--attributes" title="Permanent link">&para;</a></h5>
<p>base_estimator_ : estimator
    The base estimator from which the ensemble is grown.</p>
<p>estimators_ : list of regressors
    The collection of fitted sub-estimators.</p>
<p>estimator_weights_ : ndarray of floats
    Weights for each estimator in the boosted ensemble.</p>
<p>estimator_errors_ : ndarray of floats
    Regression error for each estimator in the boosted ensemble.</p>
<p>feature_importances_ : ndarray of shape (n_features,)
    The impurity-based feature importances if supported by the
    <code>base_estimator</code> (when based on decision trees).</p>
<div class="codehilite"><pre><span></span><code>Warning: impurity-based feature importances can be misleading for
high cardinality features (many unique values). See
:func:`sklearn.inspection.permutation_importance` as an alternative.
</code></pre></div>

<p>n_features_in_ : int
    Number of features seen during :term:<code>fit</code>.</p>
<div class="codehilite"><pre><span></span><code>.. versionadded:: 0.24
</code></pre></div>

<p>feature_names_in_ : ndarray of shape (<code>n_features_in_</code>,)
    Names of features seen during :term:<code>fit</code>. Defined only when <code>X</code>
    has feature names that are all strings.</p>
<div class="codehilite"><pre><span></span><code>.. versionadded:: 1.0
</code></pre></div>

<h5 id="giants.config.ModelEstimatorConfig.AdaBoostRegressor--see-also">See Also<a class="headerlink" href="#giants.config.ModelEstimatorConfig.AdaBoostRegressor--see-also" title="Permanent link">&para;</a></h5>
<p>AdaBoostClassifier : An AdaBoost classifier.
GradientBoostingRegressor : Gradient Boosting Classification Tree.
sklearn.tree.DecisionTreeRegressor : A decision tree regressor.</p>
<h5 id="giants.config.ModelEstimatorConfig.AdaBoostRegressor--references">References<a class="headerlink" href="#giants.config.ModelEstimatorConfig.AdaBoostRegressor--references" title="Permanent link">&para;</a></h5>
<p>.. [1] Y. Freund, R. Schapire, "A Decision-Theoretic Generalization of
       on-Line Learning and an Application to Boosting", 1995.</p>
<p>.. [2] H. Drucker, "Improving Regressors using Boosting Techniques", 1997.</p>
<h5 id="giants.config.ModelEstimatorConfig.AdaBoostRegressor--examples">Examples<a class="headerlink" href="#giants.config.ModelEstimatorConfig.AdaBoostRegressor--examples" title="Permanent link">&para;</a></h5>
<blockquote>
<blockquote>
<blockquote>
<p>from sklearn.ensemble import AdaBoostRegressor
from sklearn.datasets import make_regression
X, y = make_regression(n_features=4, n_informative=2,
...                        random_state=0, shuffle=False)
regr = AdaBoostRegressor(random_state=0, n_estimators=100)
regr.fit(X, y)
AdaBoostRegressor(n_estimators=100, random_state=0)
regr.predict([[0, 0, 0, 0]])
array([4.7972...])
regr.score(X, y)
0.9771...</p>
</blockquote>
</blockquote>
</blockquote>




  <div class="doc doc-children">










  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="giants.config.ModelEstimatorConfig.AdaBoostRegressor.fit">
<code class="codehilite language-python"><span class="n">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>


<a class="headerlink" href="#giants.config.ModelEstimatorConfig.AdaBoostRegressor.fit" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Build a boosted regressor from the training set (X, y).</p>
<h6 id="giants.config.ModelEstimatorConfig.AdaBoostRegressor.fit--parameters">Parameters<a class="headerlink" href="#giants.config.ModelEstimatorConfig.AdaBoostRegressor.fit--parameters" title="Permanent link">&para;</a></h6>
<p>X : {array-like, sparse matrix} of shape (n_samples, n_features)
    The training input samples. Sparse matrix can be CSC, CSR, COO,
    DOK, or LIL. COO, DOK, and LIL are converted to CSR.</p>
<p>y : array-like of shape (n_samples,)
    The target values (real numbers).</p>
<p>sample_weight : array-like of shape (n_samples,), default=None
    Sample weights. If None, the sample weights are initialized to
    1 / n_samples.</p>
<h6 id="giants.config.ModelEstimatorConfig.AdaBoostRegressor.fit--returns">Returns<a class="headerlink" href="#giants.config.ModelEstimatorConfig.AdaBoostRegressor.fit--returns" title="Permanent link">&para;</a></h6>
<p>self : object
    Fitted AdaBoostRegressor estimator.</p>

        <details class="quote">
          <summary>Source code in <code>giants/config.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Build a boosted regressor from the training set (X, y).</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">        The training input samples. Sparse matrix can be CSC, CSR, COO,</span>
<span class="sd">        DOK, or LIL. COO, DOK, and LIL are converted to CSR.</span>

<span class="sd">    y : array-like of shape (n_samples,)</span>
<span class="sd">        The target values (real numbers).</span>

<span class="sd">    sample_weight : array-like of shape (n_samples,), default=None</span>
<span class="sd">        Sample weights. If None, the sample weights are initialized to</span>
<span class="sd">        1 / n_samples.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    self : object</span>
<span class="sd">        Fitted AdaBoostRegressor estimator.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Check loss</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="s2">&quot;linear&quot;</span><span class="p">,</span> <span class="s2">&quot;square&quot;</span><span class="p">,</span> <span class="s2">&quot;exponential&quot;</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;loss must be &#39;linear&#39;, &#39;square&#39;, or &#39;exponential&#39;&quot;</span><span class="p">)</span>

    <span class="c1"># Fit</span>
    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="giants.config.ModelEstimatorConfig.AdaBoostRegressor.predict">
<code class="codehilite language-python"><span class="n">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span></code>


<a class="headerlink" href="#giants.config.ModelEstimatorConfig.AdaBoostRegressor.predict" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Predict regression value for X.</p>
<p>The predicted regression value of an input sample is computed
as the weighted median prediction of the regressors in the ensemble.</p>
<h6 id="giants.config.ModelEstimatorConfig.AdaBoostRegressor.predict--parameters">Parameters<a class="headerlink" href="#giants.config.ModelEstimatorConfig.AdaBoostRegressor.predict--parameters" title="Permanent link">&para;</a></h6>
<p>X : {array-like, sparse matrix} of shape (n_samples, n_features)
    The training input samples. Sparse matrix can be CSC, CSR, COO,
    DOK, or LIL. COO, DOK, and LIL are converted to CSR.</p>
<h6 id="giants.config.ModelEstimatorConfig.AdaBoostRegressor.predict--returns">Returns<a class="headerlink" href="#giants.config.ModelEstimatorConfig.AdaBoostRegressor.predict--returns" title="Permanent link">&para;</a></h6>
<p>y : ndarray of shape (n_samples,)
    The predicted regression values.</p>

        <details class="quote">
          <summary>Source code in <code>giants/config.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Predict regression value for X.</span>

<span class="sd">    The predicted regression value of an input sample is computed</span>
<span class="sd">    as the weighted median prediction of the regressors in the ensemble.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">        The training input samples. Sparse matrix can be CSC, CSR, COO,</span>
<span class="sd">        DOK, or LIL. COO, DOK, and LIL are converted to CSR.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    y : ndarray of shape (n_samples,)</span>
<span class="sd">        The predicted regression values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
    <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_check_X</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_median_predict</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">))</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="giants.config.ModelEstimatorConfig.AdaBoostRegressor.staged_predict">
<code class="codehilite language-python"><span class="n">staged_predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span></code>


<a class="headerlink" href="#giants.config.ModelEstimatorConfig.AdaBoostRegressor.staged_predict" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Return staged predictions for X.</p>
<p>The predicted regression value of an input sample is computed
as the weighted median prediction of the regressors in the ensemble.</p>
<p>This generator method yields the ensemble prediction after each
iteration of boosting and therefore allows monitoring, such as to
determine the prediction on a test set after each boost.</p>
<h6 id="giants.config.ModelEstimatorConfig.AdaBoostRegressor.staged_predict--parameters">Parameters<a class="headerlink" href="#giants.config.ModelEstimatorConfig.AdaBoostRegressor.staged_predict--parameters" title="Permanent link">&para;</a></h6>
<p>X : {array-like, sparse matrix} of shape (n_samples, n_features)
    The training input samples.</p>
<h6 id="giants.config.ModelEstimatorConfig.AdaBoostRegressor.staged_predict--yields">Yields<a class="headerlink" href="#giants.config.ModelEstimatorConfig.AdaBoostRegressor.staged_predict--yields" title="Permanent link">&para;</a></h6>
<p>y : generator of ndarray of shape (n_samples,)
    The predicted regression values.</p>

        <details class="quote">
          <summary>Source code in <code>giants/config.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">staged_predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Return staged predictions for X.</span>

<span class="sd">    The predicted regression value of an input sample is computed</span>
<span class="sd">    as the weighted median prediction of the regressors in the ensemble.</span>

<span class="sd">    This generator method yields the ensemble prediction after each</span>
<span class="sd">    iteration of boosting and therefore allows monitoring, such as to</span>
<span class="sd">    determine the prediction on a test set after each boost.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">        The training input samples.</span>

<span class="sd">    Yields</span>
<span class="sd">    -------</span>
<span class="sd">    y : generator of ndarray of shape (n_samples,)</span>
<span class="sd">        The predicted regression values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
    <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_check_X</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">,</span> <span class="mi">1</span><span class="p">):</span>
        <span class="k">yield</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_median_predict</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">limit</span><span class="o">=</span><span class="n">i</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h3 class="doc doc-heading" id="giants.config.ModelEstimatorConfig.DecisionTreeClassifier">
        <code>
DecisionTreeClassifier            (<span title="sklearn.base.ClassifierMixin">ClassifierMixin</span>, <span title="sklearn.tree._classes.BaseDecisionTree">BaseDecisionTree</span>)
        </code>



<a class="headerlink" href="#giants.config.ModelEstimatorConfig.DecisionTreeClassifier" title="Permanent link">&para;</a></h3>

    <div class="doc doc-contents ">

      <p>A decision tree classifier.</p>
<p>Read more in the :ref:<code>User Guide &lt;tree&gt;</code>.</p>
<h5 id="giants.config.ModelEstimatorConfig.DecisionTreeClassifier--parameters">Parameters<a class="headerlink" href="#giants.config.ModelEstimatorConfig.DecisionTreeClassifier--parameters" title="Permanent link">&para;</a></h5>
<p>criterion : {"gini", "entropy"}, default="gini"
    The function to measure the quality of a split. Supported criteria are
    "gini" for the Gini impurity and "entropy" for the information gain.</p>
<p>splitter : {"best", "random"}, default="best"
    The strategy used to choose the split at each node. Supported
    strategies are "best" to choose the best split and "random" to choose
    the best random split.</p>
<p>max_depth : int, default=None
    The maximum depth of the tree. If None, then nodes are expanded until
    all leaves are pure or until all leaves contain less than
    min_samples_split samples.</p>
<p>min_samples_split : int or float, default=2
    The minimum number of samples required to split an internal node:</p>
<div class="codehilite"><pre><span></span><code>- If int, then consider `min_samples_split` as the minimum number.
- If float, then `min_samples_split` is a fraction and
  `ceil(min_samples_split * n_samples)` are the minimum
  number of samples for each split.

.. versionchanged:: 0.18
   Added float values for fractions.
</code></pre></div>

<p>min_samples_leaf : int or float, default=1
    The minimum number of samples required to be at a leaf node.
    A split point at any depth will only be considered if it leaves at
    least <code>min_samples_leaf</code> training samples in each of the left and
    right branches.  This may have the effect of smoothing the model,
    especially in regression.</p>
<div class="codehilite"><pre><span></span><code>- If int, then consider `min_samples_leaf` as the minimum number.
- If float, then `min_samples_leaf` is a fraction and
  `ceil(min_samples_leaf * n_samples)` are the minimum
  number of samples for each node.

.. versionchanged:: 0.18
   Added float values for fractions.
</code></pre></div>

<p>min_weight_fraction_leaf : float, default=0.0
    The minimum weighted fraction of the sum total of weights (of all
    the input samples) required to be at a leaf node. Samples have
    equal weight when sample_weight is not provided.</p>
<p>max_features : int, float or {"auto", "sqrt", "log2"}, default=None
    The number of features to consider when looking for the best split:</p>
<div class="codehilite"><pre><span></span><code>    - If int, then consider `max_features` features at each split.
    - If float, then `max_features` is a fraction and
      `int(max_features * n_features)` features are considered at each
      split.
    - If &quot;auto&quot;, then `max_features=sqrt(n_features)`.
    - If &quot;sqrt&quot;, then `max_features=sqrt(n_features)`.
    - If &quot;log2&quot;, then `max_features=log2(n_features)`.
    - If None, then `max_features=n_features`.

Note: the search for a split does not stop until at least one
valid partition of the node samples is found, even if it requires to
effectively inspect more than ``max_features`` features.
</code></pre></div>

<p>random_state : int, RandomState instance or None, default=None
    Controls the randomness of the estimator. The features are always
    randomly permuted at each split, even if <code>splitter</code> is set to
    <code>"best"</code>. When <code>max_features &lt; n_features</code>, the algorithm will
    select <code>max_features</code> at random at each split before finding the best
    split among them. But the best found split may vary across different
    runs, even if <code>max_features=n_features</code>. That is the case, if the
    improvement of the criterion is identical for several splits and one
    split has to be selected at random. To obtain a deterministic behaviour
    during fitting, <code>random_state</code> has to be fixed to an integer.
    See :term:<code>Glossary &lt;random_state&gt;</code> for details.</p>
<p>max_leaf_nodes : int, default=None
    Grow a tree with <code>max_leaf_nodes</code> in best-first fashion.
    Best nodes are defined as relative reduction in impurity.
    If None then unlimited number of leaf nodes.</p>
<p>min_impurity_decrease : float, default=0.0
    A node will be split if this split induces a decrease of the impurity
    greater than or equal to this value.</p>
<div class="codehilite"><pre><span></span><code>The weighted impurity decrease equation is the following::

    N_t / N * (impurity - N_t_R / N_t * right_impurity
                        - N_t_L / N_t * left_impurity)

where ``N`` is the total number of samples, ``N_t`` is the number of
samples at the current node, ``N_t_L`` is the number of samples in the
left child, and ``N_t_R`` is the number of samples in the right child.

``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,
if ``sample_weight`` is passed.

.. versionadded:: 0.19
</code></pre></div>

<p>class_weight : dict, list of dict or "balanced", default=None
    Weights associated with classes in the form <code>{class_label: weight}</code>.
    If None, all classes are supposed to have weight one. For
    multi-output problems, a list of dicts can be provided in the same
    order as the columns of y.</p>
<div class="codehilite"><pre><span></span><code>Note that for multioutput (including multilabel) weights should be
defined for each class of every column in its own dict. For example,
for four-class multilabel classification weights should be
[{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of
[{1:1}, {2:5}, {3:1}, {4:1}].

The &quot;balanced&quot; mode uses the values of y to automatically adjust
weights inversely proportional to class frequencies in the input data
as ``n_samples / (n_classes * np.bincount(y))``

For multi-output, the weights of each column of y will be multiplied.

Note that these weights will be multiplied with sample_weight (passed
through the fit method) if sample_weight is specified.
</code></pre></div>

<p>ccp_alpha : non-negative float, default=0.0
    Complexity parameter used for Minimal Cost-Complexity Pruning. The
    subtree with the largest cost complexity that is smaller than
    <code>ccp_alpha</code> will be chosen. By default, no pruning is performed. See
    :ref:<code>minimal_cost_complexity_pruning</code> for details.</p>
<div class="codehilite"><pre><span></span><code>.. versionadded:: 0.22
</code></pre></div>

<h5 id="giants.config.ModelEstimatorConfig.DecisionTreeClassifier--attributes">Attributes<a class="headerlink" href="#giants.config.ModelEstimatorConfig.DecisionTreeClassifier--attributes" title="Permanent link">&para;</a></h5>
<p>classes_ : ndarray of shape (n_classes,) or list of ndarray
    The classes labels (single output problem),
    or a list of arrays of class labels (multi-output problem).</p>
<p>feature_importances_ : ndarray of shape (n_features,)
    The impurity-based feature importances.
    The higher, the more important the feature.
    The importance of a feature is computed as the (normalized)
    total reduction of the criterion brought by that feature.  It is also
    known as the Gini importance [4]_.</p>
<div class="codehilite"><pre><span></span><code>Warning: impurity-based feature importances can be misleading for
high cardinality features (many unique values). See
:func:`sklearn.inspection.permutation_importance` as an alternative.
</code></pre></div>

<p>max_features_ : int
    The inferred value of max_features.</p>
<p>n_classes_ : int or list of int
    The number of classes (for single output problems),
    or a list containing the number of classes for each
    output (for multi-output problems).</p>
<p>n_features_ : int
    The number of features when <code>fit</code> is performed.</p>
<div class="codehilite"><pre><span></span><code>.. deprecated:: 1.0
   `n_features_` is deprecated in 1.0 and will be removed in
   1.2. Use `n_features_in_` instead.
</code></pre></div>

<p>n_features_in_ : int
    Number of features seen during :term:<code>fit</code>.</p>
<div class="codehilite"><pre><span></span><code>.. versionadded:: 0.24
</code></pre></div>

<p>feature_names_in_ : ndarray of shape (<code>n_features_in_</code>,)
    Names of features seen during :term:<code>fit</code>. Defined only when <code>X</code>
    has feature names that are all strings.</p>
<div class="codehilite"><pre><span></span><code>.. versionadded:: 1.0
</code></pre></div>

<p>n_outputs_ : int
    The number of outputs when <code>fit</code> is performed.</p>
<p>tree_ : Tree instance
    The underlying Tree object. Please refer to
    <code>help(sklearn.tree._tree.Tree)</code> for attributes of Tree object and
    :ref:<code>sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py</code>
    for basic usage of these attributes.</p>
<h5 id="giants.config.ModelEstimatorConfig.DecisionTreeClassifier--see-also">See Also<a class="headerlink" href="#giants.config.ModelEstimatorConfig.DecisionTreeClassifier--see-also" title="Permanent link">&para;</a></h5>
<p>DecisionTreeRegressor : A decision tree regressor.</p>
<h5 id="giants.config.ModelEstimatorConfig.DecisionTreeClassifier--notes">Notes<a class="headerlink" href="#giants.config.ModelEstimatorConfig.DecisionTreeClassifier--notes" title="Permanent link">&para;</a></h5>
<p>The default values for the parameters controlling the size of the trees
(e.g. <code>max_depth</code>, <code>min_samples_leaf</code>, etc.) lead to fully grown and
unpruned trees which can potentially be very large on some data sets. To
reduce memory consumption, the complexity and size of the trees should be
controlled by setting those parameter values.</p>
<p>The :meth:<code>predict</code> method operates using the :func:<code>numpy.argmax</code>
function on the outputs of :meth:<code>predict_proba</code>. This means that in
case the highest predicted probabilities are tied, the classifier will
predict the tied class with the lowest index in :term:<code>classes_</code>.</p>
<h5 id="giants.config.ModelEstimatorConfig.DecisionTreeClassifier--references">References<a class="headerlink" href="#giants.config.ModelEstimatorConfig.DecisionTreeClassifier--references" title="Permanent link">&para;</a></h5>
<p>.. [1] <a href="https://en.wikipedia.org/wiki/Decision_tree_learning">en.wikipedia.org/wiki/Decision_tree_learning</a></p>
<p>.. [2] L. Breiman, J. Friedman, R. Olshen, and C. Stone, "Classification
       and Regression Trees", Wadsworth, Belmont, CA, 1984.</p>
<p>.. [3] T. Hastie, R. Tibshirani and J. Friedman. "Elements of Statistical
       Learning", Springer, 2009.</p>
<p>.. [4] L. Breiman, and A. Cutler, "Random Forests",
       <a href="https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm">www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm</a></p>
<h5 id="giants.config.ModelEstimatorConfig.DecisionTreeClassifier--examples">Examples<a class="headerlink" href="#giants.config.ModelEstimatorConfig.DecisionTreeClassifier--examples" title="Permanent link">&para;</a></h5>
<blockquote>
<blockquote>
<blockquote>
<p>from sklearn.datasets import load_iris
from sklearn.model_selection import cross_val_score
from sklearn.tree import DecisionTreeClassifier
clf = DecisionTreeClassifier(random_state=0)
iris = load_iris()
cross_val_score(clf, iris.data, iris.target, cv=10)
...                             # doctest: +SKIP
...
array([ 1.     ,  0.93...,  0.86...,  0.93...,  0.93...,
        0.93...,  0.93...,  1.     ,  0.93...,  1.      ])</p>
</blockquote>
</blockquote>
</blockquote>




  <div class="doc doc-children">






  <div class="doc doc-object doc-attribute">



<h4 class="doc doc-heading" id="giants.config.ModelEstimatorConfig.DecisionTreeClassifier.n_features_">
<code class="codehilite language-python"><span class="n">n_features_</span><span class="p">:</span> <span class="kc">None</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-readonly"><code>readonly</code></small>
  </span>

<a class="headerlink" href="#giants.config.ModelEstimatorConfig.DecisionTreeClassifier.n_features_" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>DEPRECATED: The attribute <code>n_features_</code> is deprecated in 1.0 and will be removed in 1.2. Use <code>n_features_in_</code> instead.</p>
    </div>

  </div>







  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="giants.config.ModelEstimatorConfig.DecisionTreeClassifier.fit">
<code class="codehilite language-python"><span class="n">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">check_input</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">X_idx_sorted</span><span class="o">=</span><span class="s1">&#39;deprecated&#39;</span><span class="p">)</span></code>


<a class="headerlink" href="#giants.config.ModelEstimatorConfig.DecisionTreeClassifier.fit" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Build a decision tree classifier from the training set (X, y).</p>
<h6 id="giants.config.ModelEstimatorConfig.DecisionTreeClassifier.fit--parameters">Parameters<a class="headerlink" href="#giants.config.ModelEstimatorConfig.DecisionTreeClassifier.fit--parameters" title="Permanent link">&para;</a></h6>
<p>X : {array-like, sparse matrix} of shape (n_samples, n_features)
    The training input samples. Internally, it will be converted to
    <code>dtype=np.float32</code> and if a sparse matrix is provided
    to a sparse <code>csc_matrix</code>.</p>
<p>y : array-like of shape (n_samples,) or (n_samples, n_outputs)
    The target values (class labels) as integers or strings.</p>
<p>sample_weight : array-like of shape (n_samples,), default=None
    Sample weights. If None, then samples are equally weighted. Splits
    that would create child nodes with net zero or negative weight are
    ignored while searching for a split in each node. Splits are also
    ignored if they would result in any single class carrying a
    negative weight in either child node.</p>
<p>check_input : bool, default=True
    Allow to bypass several input checking.
    Don't use this parameter unless you know what you do.</p>
<p>X_idx_sorted : deprecated, default="deprecated"
    This parameter is deprecated and has no effect.
    It will be removed in 1.1 (renaming of 0.26).</p>
<div class="codehilite"><pre><span></span><code>.. deprecated:: 0.24
</code></pre></div>

<h6 id="giants.config.ModelEstimatorConfig.DecisionTreeClassifier.fit--returns">Returns<a class="headerlink" href="#giants.config.ModelEstimatorConfig.DecisionTreeClassifier.fit--returns" title="Permanent link">&para;</a></h6>
<p>self : DecisionTreeClassifier
    Fitted estimator.</p>

        <details class="quote">
          <summary>Source code in <code>giants/config.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">fit</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">check_input</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">X_idx_sorted</span><span class="o">=</span><span class="s2">&quot;deprecated&quot;</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Build a decision tree classifier from the training set (X, y).</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">        The training input samples. Internally, it will be converted to</span>
<span class="sd">        ``dtype=np.float32`` and if a sparse matrix is provided</span>
<span class="sd">        to a sparse ``csc_matrix``.</span>

<span class="sd">    y : array-like of shape (n_samples,) or (n_samples, n_outputs)</span>
<span class="sd">        The target values (class labels) as integers or strings.</span>

<span class="sd">    sample_weight : array-like of shape (n_samples,), default=None</span>
<span class="sd">        Sample weights. If None, then samples are equally weighted. Splits</span>
<span class="sd">        that would create child nodes with net zero or negative weight are</span>
<span class="sd">        ignored while searching for a split in each node. Splits are also</span>
<span class="sd">        ignored if they would result in any single class carrying a</span>
<span class="sd">        negative weight in either child node.</span>

<span class="sd">    check_input : bool, default=True</span>
<span class="sd">        Allow to bypass several input checking.</span>
<span class="sd">        Don&#39;t use this parameter unless you know what you do.</span>

<span class="sd">    X_idx_sorted : deprecated, default=&quot;deprecated&quot;</span>
<span class="sd">        This parameter is deprecated and has no effect.</span>
<span class="sd">        It will be removed in 1.1 (renaming of 0.26).</span>

<span class="sd">        .. deprecated:: 0.24</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    self : DecisionTreeClassifier</span>
<span class="sd">        Fitted estimator.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
        <span class="n">X</span><span class="p">,</span>
        <span class="n">y</span><span class="p">,</span>
        <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">,</span>
        <span class="n">check_input</span><span class="o">=</span><span class="n">check_input</span><span class="p">,</span>
        <span class="n">X_idx_sorted</span><span class="o">=</span><span class="n">X_idx_sorted</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="giants.config.ModelEstimatorConfig.DecisionTreeClassifier.predict_log_proba">
<code class="codehilite language-python"><span class="n">predict_log_proba</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span></code>


<a class="headerlink" href="#giants.config.ModelEstimatorConfig.DecisionTreeClassifier.predict_log_proba" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Predict class log-probabilities of the input samples X.</p>
<h6 id="giants.config.ModelEstimatorConfig.DecisionTreeClassifier.predict_log_proba--parameters">Parameters<a class="headerlink" href="#giants.config.ModelEstimatorConfig.DecisionTreeClassifier.predict_log_proba--parameters" title="Permanent link">&para;</a></h6>
<p>X : {array-like, sparse matrix} of shape (n_samples, n_features)
    The input samples. Internally, it will be converted to
    <code>dtype=np.float32</code> and if a sparse matrix is provided
    to a sparse <code>csr_matrix</code>.</p>
<h6 id="giants.config.ModelEstimatorConfig.DecisionTreeClassifier.predict_log_proba--returns">Returns<a class="headerlink" href="#giants.config.ModelEstimatorConfig.DecisionTreeClassifier.predict_log_proba--returns" title="Permanent link">&para;</a></h6>
<p>proba : ndarray of shape (n_samples, n_classes) or list of n_outputs             such arrays if n_outputs &gt; 1
    The class log-probabilities of the input samples. The order of the
    classes corresponds to that in the attribute :term:<code>classes_</code>.</p>

        <details class="quote">
          <summary>Source code in <code>giants/config.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">predict_log_proba</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Predict class log-probabilities of the input samples X.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">        The input samples. Internally, it will be converted to</span>
<span class="sd">        ``dtype=np.float32`` and if a sparse matrix is provided</span>
<span class="sd">        to a sparse ``csr_matrix``.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    proba : ndarray of shape (n_samples, n_classes) or list of n_outputs \</span>
<span class="sd">        such arrays if n_outputs &gt; 1</span>
<span class="sd">        The class log-probabilities of the input samples. The order of the</span>
<span class="sd">        classes corresponds to that in the attribute :term:`classes_`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">proba</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_outputs_</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">proba</span><span class="p">)</span>

    <span class="k">else</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_outputs_</span><span class="p">):</span>
            <span class="n">proba</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">proba</span><span class="p">[</span><span class="n">k</span><span class="p">])</span>

        <span class="k">return</span> <span class="n">proba</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="giants.config.ModelEstimatorConfig.DecisionTreeClassifier.predict_proba">
<code class="codehilite language-python"><span class="n">predict_proba</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">check_input</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>


<a class="headerlink" href="#giants.config.ModelEstimatorConfig.DecisionTreeClassifier.predict_proba" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Predict class probabilities of the input samples X.</p>
<p>The predicted class probability is the fraction of samples of the same
class in a leaf.</p>
<h6 id="giants.config.ModelEstimatorConfig.DecisionTreeClassifier.predict_proba--parameters">Parameters<a class="headerlink" href="#giants.config.ModelEstimatorConfig.DecisionTreeClassifier.predict_proba--parameters" title="Permanent link">&para;</a></h6>
<p>X : {array-like, sparse matrix} of shape (n_samples, n_features)
    The input samples. Internally, it will be converted to
    <code>dtype=np.float32</code> and if a sparse matrix is provided
    to a sparse <code>csr_matrix</code>.</p>
<p>check_input : bool, default=True
    Allow to bypass several input checking.
    Don't use this parameter unless you know what you do.</p>
<h6 id="giants.config.ModelEstimatorConfig.DecisionTreeClassifier.predict_proba--returns">Returns<a class="headerlink" href="#giants.config.ModelEstimatorConfig.DecisionTreeClassifier.predict_proba--returns" title="Permanent link">&para;</a></h6>
<p>proba : ndarray of shape (n_samples, n_classes) or list of n_outputs             such arrays if n_outputs &gt; 1
    The class probabilities of the input samples. The order of the
    classes corresponds to that in the attribute :term:<code>classes_</code>.</p>

        <details class="quote">
          <summary>Source code in <code>giants/config.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">predict_proba</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">check_input</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Predict class probabilities of the input samples X.</span>

<span class="sd">    The predicted class probability is the fraction of samples of the same</span>
<span class="sd">    class in a leaf.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">        The input samples. Internally, it will be converted to</span>
<span class="sd">        ``dtype=np.float32`` and if a sparse matrix is provided</span>
<span class="sd">        to a sparse ``csr_matrix``.</span>

<span class="sd">    check_input : bool, default=True</span>
<span class="sd">        Allow to bypass several input checking.</span>
<span class="sd">        Don&#39;t use this parameter unless you know what you do.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    proba : ndarray of shape (n_samples, n_classes) or list of n_outputs \</span>
<span class="sd">        such arrays if n_outputs &gt; 1</span>
<span class="sd">        The class probabilities of the input samples. The order of the</span>
<span class="sd">        classes corresponds to that in the attribute :term:`classes_`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
    <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_X_predict</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">check_input</span><span class="p">)</span>
    <span class="n">proba</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tree_</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_outputs_</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">proba</span> <span class="o">=</span> <span class="n">proba</span><span class="p">[:,</span> <span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_classes_</span><span class="p">]</span>
        <span class="n">normalizer</span> <span class="o">=</span> <span class="n">proba</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>
        <span class="n">normalizer</span><span class="p">[</span><span class="n">normalizer</span> <span class="o">==</span> <span class="mf">0.0</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span>
        <span class="n">proba</span> <span class="o">/=</span> <span class="n">normalizer</span>

        <span class="k">return</span> <span class="n">proba</span>

    <span class="k">else</span><span class="p">:</span>
        <span class="n">all_proba</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_outputs_</span><span class="p">):</span>
            <span class="n">proba_k</span> <span class="o">=</span> <span class="n">proba</span><span class="p">[:,</span> <span class="n">k</span><span class="p">,</span> <span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_classes_</span><span class="p">[</span><span class="n">k</span><span class="p">]]</span>
            <span class="n">normalizer</span> <span class="o">=</span> <span class="n">proba_k</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>
            <span class="n">normalizer</span><span class="p">[</span><span class="n">normalizer</span> <span class="o">==</span> <span class="mf">0.0</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span>
            <span class="n">proba_k</span> <span class="o">/=</span> <span class="n">normalizer</span>
            <span class="n">all_proba</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">proba_k</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">all_proba</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h3 class="doc doc-heading" id="giants.config.ModelEstimatorConfig.GradientBoostingClassifier">
        <code>
GradientBoostingClassifier            (<span title="sklearn.base.ClassifierMixin">ClassifierMixin</span>, <span title="sklearn.ensemble._gb.BaseGradientBoosting">BaseGradientBoosting</span>)
        </code>



<a class="headerlink" href="#giants.config.ModelEstimatorConfig.GradientBoostingClassifier" title="Permanent link">&para;</a></h3>

    <div class="doc doc-contents ">

      <p>Gradient Boosting for classification.</p>
<p>GB builds an additive model in a
forward stage-wise fashion; it allows for the optimization of
arbitrary differentiable loss functions. In each stage <code>n_classes_</code>
regression trees are fit on the negative gradient of the
binomial or multinomial deviance loss function. Binary classification
is a special case where only a single regression tree is induced.</p>
<p>Read more in the :ref:<code>User Guide &lt;gradient_boosting&gt;</code>.</p>
<h5 id="giants.config.ModelEstimatorConfig.GradientBoostingClassifier--parameters">Parameters<a class="headerlink" href="#giants.config.ModelEstimatorConfig.GradientBoostingClassifier--parameters" title="Permanent link">&para;</a></h5>
<p>loss : {'deviance', 'exponential'}, default='deviance'
    The loss function to be optimized. 'deviance' refers to
    deviance (= logistic regression) for classification
    with probabilistic outputs. For loss 'exponential' gradient
    boosting recovers the AdaBoost algorithm.</p>
<p>learning_rate : float, default=0.1
    Learning rate shrinks the contribution of each tree by <code>learning_rate</code>.
    There is a trade-off between learning_rate and n_estimators.</p>
<p>n_estimators : int, default=100
    The number of boosting stages to perform. Gradient boosting
    is fairly robust to over-fitting so a large number usually
    results in better performance.</p>
<p>subsample : float, default=1.0
    The fraction of samples to be used for fitting the individual base
    learners. If smaller than 1.0 this results in Stochastic Gradient
    Boosting. <code>subsample</code> interacts with the parameter <code>n_estimators</code>.
    Choosing <code>subsample &lt; 1.0</code> leads to a reduction of variance
    and an increase in bias.</p>
<p>criterion : {'friedman_mse', 'squared_error', 'mse', 'mae'},             default='friedman_mse'
    The function to measure the quality of a split. Supported criteria
    are 'friedman_mse' for the mean squared error with improvement
    score by Friedman, 'squared_error' for mean squared error, and 'mae'
    for the mean absolute error. The default value of 'friedman_mse' is
    generally the best as it can provide a better approximation in some
    cases.</p>
<div class="codehilite"><pre><span></span><code>.. versionadded:: 0.18

.. deprecated:: 0.24
    `criterion=&#39;mae&#39;` is deprecated and will be removed in version
    1.1 (renaming of 0.26). Use `criterion=&#39;friedman_mse&#39;` or
    `&#39;squared_error&#39;` instead, as trees should use a squared error
    criterion in Gradient Boosting.

.. deprecated:: 1.0
    Criterion &#39;mse&#39; was deprecated in v1.0 and will be removed in
    version 1.2. Use `criterion=&#39;squared_error&#39;` which is equivalent.
</code></pre></div>

<p>min_samples_split : int or float, default=2
    The minimum number of samples required to split an internal node:</p>
<div class="codehilite"><pre><span></span><code>- If int, then consider `min_samples_split` as the minimum number.
- If float, then `min_samples_split` is a fraction and
  `ceil(min_samples_split * n_samples)` are the minimum
  number of samples for each split.

.. versionchanged:: 0.18
   Added float values for fractions.
</code></pre></div>

<p>min_samples_leaf : int or float, default=1
    The minimum number of samples required to be at a leaf node.
    A split point at any depth will only be considered if it leaves at
    least <code>min_samples_leaf</code> training samples in each of the left and
    right branches.  This may have the effect of smoothing the model,
    especially in regression.</p>
<div class="codehilite"><pre><span></span><code>- If int, then consider `min_samples_leaf` as the minimum number.
- If float, then `min_samples_leaf` is a fraction and
  `ceil(min_samples_leaf * n_samples)` are the minimum
  number of samples for each node.

.. versionchanged:: 0.18
   Added float values for fractions.
</code></pre></div>

<p>min_weight_fraction_leaf : float, default=0.0
    The minimum weighted fraction of the sum total of weights (of all
    the input samples) required to be at a leaf node. Samples have
    equal weight when sample_weight is not provided.</p>
<p>max_depth : int, default=3
    The maximum depth of the individual regression estimators. The maximum
    depth limits the number of nodes in the tree. Tune this parameter
    for best performance; the best value depends on the interaction
    of the input variables.</p>
<p>min_impurity_decrease : float, default=0.0
    A node will be split if this split induces a decrease of the impurity
    greater than or equal to this value.</p>
<div class="codehilite"><pre><span></span><code>The weighted impurity decrease equation is the following::

    N_t / N * (impurity - N_t_R / N_t * right_impurity
                        - N_t_L / N_t * left_impurity)

where ``N`` is the total number of samples, ``N_t`` is the number of
samples at the current node, ``N_t_L`` is the number of samples in the
left child, and ``N_t_R`` is the number of samples in the right child.

``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,
if ``sample_weight`` is passed.

.. versionadded:: 0.19
</code></pre></div>

<p>init : estimator or 'zero', default=None
    An estimator object that is used to compute the initial predictions.
    <code>init</code> has to provide :meth:<code>fit</code> and :meth:<code>predict_proba</code>. If
    'zero', the initial raw predictions are set to zero. By default, a
    <code>DummyEstimator</code> predicting the classes priors is used.</p>
<p>random_state : int, RandomState instance or None, default=None
    Controls the random seed given to each Tree estimator at each
    boosting iteration.
    In addition, it controls the random permutation of the features at
    each split (see Notes for more details).
    It also controls the random splitting of the training data to obtain a
    validation set if <code>n_iter_no_change</code> is not None.
    Pass an int for reproducible output across multiple function calls.
    See :term:<code>Glossary &lt;random_state&gt;</code>.</p>
<p>max_features : {'auto', 'sqrt', 'log2'}, int or float, default=None
    The number of features to consider when looking for the best split:</p>
<div class="codehilite"><pre><span></span><code>- If int, then consider `max_features` features at each split.
- If float, then `max_features` is a fraction and
  `int(max_features * n_features)` features are considered at each
  split.
- If &#39;auto&#39;, then `max_features=sqrt(n_features)`.
- If &#39;sqrt&#39;, then `max_features=sqrt(n_features)`.
- If &#39;log2&#39;, then `max_features=log2(n_features)`.
- If None, then `max_features=n_features`.

Choosing `max_features &lt; n_features` leads to a reduction of variance
and an increase in bias.

Note: the search for a split does not stop until at least one
valid partition of the node samples is found, even if it requires to
effectively inspect more than ``max_features`` features.
</code></pre></div>

<p>verbose : int, default=0
    Enable verbose output. If 1 then it prints progress and performance
    once in a while (the more trees the lower the frequency). If greater
    than 1 then it prints progress and performance for every tree.</p>
<p>max_leaf_nodes : int, default=None
    Grow trees with <code>max_leaf_nodes</code> in best-first fashion.
    Best nodes are defined as relative reduction in impurity.
    If None then unlimited number of leaf nodes.</p>
<p>warm_start : bool, default=False
    When set to <code>True</code>, reuse the solution of the previous call to fit
    and add more estimators to the ensemble, otherwise, just erase the
    previous solution. See :term:<code>the Glossary &lt;warm_start&gt;</code>.</p>
<p>validation_fraction : float, default=0.1
    The proportion of training data to set aside as validation set for
    early stopping. Must be between 0 and 1.
    Only used if <code>n_iter_no_change</code> is set to an integer.</p>
<div class="codehilite"><pre><span></span><code>.. versionadded:: 0.20
</code></pre></div>

<p>n_iter_no_change : int, default=None
    <code>n_iter_no_change</code> is used to decide if early stopping will be used
    to terminate training when validation score is not improving. By
    default it is set to None to disable early stopping. If set to a
    number, it will set aside <code>validation_fraction</code> size of the training
    data as validation and terminate training when validation score is not
    improving in all of the previous <code>n_iter_no_change</code> numbers of
    iterations. The split is stratified.</p>
<div class="codehilite"><pre><span></span><code>.. versionadded:: 0.20
</code></pre></div>

<p>tol : float, default=1e-4
    Tolerance for the early stopping. When the loss is not improving
    by at least tol for <code>n_iter_no_change</code> iterations (if set to a
    number), the training stops.</p>
<div class="codehilite"><pre><span></span><code>.. versionadded:: 0.20
</code></pre></div>

<p>ccp_alpha : non-negative float, default=0.0
    Complexity parameter used for Minimal Cost-Complexity Pruning. The
    subtree with the largest cost complexity that is smaller than
    <code>ccp_alpha</code> will be chosen. By default, no pruning is performed. See
    :ref:<code>minimal_cost_complexity_pruning</code> for details.</p>
<div class="codehilite"><pre><span></span><code>.. versionadded:: 0.22
</code></pre></div>

<h5 id="giants.config.ModelEstimatorConfig.GradientBoostingClassifier--attributes">Attributes<a class="headerlink" href="#giants.config.ModelEstimatorConfig.GradientBoostingClassifier--attributes" title="Permanent link">&para;</a></h5>
<p>n_estimators_ : int
    The number of estimators as selected by early stopping (if
    <code>n_iter_no_change</code> is specified). Otherwise it is set to
    <code>n_estimators</code>.</p>
<div class="codehilite"><pre><span></span><code>.. versionadded:: 0.20
</code></pre></div>

<p>feature_importances_ : ndarray of shape (n_features,)
    The impurity-based feature importances.
    The higher, the more important the feature.
    The importance of a feature is computed as the (normalized)
    total reduction of the criterion brought by that feature.  It is also
    known as the Gini importance.</p>
<div class="codehilite"><pre><span></span><code>Warning: impurity-based feature importances can be misleading for
high cardinality features (many unique values). See
:func:`sklearn.inspection.permutation_importance` as an alternative.
</code></pre></div>

<p>oob_improvement_ : ndarray of shape (n_estimators,)
    The improvement in loss (= deviance) on the out-of-bag samples
    relative to the previous iteration.
    <code>oob_improvement_[0]</code> is the improvement in
    loss of the first stage over the <code>init</code> estimator.
    Only available if <code>subsample &lt; 1.0</code></p>
<p>train_score_ : ndarray of shape (n_estimators,)
    The i-th score <code>train_score_[i]</code> is the deviance (= loss) of the
    model at iteration <code>i</code> on the in-bag sample.
    If <code>subsample == 1</code> this is the deviance on the training data.</p>
<p>loss_ : LossFunction
    The concrete <code>LossFunction</code> object.</p>
<p>init_ : estimator
    The estimator that provides the initial predictions.
    Set via the <code>init</code> argument or <code>loss.init_estimator</code>.</p>
<p>estimators_ : ndarray of DecisionTreeRegressor of             shape (n_estimators, <code>loss_.K</code>)
    The collection of fitted sub-estimators. <code>loss_.K</code> is 1 for binary
    classification, otherwise n_classes.</p>
<p>classes_ : ndarray of shape (n_classes,)
    The classes labels.</p>
<p>n_features_ : int
    The number of data features.</p>
<div class="codehilite"><pre><span></span><code>.. deprecated:: 1.0
    Attribute `n_features_` was deprecated in version 1.0 and will be
    removed in 1.2. Use `n_features_in_` instead.
</code></pre></div>

<p>n_features_in_ : int
    Number of features seen during :term:<code>fit</code>.</p>
<div class="codehilite"><pre><span></span><code>.. versionadded:: 0.24
</code></pre></div>

<p>feature_names_in_ : ndarray of shape (<code>n_features_in_</code>,)
    Names of features seen during :term:<code>fit</code>. Defined only when <code>X</code>
    has feature names that are all strings.</p>
<div class="codehilite"><pre><span></span><code>.. versionadded:: 1.0
</code></pre></div>

<p>n_classes_ : int
    The number of classes.</p>
<p>max_features_ : int
    The inferred value of max_features.</p>
<h5 id="giants.config.ModelEstimatorConfig.GradientBoostingClassifier--see-also">See Also<a class="headerlink" href="#giants.config.ModelEstimatorConfig.GradientBoostingClassifier--see-also" title="Permanent link">&para;</a></h5>
<p>HistGradientBoostingClassifier : Histogram-based Gradient Boosting
    Classification Tree.
sklearn.tree.DecisionTreeClassifier : A decision tree classifier.
RandomForestClassifier : A meta-estimator that fits a number of decision
    tree classifiers on various sub-samples of the dataset and uses
    averaging to improve the predictive accuracy and control over-fitting.
AdaBoostClassifier : A meta-estimator that begins by fitting a classifier
    on the original dataset and then fits additional copies of the
    classifier on the same dataset where the weights of incorrectly
    classified instances are adjusted such that subsequent classifiers
    focus more on difficult cases.</p>
<h5 id="giants.config.ModelEstimatorConfig.GradientBoostingClassifier--notes">Notes<a class="headerlink" href="#giants.config.ModelEstimatorConfig.GradientBoostingClassifier--notes" title="Permanent link">&para;</a></h5>
<p>The features are always randomly permuted at each split. Therefore,
the best found split may vary, even with the same training data and
<code>max_features=n_features</code>, if the improvement of the criterion is
identical for several splits enumerated during the search of the best
split. To obtain a deterministic behaviour during fitting,
<code>random_state</code> has to be fixed.</p>
<h5 id="giants.config.ModelEstimatorConfig.GradientBoostingClassifier--references">References<a class="headerlink" href="#giants.config.ModelEstimatorConfig.GradientBoostingClassifier--references" title="Permanent link">&para;</a></h5>
<p>J. Friedman, Greedy Function Approximation: A Gradient Boosting
Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.</p>
<p>J. Friedman, Stochastic Gradient Boosting, 1999</p>
<p>T. Hastie, R. Tibshirani and J. Friedman.
Elements of Statistical Learning Ed. 2, Springer, 2009.</p>
<h5 id="giants.config.ModelEstimatorConfig.GradientBoostingClassifier--examples">Examples<a class="headerlink" href="#giants.config.ModelEstimatorConfig.GradientBoostingClassifier--examples" title="Permanent link">&para;</a></h5>
<p>The following example shows how to fit a gradient boosting classifier with
100 decision stumps as weak learners.</p>
<blockquote>
<blockquote>
<blockquote>
<p>from sklearn.datasets import make_hastie_10_2
from sklearn.ensemble import GradientBoostingClassifier</p>
<p>X, y = make_hastie_10_2(random_state=0)
X_train, X_test = X[:2000], X[2000:]
y_train, y_test = y[:2000], y[2000:]</p>
<p>clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,
...     max_depth=1, random_state=0).fit(X_train, y_train)
clf.score(X_test, y_test)
0.913...</p>
</blockquote>
</blockquote>
</blockquote>




  <div class="doc doc-children">










  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="giants.config.ModelEstimatorConfig.GradientBoostingClassifier.decision_function">
<code class="codehilite language-python"><span class="n">decision_function</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span></code>


<a class="headerlink" href="#giants.config.ModelEstimatorConfig.GradientBoostingClassifier.decision_function" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Compute the decision function of <code>X</code>.</p>
<h6 id="giants.config.ModelEstimatorConfig.GradientBoostingClassifier.decision_function--parameters">Parameters<a class="headerlink" href="#giants.config.ModelEstimatorConfig.GradientBoostingClassifier.decision_function--parameters" title="Permanent link">&para;</a></h6>
<p>X : {array-like, sparse matrix} of shape (n_samples, n_features)
    The input samples. Internally, it will be converted to
    <code>dtype=np.float32</code> and if a sparse matrix is provided
    to a sparse <code>csr_matrix</code>.</p>
<h6 id="giants.config.ModelEstimatorConfig.GradientBoostingClassifier.decision_function--returns">Returns<a class="headerlink" href="#giants.config.ModelEstimatorConfig.GradientBoostingClassifier.decision_function--returns" title="Permanent link">&para;</a></h6>
<p>score : ndarray of shape (n_samples, n_classes) or (n_samples,)
    The decision function of the input samples, which corresponds to
    the raw values predicted from the trees of the ensemble . The
    order of the classes corresponds to that in the attribute
    :term:<code>classes_</code>. Regression and binary classification produce an
    array of shape (n_samples,).</p>

        <details class="quote">
          <summary>Source code in <code>giants/config.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">decision_function</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Compute the decision function of ``X``.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">        The input samples. Internally, it will be converted to</span>
<span class="sd">        ``dtype=np.float32`` and if a sparse matrix is provided</span>
<span class="sd">        to a sparse ``csr_matrix``.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    score : ndarray of shape (n_samples, n_classes) or (n_samples,)</span>
<span class="sd">        The decision function of the input samples, which corresponds to</span>
<span class="sd">        the raw values predicted from the trees of the ensemble . The</span>
<span class="sd">        order of the classes corresponds to that in the attribute</span>
<span class="sd">        :term:`classes_`. Regression and binary classification produce an</span>
<span class="sd">        array of shape (n_samples,).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_data</span><span class="p">(</span>
        <span class="n">X</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">DTYPE</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="s2">&quot;C&quot;</span><span class="p">,</span> <span class="n">accept_sparse</span><span class="o">=</span><span class="s2">&quot;csr&quot;</span><span class="p">,</span> <span class="n">reset</span><span class="o">=</span><span class="kc">False</span>
    <span class="p">)</span>
    <span class="n">raw_predictions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_raw_predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">raw_predictions</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">raw_predictions</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">raw_predictions</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="giants.config.ModelEstimatorConfig.GradientBoostingClassifier.predict">
<code class="codehilite language-python"><span class="n">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span></code>


<a class="headerlink" href="#giants.config.ModelEstimatorConfig.GradientBoostingClassifier.predict" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Predict class for X.</p>
<h6 id="giants.config.ModelEstimatorConfig.GradientBoostingClassifier.predict--parameters">Parameters<a class="headerlink" href="#giants.config.ModelEstimatorConfig.GradientBoostingClassifier.predict--parameters" title="Permanent link">&para;</a></h6>
<p>X : {array-like, sparse matrix} of shape (n_samples, n_features)
    The input samples. Internally, it will be converted to
    <code>dtype=np.float32</code> and if a sparse matrix is provided
    to a sparse <code>csr_matrix</code>.</p>
<h6 id="giants.config.ModelEstimatorConfig.GradientBoostingClassifier.predict--returns">Returns<a class="headerlink" href="#giants.config.ModelEstimatorConfig.GradientBoostingClassifier.predict--returns" title="Permanent link">&para;</a></h6>
<p>y : ndarray of shape (n_samples,)
    The predicted values.</p>

        <details class="quote">
          <summary>Source code in <code>giants/config.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Predict class for X.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">        The input samples. Internally, it will be converted to</span>
<span class="sd">        ``dtype=np.float32`` and if a sparse matrix is provided</span>
<span class="sd">        to a sparse ``csr_matrix``.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    y : ndarray of shape (n_samples,)</span>
<span class="sd">        The predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">raw_predictions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decision_function</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">encoded_labels</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_</span><span class="o">.</span><span class="n">_raw_prediction_to_decision</span><span class="p">(</span><span class="n">raw_predictions</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">encoded_labels</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="giants.config.ModelEstimatorConfig.GradientBoostingClassifier.predict_log_proba">
<code class="codehilite language-python"><span class="n">predict_log_proba</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span></code>


<a class="headerlink" href="#giants.config.ModelEstimatorConfig.GradientBoostingClassifier.predict_log_proba" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Predict class log-probabilities for X.</p>
<h6 id="giants.config.ModelEstimatorConfig.GradientBoostingClassifier.predict_log_proba--parameters">Parameters<a class="headerlink" href="#giants.config.ModelEstimatorConfig.GradientBoostingClassifier.predict_log_proba--parameters" title="Permanent link">&para;</a></h6>
<p>X : {array-like, sparse matrix} of shape (n_samples, n_features)
    The input samples. Internally, it will be converted to
    <code>dtype=np.float32</code> and if a sparse matrix is provided
    to a sparse <code>csr_matrix</code>.</p>
<h6 id="giants.config.ModelEstimatorConfig.GradientBoostingClassifier.predict_log_proba--returns">Returns<a class="headerlink" href="#giants.config.ModelEstimatorConfig.GradientBoostingClassifier.predict_log_proba--returns" title="Permanent link">&para;</a></h6>
<p>p : ndarray of shape (n_samples, n_classes)
    The class log-probabilities of the input samples. The order of the
    classes corresponds to that in the attribute :term:<code>classes_</code>.</p>
<h6 id="giants.config.ModelEstimatorConfig.GradientBoostingClassifier.predict_log_proba--raises">Raises<a class="headerlink" href="#giants.config.ModelEstimatorConfig.GradientBoostingClassifier.predict_log_proba--raises" title="Permanent link">&para;</a></h6>
<p>AttributeError
    If the <code>loss</code> does not support probabilities.</p>

        <details class="quote">
          <summary>Source code in <code>giants/config.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">predict_log_proba</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Predict class log-probabilities for X.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">        The input samples. Internally, it will be converted to</span>
<span class="sd">        ``dtype=np.float32`` and if a sparse matrix is provided</span>
<span class="sd">        to a sparse ``csr_matrix``.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    p : ndarray of shape (n_samples, n_classes)</span>
<span class="sd">        The class log-probabilities of the input samples. The order of the</span>
<span class="sd">        classes corresponds to that in the attribute :term:`classes_`.</span>

<span class="sd">    Raises</span>
<span class="sd">    ------</span>
<span class="sd">    AttributeError</span>
<span class="sd">        If the ``loss`` does not support probabilities.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">proba</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">proba</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="giants.config.ModelEstimatorConfig.GradientBoostingClassifier.predict_proba">
<code class="codehilite language-python"><span class="n">predict_proba</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span></code>


<a class="headerlink" href="#giants.config.ModelEstimatorConfig.GradientBoostingClassifier.predict_proba" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Predict class probabilities for X.</p>
<h6 id="giants.config.ModelEstimatorConfig.GradientBoostingClassifier.predict_proba--parameters">Parameters<a class="headerlink" href="#giants.config.ModelEstimatorConfig.GradientBoostingClassifier.predict_proba--parameters" title="Permanent link">&para;</a></h6>
<p>X : {array-like, sparse matrix} of shape (n_samples, n_features)
    The input samples. Internally, it will be converted to
    <code>dtype=np.float32</code> and if a sparse matrix is provided
    to a sparse <code>csr_matrix</code>.</p>
<h6 id="giants.config.ModelEstimatorConfig.GradientBoostingClassifier.predict_proba--returns">Returns<a class="headerlink" href="#giants.config.ModelEstimatorConfig.GradientBoostingClassifier.predict_proba--returns" title="Permanent link">&para;</a></h6>
<p>p : ndarray of shape (n_samples, n_classes)
    The class probabilities of the input samples. The order of the
    classes corresponds to that in the attribute :term:<code>classes_</code>.</p>
<h6 id="giants.config.ModelEstimatorConfig.GradientBoostingClassifier.predict_proba--raises">Raises<a class="headerlink" href="#giants.config.ModelEstimatorConfig.GradientBoostingClassifier.predict_proba--raises" title="Permanent link">&para;</a></h6>
<p>AttributeError
    If the <code>loss</code> does not support probabilities.</p>

        <details class="quote">
          <summary>Source code in <code>giants/config.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">predict_proba</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Predict class probabilities for X.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">        The input samples. Internally, it will be converted to</span>
<span class="sd">        ``dtype=np.float32`` and if a sparse matrix is provided</span>
<span class="sd">        to a sparse ``csr_matrix``.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    p : ndarray of shape (n_samples, n_classes)</span>
<span class="sd">        The class probabilities of the input samples. The order of the</span>
<span class="sd">        classes corresponds to that in the attribute :term:`classes_`.</span>

<span class="sd">    Raises</span>
<span class="sd">    ------</span>
<span class="sd">    AttributeError</span>
<span class="sd">        If the ``loss`` does not support probabilities.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">raw_predictions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decision_function</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_</span><span class="o">.</span><span class="n">_raw_prediction_to_proba</span><span class="p">(</span><span class="n">raw_predictions</span><span class="p">)</span>
    <span class="k">except</span> <span class="n">NotFittedError</span><span class="p">:</span>
        <span class="k">raise</span>
    <span class="k">except</span> <span class="ne">AttributeError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span>
            <span class="s2">&quot;loss=</span><span class="si">%r</span><span class="s2"> does not support predict_proba&quot;</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span>
        <span class="p">)</span> <span class="kn">from</span> <span class="nn">e</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="giants.config.ModelEstimatorConfig.GradientBoostingClassifier.staged_decision_function">
<code class="codehilite language-python"><span class="n">staged_decision_function</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span></code>


<a class="headerlink" href="#giants.config.ModelEstimatorConfig.GradientBoostingClassifier.staged_decision_function" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Compute decision function of <code>X</code> for each iteration.</p>
<p>This method allows monitoring (i.e. determine error on testing set)
after each stage.</p>
<h6 id="giants.config.ModelEstimatorConfig.GradientBoostingClassifier.staged_decision_function--parameters">Parameters<a class="headerlink" href="#giants.config.ModelEstimatorConfig.GradientBoostingClassifier.staged_decision_function--parameters" title="Permanent link">&para;</a></h6>
<p>X : {array-like, sparse matrix} of shape (n_samples, n_features)
    The input samples. Internally, it will be converted to
    <code>dtype=np.float32</code> and if a sparse matrix is provided
    to a sparse <code>csr_matrix</code>.</p>
<h6 id="giants.config.ModelEstimatorConfig.GradientBoostingClassifier.staged_decision_function--yields">Yields<a class="headerlink" href="#giants.config.ModelEstimatorConfig.GradientBoostingClassifier.staged_decision_function--yields" title="Permanent link">&para;</a></h6>
<p>score : generator of ndarray of shape (n_samples, k)
    The decision function of the input samples, which corresponds to
    the raw values predicted from the trees of the ensemble . The
    classes corresponds to that in the attribute :term:<code>classes_</code>.
    Regression and binary classification are special cases with
    <code>k == 1</code>, otherwise <code>k==n_classes</code>.</p>

        <details class="quote">
          <summary>Source code in <code>giants/config.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">staged_decision_function</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Compute decision function of ``X`` for each iteration.</span>

<span class="sd">    This method allows monitoring (i.e. determine error on testing set)</span>
<span class="sd">    after each stage.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">        The input samples. Internally, it will be converted to</span>
<span class="sd">        ``dtype=np.float32`` and if a sparse matrix is provided</span>
<span class="sd">        to a sparse ``csr_matrix``.</span>

<span class="sd">    Yields</span>
<span class="sd">    ------</span>
<span class="sd">    score : generator of ndarray of shape (n_samples, k)</span>
<span class="sd">        The decision function of the input samples, which corresponds to</span>
<span class="sd">        the raw values predicted from the trees of the ensemble . The</span>
<span class="sd">        classes corresponds to that in the attribute :term:`classes_`.</span>
<span class="sd">        Regression and binary classification are special cases with</span>
<span class="sd">        ``k == 1``, otherwise ``k==n_classes``.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">yield from</span> <span class="bp">self</span><span class="o">.</span><span class="n">_staged_raw_predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="giants.config.ModelEstimatorConfig.GradientBoostingClassifier.staged_predict">
<code class="codehilite language-python"><span class="n">staged_predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span></code>


<a class="headerlink" href="#giants.config.ModelEstimatorConfig.GradientBoostingClassifier.staged_predict" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Predict class at each stage for X.</p>
<p>This method allows monitoring (i.e. determine error on testing set)
after each stage.</p>
<h6 id="giants.config.ModelEstimatorConfig.GradientBoostingClassifier.staged_predict--parameters">Parameters<a class="headerlink" href="#giants.config.ModelEstimatorConfig.GradientBoostingClassifier.staged_predict--parameters" title="Permanent link">&para;</a></h6>
<p>X : {array-like, sparse matrix} of shape (n_samples, n_features)
    The input samples. Internally, it will be converted to
    <code>dtype=np.float32</code> and if a sparse matrix is provided
    to a sparse <code>csr_matrix</code>.</p>
<h6 id="giants.config.ModelEstimatorConfig.GradientBoostingClassifier.staged_predict--yields">Yields<a class="headerlink" href="#giants.config.ModelEstimatorConfig.GradientBoostingClassifier.staged_predict--yields" title="Permanent link">&para;</a></h6>
<p>y : generator of ndarray of shape (n_samples,)
    The predicted value of the input samples.</p>

        <details class="quote">
          <summary>Source code in <code>giants/config.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">staged_predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Predict class at each stage for X.</span>

<span class="sd">    This method allows monitoring (i.e. determine error on testing set)</span>
<span class="sd">    after each stage.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">        The input samples. Internally, it will be converted to</span>
<span class="sd">        ``dtype=np.float32`` and if a sparse matrix is provided</span>
<span class="sd">        to a sparse ``csr_matrix``.</span>

<span class="sd">    Yields</span>
<span class="sd">    -------</span>
<span class="sd">    y : generator of ndarray of shape (n_samples,)</span>
<span class="sd">        The predicted value of the input samples.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">raw_predictions</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_staged_raw_predict</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
        <span class="n">encoded_labels</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_</span><span class="o">.</span><span class="n">_raw_prediction_to_decision</span><span class="p">(</span><span class="n">raw_predictions</span><span class="p">)</span>
        <span class="k">yield</span> <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">encoded_labels</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="giants.config.ModelEstimatorConfig.GradientBoostingClassifier.staged_predict_proba">
<code class="codehilite language-python"><span class="n">staged_predict_proba</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span></code>


<a class="headerlink" href="#giants.config.ModelEstimatorConfig.GradientBoostingClassifier.staged_predict_proba" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Predict class probabilities at each stage for X.</p>
<p>This method allows monitoring (i.e. determine error on testing set)
after each stage.</p>
<h6 id="giants.config.ModelEstimatorConfig.GradientBoostingClassifier.staged_predict_proba--parameters">Parameters<a class="headerlink" href="#giants.config.ModelEstimatorConfig.GradientBoostingClassifier.staged_predict_proba--parameters" title="Permanent link">&para;</a></h6>
<p>X : {array-like, sparse matrix} of shape (n_samples, n_features)
    The input samples. Internally, it will be converted to
    <code>dtype=np.float32</code> and if a sparse matrix is provided
    to a sparse <code>csr_matrix</code>.</p>
<h6 id="giants.config.ModelEstimatorConfig.GradientBoostingClassifier.staged_predict_proba--yields">Yields<a class="headerlink" href="#giants.config.ModelEstimatorConfig.GradientBoostingClassifier.staged_predict_proba--yields" title="Permanent link">&para;</a></h6>
<p>y : generator of ndarray of shape (n_samples,)
    The predicted value of the input samples.</p>

        <details class="quote">
          <summary>Source code in <code>giants/config.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">staged_predict_proba</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Predict class probabilities at each stage for X.</span>

<span class="sd">    This method allows monitoring (i.e. determine error on testing set)</span>
<span class="sd">    after each stage.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">        The input samples. Internally, it will be converted to</span>
<span class="sd">        ``dtype=np.float32`` and if a sparse matrix is provided</span>
<span class="sd">        to a sparse ``csr_matrix``.</span>

<span class="sd">    Yields</span>
<span class="sd">    ------</span>
<span class="sd">    y : generator of ndarray of shape (n_samples,)</span>
<span class="sd">        The predicted value of the input samples.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">raw_predictions</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_staged_raw_predict</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
            <span class="k">yield</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_</span><span class="o">.</span><span class="n">_raw_prediction_to_proba</span><span class="p">(</span><span class="n">raw_predictions</span><span class="p">)</span>
    <span class="k">except</span> <span class="n">NotFittedError</span><span class="p">:</span>
        <span class="k">raise</span>
    <span class="k">except</span> <span class="ne">AttributeError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span>
            <span class="s2">&quot;loss=</span><span class="si">%r</span><span class="s2"> does not support predict_proba&quot;</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span>
        <span class="p">)</span> <span class="kn">from</span> <span class="nn">e</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h3 class="doc doc-heading" id="giants.config.ModelEstimatorConfig.GradientBoostingRegressor">
        <code>
GradientBoostingRegressor            (<span title="sklearn.base.RegressorMixin">RegressorMixin</span>, <span title="sklearn.ensemble._gb.BaseGradientBoosting">BaseGradientBoosting</span>)
        </code>



<a class="headerlink" href="#giants.config.ModelEstimatorConfig.GradientBoostingRegressor" title="Permanent link">&para;</a></h3>

    <div class="doc doc-contents ">

      <p>Gradient Boosting for regression.</p>
<p>GB builds an additive model in a forward stage-wise fashion;
it allows for the optimization of arbitrary differentiable loss functions.
In each stage a regression tree is fit on the negative gradient of the
given loss function.</p>
<p>Read more in the :ref:<code>User Guide &lt;gradient_boosting&gt;</code>.</p>
<h5 id="giants.config.ModelEstimatorConfig.GradientBoostingRegressor--parameters">Parameters<a class="headerlink" href="#giants.config.ModelEstimatorConfig.GradientBoostingRegressor--parameters" title="Permanent link">&para;</a></h5>
<p>loss : {'squared_error', 'absolute_error', 'huber', 'quantile'},             default='squared_error'
    Loss function to be optimized. 'squared_error' refers to the squared
    error for regression. 'absolute_error' refers to the absolute error of
    regression and is a robust loss function. 'huber' is a
    combination of the two. 'quantile' allows quantile regression (use
    <code>alpha</code> to specify the quantile).</p>
<div class="codehilite"><pre><span></span><code>.. deprecated:: 1.0
    The loss &#39;ls&#39; was deprecated in v1.0 and will be removed in
    version 1.2. Use `loss=&#39;squared_error&#39;` which is equivalent.

.. deprecated:: 1.0
    The loss &#39;lad&#39; was deprecated in v1.0 and will be removed in
    version 1.2. Use `loss=&#39;absolute_error&#39;` which is equivalent.
</code></pre></div>

<p>learning_rate : float, default=0.1
    Learning rate shrinks the contribution of each tree by <code>learning_rate</code>.
    There is a trade-off between learning_rate and n_estimators.</p>
<p>n_estimators : int, default=100
    The number of boosting stages to perform. Gradient boosting
    is fairly robust to over-fitting so a large number usually
    results in better performance.</p>
<p>subsample : float, default=1.0
    The fraction of samples to be used for fitting the individual base
    learners. If smaller than 1.0 this results in Stochastic Gradient
    Boosting. <code>subsample</code> interacts with the parameter <code>n_estimators</code>.
    Choosing <code>subsample &lt; 1.0</code> leads to a reduction of variance
    and an increase in bias.</p>
<p>criterion : {'friedman_mse', 'squared_error', 'mse', 'mae'},             default='friedman_mse'
    The function to measure the quality of a split. Supported criteria
    are "friedman_mse" for the mean squared error with improvement
    score by Friedman, "squared_error" for mean squared error, and "mae"
    for the mean absolute error. The default value of "friedman_mse" is
    generally the best as it can provide a better approximation in some
    cases.</p>
<div class="codehilite"><pre><span></span><code>.. versionadded:: 0.18

.. deprecated:: 0.24
    `criterion=&#39;mae&#39;` is deprecated and will be removed in version
    1.1 (renaming of 0.26). The correct way of minimizing the absolute
    error is to use `loss=&#39;absolute_error&#39;` instead.

.. deprecated:: 1.0
    Criterion &#39;mse&#39; was deprecated in v1.0 and will be removed in
    version 1.2. Use `criterion=&#39;squared_error&#39;` which is equivalent.
</code></pre></div>

<p>min_samples_split : int or float, default=2
    The minimum number of samples required to split an internal node:</p>
<div class="codehilite"><pre><span></span><code>- If int, then consider `min_samples_split` as the minimum number.
- If float, then `min_samples_split` is a fraction and
  `ceil(min_samples_split * n_samples)` are the minimum
  number of samples for each split.

.. versionchanged:: 0.18
   Added float values for fractions.
</code></pre></div>

<p>min_samples_leaf : int or float, default=1
    The minimum number of samples required to be at a leaf node.
    A split point at any depth will only be considered if it leaves at
    least <code>min_samples_leaf</code> training samples in each of the left and
    right branches.  This may have the effect of smoothing the model,
    especially in regression.</p>
<div class="codehilite"><pre><span></span><code>- If int, then consider `min_samples_leaf` as the minimum number.
- If float, then `min_samples_leaf` is a fraction and
  `ceil(min_samples_leaf * n_samples)` are the minimum
  number of samples for each node.

.. versionchanged:: 0.18
   Added float values for fractions.
</code></pre></div>

<p>min_weight_fraction_leaf : float, default=0.0
    The minimum weighted fraction of the sum total of weights (of all
    the input samples) required to be at a leaf node. Samples have
    equal weight when sample_weight is not provided.</p>
<p>max_depth : int, default=3
    Maximum depth of the individual regression estimators. The maximum
    depth limits the number of nodes in the tree. Tune this parameter
    for best performance; the best value depends on the interaction
    of the input variables.</p>
<p>min_impurity_decrease : float, default=0.0
    A node will be split if this split induces a decrease of the impurity
    greater than or equal to this value.</p>
<div class="codehilite"><pre><span></span><code>The weighted impurity decrease equation is the following::

    N_t / N * (impurity - N_t_R / N_t * right_impurity
                        - N_t_L / N_t * left_impurity)

where ``N`` is the total number of samples, ``N_t`` is the number of
samples at the current node, ``N_t_L`` is the number of samples in the
left child, and ``N_t_R`` is the number of samples in the right child.

``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,
if ``sample_weight`` is passed.

.. versionadded:: 0.19
</code></pre></div>

<p>init : estimator or 'zero', default=None
    An estimator object that is used to compute the initial predictions.
    <code>init</code> has to provide :term:<code>fit</code> and :term:<code>predict</code>. If 'zero', the
    initial raw predictions are set to zero. By default a
    <code>DummyEstimator</code> is used, predicting either the average target value
    (for loss='squared_error'), or a quantile for the other losses.</p>
<p>random_state : int, RandomState instance or None, default=None
    Controls the random seed given to each Tree estimator at each
    boosting iteration.
    In addition, it controls the random permutation of the features at
    each split (see Notes for more details).
    It also controls the random splitting of the training data to obtain a
    validation set if <code>n_iter_no_change</code> is not None.
    Pass an int for reproducible output across multiple function calls.
    See :term:<code>Glossary &lt;random_state&gt;</code>.</p>
<p>max_features : {'auto', 'sqrt', 'log2'}, int or float, default=None
    The number of features to consider when looking for the best split:</p>
<div class="codehilite"><pre><span></span><code>- If int, then consider `max_features` features at each split.
- If float, then `max_features` is a fraction and
  `int(max_features * n_features)` features are considered at each
  split.
- If &quot;auto&quot;, then `max_features=n_features`.
- If &quot;sqrt&quot;, then `max_features=sqrt(n_features)`.
- If &quot;log2&quot;, then `max_features=log2(n_features)`.
- If None, then `max_features=n_features`.

Choosing `max_features &lt; n_features` leads to a reduction of variance
and an increase in bias.

Note: the search for a split does not stop until at least one
valid partition of the node samples is found, even if it requires to
effectively inspect more than ``max_features`` features.
</code></pre></div>

<p>alpha : float, default=0.9
    The alpha-quantile of the huber loss function and the quantile
    loss function. Only if <code>loss='huber'</code> or <code>loss='quantile'</code>.</p>
<p>verbose : int, default=0
    Enable verbose output. If 1 then it prints progress and performance
    once in a while (the more trees the lower the frequency). If greater
    than 1 then it prints progress and performance for every tree.</p>
<p>max_leaf_nodes : int, default=None
    Grow trees with <code>max_leaf_nodes</code> in best-first fashion.
    Best nodes are defined as relative reduction in impurity.
    If None then unlimited number of leaf nodes.</p>
<p>warm_start : bool, default=False
    When set to <code>True</code>, reuse the solution of the previous call to fit
    and add more estimators to the ensemble, otherwise, just erase the
    previous solution. See :term:<code>the Glossary &lt;warm_start&gt;</code>.</p>
<p>validation_fraction : float, default=0.1
    The proportion of training data to set aside as validation set for
    early stopping. Must be between 0 and 1.
    Only used if <code>n_iter_no_change</code> is set to an integer.</p>
<div class="codehilite"><pre><span></span><code>.. versionadded:: 0.20
</code></pre></div>

<p>n_iter_no_change : int, default=None
    <code>n_iter_no_change</code> is used to decide if early stopping will be used
    to terminate training when validation score is not improving. By
    default it is set to None to disable early stopping. If set to a
    number, it will set aside <code>validation_fraction</code> size of the training
    data as validation and terminate training when validation score is not
    improving in all of the previous <code>n_iter_no_change</code> numbers of
    iterations.</p>
<div class="codehilite"><pre><span></span><code>.. versionadded:: 0.20
</code></pre></div>

<p>tol : float, default=1e-4
    Tolerance for the early stopping. When the loss is not improving
    by at least tol for <code>n_iter_no_change</code> iterations (if set to a
    number), the training stops.</p>
<div class="codehilite"><pre><span></span><code>.. versionadded:: 0.20
</code></pre></div>

<p>ccp_alpha : non-negative float, default=0.0
    Complexity parameter used for Minimal Cost-Complexity Pruning. The
    subtree with the largest cost complexity that is smaller than
    <code>ccp_alpha</code> will be chosen. By default, no pruning is performed. See
    :ref:<code>minimal_cost_complexity_pruning</code> for details.</p>
<div class="codehilite"><pre><span></span><code>.. versionadded:: 0.22
</code></pre></div>

<h5 id="giants.config.ModelEstimatorConfig.GradientBoostingRegressor--attributes">Attributes<a class="headerlink" href="#giants.config.ModelEstimatorConfig.GradientBoostingRegressor--attributes" title="Permanent link">&para;</a></h5>
<p>feature_importances_ : ndarray of shape (n_features,)
    The impurity-based feature importances.
    The higher, the more important the feature.
    The importance of a feature is computed as the (normalized)
    total reduction of the criterion brought by that feature.  It is also
    known as the Gini importance.</p>
<div class="codehilite"><pre><span></span><code>Warning: impurity-based feature importances can be misleading for
high cardinality features (many unique values). See
:func:`sklearn.inspection.permutation_importance` as an alternative.
</code></pre></div>

<p>oob_improvement_ : ndarray of shape (n_estimators,)
    The improvement in loss (= deviance) on the out-of-bag samples
    relative to the previous iteration.
    <code>oob_improvement_[0]</code> is the improvement in
    loss of the first stage over the <code>init</code> estimator.
    Only available if <code>subsample &lt; 1.0</code></p>
<p>train_score_ : ndarray of shape (n_estimators,)
    The i-th score <code>train_score_[i]</code> is the deviance (= loss) of the
    model at iteration <code>i</code> on the in-bag sample.
    If <code>subsample == 1</code> this is the deviance on the training data.</p>
<p>loss_ : LossFunction
    The concrete <code>LossFunction</code> object.</p>
<p>init_ : estimator
    The estimator that provides the initial predictions.
    Set via the <code>init</code> argument or <code>loss.init_estimator</code>.</p>
<p>estimators_ : ndarray of DecisionTreeRegressor of shape (n_estimators, 1)
    The collection of fitted sub-estimators.</p>
<p>n_classes_ : int
    The number of classes, set to 1 for regressors.</p>
<div class="codehilite"><pre><span></span><code>.. deprecated:: 0.24
    Attribute ``n_classes_`` was deprecated in version 0.24 and
    will be removed in 1.1 (renaming of 0.26).
</code></pre></div>

<p>n_estimators_ : int
    The number of estimators as selected by early stopping (if
    <code>n_iter_no_change</code> is specified). Otherwise it is set to
    <code>n_estimators</code>.</p>
<p>n_features_ : int
    The number of data features.</p>
<div class="codehilite"><pre><span></span><code>.. deprecated:: 1.0
    Attribute `n_features_` was deprecated in version 1.0 and will be
    removed in 1.2. Use `n_features_in_` instead.
</code></pre></div>

<p>n_features_in_ : int
    Number of features seen during :term:<code>fit</code>.</p>
<div class="codehilite"><pre><span></span><code>.. versionadded:: 0.24
</code></pre></div>

<p>feature_names_in_ : ndarray of shape (<code>n_features_in_</code>,)
    Names of features seen during :term:<code>fit</code>. Defined only when <code>X</code>
    has feature names that are all strings.</p>
<div class="codehilite"><pre><span></span><code>.. versionadded:: 1.0
</code></pre></div>

<p>max_features_ : int
    The inferred value of max_features.</p>
<h5 id="giants.config.ModelEstimatorConfig.GradientBoostingRegressor--see-also">See Also<a class="headerlink" href="#giants.config.ModelEstimatorConfig.GradientBoostingRegressor--see-also" title="Permanent link">&para;</a></h5>
<p>HistGradientBoostingRegressor : Histogram-based Gradient Boosting
    Classification Tree.
sklearn.tree.DecisionTreeRegressor : A decision tree regressor.
sklearn.ensemble.RandomForestRegressor : A random forest regressor.</p>
<h5 id="giants.config.ModelEstimatorConfig.GradientBoostingRegressor--notes">Notes<a class="headerlink" href="#giants.config.ModelEstimatorConfig.GradientBoostingRegressor--notes" title="Permanent link">&para;</a></h5>
<p>The features are always randomly permuted at each split. Therefore,
the best found split may vary, even with the same training data and
<code>max_features=n_features</code>, if the improvement of the criterion is
identical for several splits enumerated during the search of the best
split. To obtain a deterministic behaviour during fitting,
<code>random_state</code> has to be fixed.</p>
<h5 id="giants.config.ModelEstimatorConfig.GradientBoostingRegressor--references">References<a class="headerlink" href="#giants.config.ModelEstimatorConfig.GradientBoostingRegressor--references" title="Permanent link">&para;</a></h5>
<p>J. Friedman, Greedy Function Approximation: A Gradient Boosting
Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.</p>
<p>J. Friedman, Stochastic Gradient Boosting, 1999</p>
<p>T. Hastie, R. Tibshirani and J. Friedman.
Elements of Statistical Learning Ed. 2, Springer, 2009.</p>
<h5 id="giants.config.ModelEstimatorConfig.GradientBoostingRegressor--examples">Examples<a class="headerlink" href="#giants.config.ModelEstimatorConfig.GradientBoostingRegressor--examples" title="Permanent link">&para;</a></h5>
<blockquote>
<blockquote>
<blockquote>
<p>from sklearn.datasets import make_regression
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.model_selection import train_test_split
X, y = make_regression(random_state=0)
X_train, X_test, y_train, y_test = train_test_split(
...     X, y, random_state=0)
reg = GradientBoostingRegressor(random_state=0)
reg.fit(X_train, y_train)
GradientBoostingRegressor(random_state=0)
reg.predict(X_test[1:2])
array([-61...])
reg.score(X_test, y_test)
0.4...</p>
</blockquote>
</blockquote>
</blockquote>




  <div class="doc doc-children">






  <div class="doc doc-object doc-attribute">



<h4 class="doc doc-heading" id="giants.config.ModelEstimatorConfig.GradientBoostingRegressor.n_classes_">
<code class="codehilite language-python"><span class="n">n_classes_</span><span class="p">:</span> <span class="kc">None</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-readonly"><code>readonly</code></small>
  </span>

<a class="headerlink" href="#giants.config.ModelEstimatorConfig.GradientBoostingRegressor.n_classes_" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>DEPRECATED: Attribute <code>n_classes_</code> was deprecated in version 0.24 and will be removed in 1.1 (renaming of 0.26).</p>
    </div>

  </div>







  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="giants.config.ModelEstimatorConfig.GradientBoostingRegressor.apply">
<code class="codehilite language-python"><span class="n">apply</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span></code>


<a class="headerlink" href="#giants.config.ModelEstimatorConfig.GradientBoostingRegressor.apply" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Apply trees in the ensemble to X, return leaf indices.</p>
<p>.. versionadded:: 0.17</p>
<h6 id="giants.config.ModelEstimatorConfig.GradientBoostingRegressor.apply--parameters">Parameters<a class="headerlink" href="#giants.config.ModelEstimatorConfig.GradientBoostingRegressor.apply--parameters" title="Permanent link">&para;</a></h6>
<p>X : {array-like, sparse matrix} of shape (n_samples, n_features)
    The input samples. Internally, its dtype will be converted to
    <code>dtype=np.float32</code>. If a sparse matrix is provided, it will
    be converted to a sparse <code>csr_matrix</code>.</p>
<h6 id="giants.config.ModelEstimatorConfig.GradientBoostingRegressor.apply--returns">Returns<a class="headerlink" href="#giants.config.ModelEstimatorConfig.GradientBoostingRegressor.apply--returns" title="Permanent link">&para;</a></h6>
<p>X_leaves : array-like of shape (n_samples, n_estimators)
    For each datapoint x in X and for each tree in the ensemble,
    return the index of the leaf x ends up in each estimator.</p>

        <details class="quote">
          <summary>Source code in <code>giants/config.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">apply</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Apply trees in the ensemble to X, return leaf indices.</span>

<span class="sd">    .. versionadded:: 0.17</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">        The input samples. Internally, its dtype will be converted to</span>
<span class="sd">        ``dtype=np.float32``. If a sparse matrix is provided, it will</span>
<span class="sd">        be converted to a sparse ``csr_matrix``.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    X_leaves : array-like of shape (n_samples, n_estimators)</span>
<span class="sd">        For each datapoint x in X and for each tree in the ensemble,</span>
<span class="sd">        return the index of the leaf x ends up in each estimator.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">leaves</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">leaves</span> <span class="o">=</span> <span class="n">leaves</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">leaves</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="giants.config.ModelEstimatorConfig.GradientBoostingRegressor.predict">
<code class="codehilite language-python"><span class="n">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span></code>


<a class="headerlink" href="#giants.config.ModelEstimatorConfig.GradientBoostingRegressor.predict" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Predict regression target for X.</p>
<h6 id="giants.config.ModelEstimatorConfig.GradientBoostingRegressor.predict--parameters">Parameters<a class="headerlink" href="#giants.config.ModelEstimatorConfig.GradientBoostingRegressor.predict--parameters" title="Permanent link">&para;</a></h6>
<p>X : {array-like, sparse matrix} of shape (n_samples, n_features)
    The input samples. Internally, it will be converted to
    <code>dtype=np.float32</code> and if a sparse matrix is provided
    to a sparse <code>csr_matrix</code>.</p>
<h6 id="giants.config.ModelEstimatorConfig.GradientBoostingRegressor.predict--returns">Returns<a class="headerlink" href="#giants.config.ModelEstimatorConfig.GradientBoostingRegressor.predict--returns" title="Permanent link">&para;</a></h6>
<p>y : ndarray of shape (n_samples,)
    The predicted values.</p>

        <details class="quote">
          <summary>Source code in <code>giants/config.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Predict regression target for X.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">        The input samples. Internally, it will be converted to</span>
<span class="sd">        ``dtype=np.float32`` and if a sparse matrix is provided</span>
<span class="sd">        to a sparse ``csr_matrix``.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    y : ndarray of shape (n_samples,)</span>
<span class="sd">        The predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_data</span><span class="p">(</span>
        <span class="n">X</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">DTYPE</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="s2">&quot;C&quot;</span><span class="p">,</span> <span class="n">accept_sparse</span><span class="o">=</span><span class="s2">&quot;csr&quot;</span><span class="p">,</span> <span class="n">reset</span><span class="o">=</span><span class="kc">False</span>
    <span class="p">)</span>
    <span class="c1"># In regression we can directly return the raw value from the trees.</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_raw_predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="giants.config.ModelEstimatorConfig.GradientBoostingRegressor.staged_predict">
<code class="codehilite language-python"><span class="n">staged_predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span></code>


<a class="headerlink" href="#giants.config.ModelEstimatorConfig.GradientBoostingRegressor.staged_predict" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Predict regression target at each stage for X.</p>
<p>This method allows monitoring (i.e. determine error on testing set)
after each stage.</p>
<h6 id="giants.config.ModelEstimatorConfig.GradientBoostingRegressor.staged_predict--parameters">Parameters<a class="headerlink" href="#giants.config.ModelEstimatorConfig.GradientBoostingRegressor.staged_predict--parameters" title="Permanent link">&para;</a></h6>
<p>X : {array-like, sparse matrix} of shape (n_samples, n_features)
    The input samples. Internally, it will be converted to
    <code>dtype=np.float32</code> and if a sparse matrix is provided
    to a sparse <code>csr_matrix</code>.</p>
<h6 id="giants.config.ModelEstimatorConfig.GradientBoostingRegressor.staged_predict--yields">Yields<a class="headerlink" href="#giants.config.ModelEstimatorConfig.GradientBoostingRegressor.staged_predict--yields" title="Permanent link">&para;</a></h6>
<p>y : generator of ndarray of shape (n_samples,)
    The predicted value of the input samples.</p>

        <details class="quote">
          <summary>Source code in <code>giants/config.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">staged_predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Predict regression target at each stage for X.</span>

<span class="sd">    This method allows monitoring (i.e. determine error on testing set)</span>
<span class="sd">    after each stage.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">        The input samples. Internally, it will be converted to</span>
<span class="sd">        ``dtype=np.float32`` and if a sparse matrix is provided</span>
<span class="sd">        to a sparse ``csr_matrix``.</span>

<span class="sd">    Yields</span>
<span class="sd">    ------</span>
<span class="sd">    y : generator of ndarray of shape (n_samples,)</span>
<span class="sd">        The predicted value of the input samples.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">raw_predictions</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_staged_raw_predict</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
        <span class="k">yield</span> <span class="n">raw_predictions</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h3 class="doc doc-heading" id="giants.config.ModelEstimatorConfig.LinearRegression">
        <code>
LinearRegression            (<span title="sklearn.base.MultiOutputMixin">MultiOutputMixin</span>, <span title="sklearn.base.RegressorMixin">RegressorMixin</span>, <span title="sklearn.linear_model._base.LinearModel">LinearModel</span>)
        </code>



<a class="headerlink" href="#giants.config.ModelEstimatorConfig.LinearRegression" title="Permanent link">&para;</a></h3>

    <div class="doc doc-contents ">

      <p>Ordinary least squares Linear Regression.</p>
<p>LinearRegression fits a linear model with coefficients w = (w1, ..., wp)
to minimize the residual sum of squares between the observed targets in
the dataset, and the targets predicted by the linear approximation.</p>
<h5 id="giants.config.ModelEstimatorConfig.LinearRegression--parameters">Parameters<a class="headerlink" href="#giants.config.ModelEstimatorConfig.LinearRegression--parameters" title="Permanent link">&para;</a></h5>
<p>fit_intercept : bool, default=True
    Whether to calculate the intercept for this model. If set
    to False, no intercept will be used in calculations
    (i.e. data is expected to be centered).</p>
<p>normalize : bool, default=False
    This parameter is ignored when <code>fit_intercept</code> is set to False.
    If True, the regressors X will be normalized before regression by
    subtracting the mean and dividing by the l2-norm.
    If you wish to standardize, please use
    :class:<code>~sklearn.preprocessing.StandardScaler</code> before calling <code>fit</code>
    on an estimator with <code>normalize=False</code>.</p>
<div class="codehilite"><pre><span></span><code>.. deprecated:: 1.0
   `normalize` was deprecated in version 1.0 and will be
   removed in 1.2.
</code></pre></div>

<p>copy_X : bool, default=True
    If True, X will be copied; else, it may be overwritten.</p>
<p>n_jobs : int, default=None
    The number of jobs to use for the computation. This will only provide
    speedup in case of sufficiently large problems, that is if firstly
    <code>n_targets &gt; 1</code> and secondly <code>X</code> is sparse or if <code>positive</code> is set
    to <code>True</code>. <code>None</code> means 1 unless in a
    :obj:<code>joblib.parallel_backend</code> context. <code>-1</code> means using all
    processors. See :term:<code>Glossary &lt;n_jobs&gt;</code> for more details.</p>
<p>positive : bool, default=False
    When set to <code>True</code>, forces the coefficients to be positive. This
    option is only supported for dense arrays.</p>
<div class="codehilite"><pre><span></span><code>.. versionadded:: 0.24
</code></pre></div>

<h5 id="giants.config.ModelEstimatorConfig.LinearRegression--attributes">Attributes<a class="headerlink" href="#giants.config.ModelEstimatorConfig.LinearRegression--attributes" title="Permanent link">&para;</a></h5>
<p>coef_ : array of shape (n_features, ) or (n_targets, n_features)
    Estimated coefficients for the linear regression problem.
    If multiple targets are passed during the fit (y 2D), this
    is a 2D array of shape (n_targets, n_features), while if only
    one target is passed, this is a 1D array of length n_features.</p>
<p>rank_ : int
    Rank of matrix <code>X</code>. Only available when <code>X</code> is dense.</p>
<p>singular_ : array of shape (min(X, y),)
    Singular values of <code>X</code>. Only available when <code>X</code> is dense.</p>
<p>intercept_ : float or array of shape (n_targets,)
    Independent term in the linear model. Set to 0.0 if
    <code>fit_intercept = False</code>.</p>
<p>n_features_in_ : int
    Number of features seen during :term:<code>fit</code>.</p>
<div class="codehilite"><pre><span></span><code>.. versionadded:: 0.24
</code></pre></div>

<p>feature_names_in_ : ndarray of shape (<code>n_features_in_</code>,)
    Names of features seen during :term:<code>fit</code>. Defined only when <code>X</code>
    has feature names that are all strings.</p>
<div class="codehilite"><pre><span></span><code>.. versionadded:: 1.0
</code></pre></div>

<h5 id="giants.config.ModelEstimatorConfig.LinearRegression--see-also">See Also<a class="headerlink" href="#giants.config.ModelEstimatorConfig.LinearRegression--see-also" title="Permanent link">&para;</a></h5>
<p>Ridge : Ridge regression addresses some of the
    problems of Ordinary Least Squares by imposing a penalty on the
    size of the coefficients with l2 regularization.
Lasso : The Lasso is a linear model that estimates
    sparse coefficients with l1 regularization.
ElasticNet : Elastic-Net is a linear regression
    model trained with both l1 and l2 -norm regularization of the
    coefficients.</p>
<h5 id="giants.config.ModelEstimatorConfig.LinearRegression--notes">Notes<a class="headerlink" href="#giants.config.ModelEstimatorConfig.LinearRegression--notes" title="Permanent link">&para;</a></h5>
<p>From the implementation point of view, this is just plain Ordinary
Least Squares (scipy.linalg.lstsq) or Non Negative Least Squares
(scipy.optimize.nnls) wrapped as a predictor object.</p>
<h5 id="giants.config.ModelEstimatorConfig.LinearRegression--examples">Examples<a class="headerlink" href="#giants.config.ModelEstimatorConfig.LinearRegression--examples" title="Permanent link">&para;</a></h5>
<blockquote>
<blockquote>
<blockquote>
<p>import numpy as np
from sklearn.linear_model import LinearRegression
X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])</p>
<h4 id="giants.config.ModelEstimatorConfig.LinearRegression--y-1-x_0-2-x_1-3">y = 1 * x_0 + 2 * x_1 + 3<a class="headerlink" href="#giants.config.ModelEstimatorConfig.LinearRegression--y-1-x_0-2-x_1-3" title="Permanent link">&para;</a></h4>
<p>y = np.dot(X, np.array([1, 2])) + 3
reg = LinearRegression().fit(X, y)
reg.score(X, y)
1.0
reg.coef_
array([1., 2.])
reg.intercept_
3.0...
reg.predict(np.array([[3, 5]]))
array([16.])</p>
</blockquote>
</blockquote>
</blockquote>




  <div class="doc doc-children">










  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="giants.config.ModelEstimatorConfig.LinearRegression.fit">
<code class="codehilite language-python"><span class="n">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>


<a class="headerlink" href="#giants.config.ModelEstimatorConfig.LinearRegression.fit" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Fit linear model.</p>
<h6 id="giants.config.ModelEstimatorConfig.LinearRegression.fit--parameters">Parameters<a class="headerlink" href="#giants.config.ModelEstimatorConfig.LinearRegression.fit--parameters" title="Permanent link">&para;</a></h6>
<p>X : {array-like, sparse matrix} of shape (n_samples, n_features)
    Training data.</p>
<p>y : array-like of shape (n_samples,) or (n_samples, n_targets)
    Target values. Will be cast to X's dtype if necessary.</p>
<p>sample_weight : array-like of shape (n_samples,), default=None
    Individual weights for each sample.</p>
<div class="codehilite"><pre><span></span><code>.. versionadded:: 0.17
   parameter *sample_weight* support to LinearRegression.
</code></pre></div>

<h6 id="giants.config.ModelEstimatorConfig.LinearRegression.fit--returns">Returns<a class="headerlink" href="#giants.config.ModelEstimatorConfig.LinearRegression.fit--returns" title="Permanent link">&para;</a></h6>
<p>self : object
    Fitted Estimator.</p>

        <details class="quote">
          <summary>Source code in <code>giants/config.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Fit linear model.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">        Training data.</span>

<span class="sd">    y : array-like of shape (n_samples,) or (n_samples, n_targets)</span>
<span class="sd">        Target values. Will be cast to X&#39;s dtype if necessary.</span>

<span class="sd">    sample_weight : array-like of shape (n_samples,), default=None</span>
<span class="sd">        Individual weights for each sample.</span>

<span class="sd">        .. versionadded:: 0.17</span>
<span class="sd">           parameter *sample_weight* support to LinearRegression.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    self : object</span>
<span class="sd">        Fitted Estimator.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_normalize</span> <span class="o">=</span> <span class="n">_deprecate_normalize</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">normalize</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">estimator_name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span>
    <span class="p">)</span>

    <span class="n">n_jobs_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_jobs</span>

    <span class="n">accept_sparse</span> <span class="o">=</span> <span class="kc">False</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">positive</span> <span class="k">else</span> <span class="p">[</span><span class="s2">&quot;csr&quot;</span><span class="p">,</span> <span class="s2">&quot;csc&quot;</span><span class="p">,</span> <span class="s2">&quot;coo&quot;</span><span class="p">]</span>

    <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_data</span><span class="p">(</span>
        <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">accept_sparse</span><span class="o">=</span><span class="n">accept_sparse</span><span class="p">,</span> <span class="n">y_numeric</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">multi_output</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="n">sample_weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">_check_sample_weight</span><span class="p">(</span><span class="n">sample_weight</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">X_offset</span><span class="p">,</span> <span class="n">y_offset</span><span class="p">,</span> <span class="n">X_scale</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_preprocess_data</span><span class="p">(</span>
        <span class="n">X</span><span class="p">,</span>
        <span class="n">y</span><span class="p">,</span>
        <span class="n">fit_intercept</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">fit_intercept</span><span class="p">,</span>
        <span class="n">normalize</span><span class="o">=</span><span class="n">_normalize</span><span class="p">,</span>
        <span class="n">copy</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">copy_X</span><span class="p">,</span>
        <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">,</span>
        <span class="n">return_mean</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="n">sample_weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># Sample weight can be implemented via a simple rescaling.</span>
        <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">_rescale_data</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">positive</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">y</span><span class="o">.</span><span class="n">ndim</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_residues</span> <span class="o">=</span> <span class="n">optimize</span><span class="o">.</span><span class="n">nnls</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># scipy.optimize.nnls cannot handle y with shape (M, K)</span>
            <span class="n">outs</span> <span class="o">=</span> <span class="n">Parallel</span><span class="p">(</span><span class="n">n_jobs</span><span class="o">=</span><span class="n">n_jobs_</span><span class="p">)(</span>
                <span class="n">delayed</span><span class="p">(</span><span class="n">optimize</span><span class="o">.</span><span class="n">nnls</span><span class="p">)(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">[:,</span> <span class="n">j</span><span class="p">])</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_residues</span> <span class="o">=</span> <span class="nb">map</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">,</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">outs</span><span class="p">))</span>
    <span class="k">elif</span> <span class="n">sp</span><span class="o">.</span><span class="n">issparse</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
        <span class="n">X_offset_scale</span> <span class="o">=</span> <span class="n">X_offset</span> <span class="o">/</span> <span class="n">X_scale</span>

        <span class="k">def</span> <span class="nf">matvec</span><span class="p">(</span><span class="n">b</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">X</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">b</span><span class="p">)</span> <span class="o">-</span> <span class="n">b</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X_offset_scale</span><span class="p">)</span>

        <span class="k">def</span> <span class="nf">rmatvec</span><span class="p">(</span><span class="n">b</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">b</span><span class="p">)</span> <span class="o">-</span> <span class="n">X_offset_scale</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>

        <span class="n">X_centered</span> <span class="o">=</span> <span class="n">sparse</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">LinearOperator</span><span class="p">(</span>
            <span class="n">shape</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">matvec</span><span class="o">=</span><span class="n">matvec</span><span class="p">,</span> <span class="n">rmatvec</span><span class="o">=</span><span class="n">rmatvec</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="n">y</span><span class="o">.</span><span class="n">ndim</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">sparse_lsqr</span><span class="p">(</span><span class="n">X_centered</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span> <span class="o">=</span> <span class="n">out</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_residues</span> <span class="o">=</span> <span class="n">out</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># sparse_lstsq cannot handle y with shape (M, K)</span>
            <span class="n">outs</span> <span class="o">=</span> <span class="n">Parallel</span><span class="p">(</span><span class="n">n_jobs</span><span class="o">=</span><span class="n">n_jobs_</span><span class="p">)(</span>
                <span class="n">delayed</span><span class="p">(</span><span class="n">sparse_lsqr</span><span class="p">)(</span><span class="n">X_centered</span><span class="p">,</span> <span class="n">y</span><span class="p">[:,</span> <span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">ravel</span><span class="p">())</span>
                <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">out</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">out</span> <span class="ow">in</span> <span class="n">outs</span><span class="p">])</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_residues</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">out</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="k">for</span> <span class="n">out</span> <span class="ow">in</span> <span class="n">outs</span><span class="p">])</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_residues</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">rank_</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">singular_</span> <span class="o">=</span> <span class="n">linalg</span><span class="o">.</span><span class="n">lstsq</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span><span class="o">.</span><span class="n">T</span>

    <span class="k">if</span> <span class="n">y</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ravel</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_set_intercept</span><span class="p">(</span><span class="n">X_offset</span><span class="p">,</span> <span class="n">y_offset</span><span class="p">,</span> <span class="n">X_scale</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h3 class="doc doc-heading" id="giants.config.ModelEstimatorConfig.LinearSVC">
        <code>
LinearSVC            (<span title="sklearn.linear_model._base.LinearClassifierMixin">LinearClassifierMixin</span>, <span title="sklearn.linear_model._base.SparseCoefMixin">SparseCoefMixin</span>, <span title="sklearn.base.BaseEstimator">BaseEstimator</span>)
        </code>



<a class="headerlink" href="#giants.config.ModelEstimatorConfig.LinearSVC" title="Permanent link">&para;</a></h3>

    <div class="doc doc-contents ">

      <p>Linear Support Vector Classification.</p>
<p>Similar to SVC with parameter kernel='linear', but implemented in terms of
liblinear rather than libsvm, so it has more flexibility in the choice of
penalties and loss functions and should scale better to large numbers of
samples.</p>
<p>This class supports both dense and sparse input and the multiclass support
is handled according to a one-vs-the-rest scheme.</p>
<p>Read more in the :ref:<code>User Guide &lt;svm_classification&gt;</code>.</p>
<h5 id="giants.config.ModelEstimatorConfig.LinearSVC--parameters">Parameters<a class="headerlink" href="#giants.config.ModelEstimatorConfig.LinearSVC--parameters" title="Permanent link">&para;</a></h5>
<p>penalty : {'l1', 'l2'}, default='l2'
    Specifies the norm used in the penalization. The 'l2'
    penalty is the standard used in SVC. The 'l1' leads to <code>coef_</code>
    vectors that are sparse.</p>
<p>loss : {'hinge', 'squared_hinge'}, default='squared_hinge'
    Specifies the loss function. 'hinge' is the standard SVM loss
    (used e.g. by the SVC class) while 'squared_hinge' is the
    square of the hinge loss. The combination of <code>penalty='l1'</code>
    and <code>loss='hinge'</code> is not supported.</p>
<p>dual : bool, default=True
    Select the algorithm to either solve the dual or primal
    optimization problem. Prefer dual=False when n_samples &gt; n_features.</p>
<p>tol : float, default=1e-4
    Tolerance for stopping criteria.</p>
<p>C : float, default=1.0
    Regularization parameter. The strength of the regularization is
    inversely proportional to C. Must be strictly positive.</p>
<p>multi_class : {'ovr', 'crammer_singer'}, default='ovr'
    Determines the multi-class strategy if <code>y</code> contains more than
    two classes.
    <code>"ovr"</code> trains n_classes one-vs-rest classifiers, while
    <code>"crammer_singer"</code> optimizes a joint objective over all classes.
    While <code>crammer_singer</code> is interesting from a theoretical perspective
    as it is consistent, it is seldom used in practice as it rarely leads
    to better accuracy and is more expensive to compute.
    If <code>"crammer_singer"</code> is chosen, the options loss, penalty and dual
    will be ignored.</p>
<p>fit_intercept : bool, default=True
    Whether to calculate the intercept for this model. If set
    to false, no intercept will be used in calculations
    (i.e. data is expected to be already centered).</p>
<p>intercept_scaling : float, default=1
    When self.fit_intercept is True, instance vector x becomes
    <code>[x, self.intercept_scaling]</code>,
    i.e. a "synthetic" feature with constant value equals to
    intercept_scaling is appended to the instance vector.
    The intercept becomes intercept_scaling * synthetic feature weight
    Note! the synthetic feature weight is subject to l1/l2 regularization
    as all other features.
    To lessen the effect of regularization on synthetic feature weight
    (and therefore on the intercept) intercept_scaling has to be increased.</p>
<p>class_weight : dict or 'balanced', default=None
    Set the parameter C of class i to <code>class_weight[i]*C</code> for
    SVC. If not given, all classes are supposed to have
    weight one.
    The "balanced" mode uses the values of y to automatically adjust
    weights inversely proportional to class frequencies in the input data
    as <code>n_samples / (n_classes * np.bincount(y))</code>.</p>
<p>verbose : int, default=0
    Enable verbose output. Note that this setting takes advantage of a
    per-process runtime setting in liblinear that, if enabled, may not work
    properly in a multithreaded context.</p>
<p>random_state : int, RandomState instance or None, default=None
    Controls the pseudo random number generation for shuffling the data for
    the dual coordinate descent (if <code>dual=True</code>). When <code>dual=False</code> the
    underlying implementation of :class:<code>LinearSVC</code> is not random and
    <code>random_state</code> has no effect on the results.
    Pass an int for reproducible output across multiple function calls.
    See :term:<code>Glossary &lt;random_state&gt;</code>.</p>
<p>max_iter : int, default=1000
    The maximum number of iterations to be run.</p>
<h5 id="giants.config.ModelEstimatorConfig.LinearSVC--attributes">Attributes<a class="headerlink" href="#giants.config.ModelEstimatorConfig.LinearSVC--attributes" title="Permanent link">&para;</a></h5>
<p>coef_ : ndarray of shape (1, n_features) if n_classes == 2             else (n_classes, n_features)
    Weights assigned to the features (coefficients in the primal
    problem).</p>
<div class="codehilite"><pre><span></span><code>``coef_`` is a readonly property derived from ``raw_coef_`` that
follows the internal memory layout of liblinear.
</code></pre></div>

<p>intercept_ : ndarray of shape (1,) if n_classes == 2 else (n_classes,)
    Constants in decision function.</p>
<p>classes_ : ndarray of shape (n_classes,)
    The unique classes labels.</p>
<p>n_features_in_ : int
    Number of features seen during :term:<code>fit</code>.</p>
<div class="codehilite"><pre><span></span><code>.. versionadded:: 0.24
</code></pre></div>

<p>feature_names_in_ : ndarray of shape (<code>n_features_in_</code>,)
    Names of features seen during :term:<code>fit</code>. Defined only when <code>X</code>
    has feature names that are all strings.</p>
<div class="codehilite"><pre><span></span><code>.. versionadded:: 1.0
</code></pre></div>

<p>n_iter_ : int
    Maximum number of iterations run across all classes.</p>
<h5 id="giants.config.ModelEstimatorConfig.LinearSVC--see-also">See Also<a class="headerlink" href="#giants.config.ModelEstimatorConfig.LinearSVC--see-also" title="Permanent link">&para;</a></h5>
<p>SVC : Implementation of Support Vector Machine classifier using libsvm:
    the kernel can be non-linear but its SMO algorithm does not
    scale to large number of samples as LinearSVC does.</p>
<div class="codehilite"><pre><span></span><code>Furthermore SVC multi-class mode is implemented using one
vs one scheme while LinearSVC uses one vs the rest. It is
possible to implement one vs the rest with SVC by using the
:class:`~sklearn.multiclass.OneVsRestClassifier` wrapper.

Finally SVC can fit dense data without memory copy if the input
is C-contiguous. Sparse data will still incur memory copy though.
</code></pre></div>

<p>sklearn.linear_model.SGDClassifier : SGDClassifier can optimize the same
    cost function as LinearSVC
    by adjusting the penalty and loss parameters. In addition it requires
    less memory, allows incremental (online) learning, and implements
    various loss functions and regularization regimes.</p>
<h5 id="giants.config.ModelEstimatorConfig.LinearSVC--notes">Notes<a class="headerlink" href="#giants.config.ModelEstimatorConfig.LinearSVC--notes" title="Permanent link">&para;</a></h5>
<p>The underlying C implementation uses a random number generator to
select features when fitting the model. It is thus not uncommon
to have slightly different results for the same input data. If
that happens, try with a smaller <code>tol</code> parameter.</p>
<p>The underlying implementation, liblinear, uses a sparse internal
representation for the data that will incur a memory copy.</p>
<p>Predict output may not match that of standalone liblinear in certain
cases. See :ref:<code>differences from liblinear &lt;liblinear_differences&gt;</code>
in the narrative documentation.</p>
<h5 id="giants.config.ModelEstimatorConfig.LinearSVC--references">References<a class="headerlink" href="#giants.config.ModelEstimatorConfig.LinearSVC--references" title="Permanent link">&para;</a></h5>
<p><code>LIBLINEAR: A Library for Large Linear Classification
&lt;https://www.csie.ntu.edu.tw/~cjlin/liblinear/&gt;</code>__</p>
<h5 id="giants.config.ModelEstimatorConfig.LinearSVC--examples">Examples<a class="headerlink" href="#giants.config.ModelEstimatorConfig.LinearSVC--examples" title="Permanent link">&para;</a></h5>
<blockquote>
<blockquote>
<blockquote>
<p>from sklearn.svm import LinearSVC
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import make_classification
X, y = make_classification(n_features=4, random_state=0)
clf = make_pipeline(StandardScaler(),
...                     LinearSVC(random_state=0, tol=1e-5))
clf.fit(X, y)
Pipeline(steps=[('standardscaler', StandardScaler()),
                ('linearsvc', LinearSVC(random_state=0, tol=1e-05))])</p>
<p>print(clf.named_steps['linearsvc'].coef_)
[[0.141...   0.526... 0.679... 0.493...]]</p>
<p>print(clf.named_steps['linearsvc'].intercept_)
[0.1693...]
print(clf.predict([[0, 0, 0, 0]]))
[1]</p>
</blockquote>
</blockquote>
</blockquote>




  <div class="doc doc-children">










  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="giants.config.ModelEstimatorConfig.LinearSVC.fit">
<code class="codehilite language-python"><span class="n">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>


<a class="headerlink" href="#giants.config.ModelEstimatorConfig.LinearSVC.fit" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Fit the model according to the given training data.</p>
<h6 id="giants.config.ModelEstimatorConfig.LinearSVC.fit--parameters">Parameters<a class="headerlink" href="#giants.config.ModelEstimatorConfig.LinearSVC.fit--parameters" title="Permanent link">&para;</a></h6>
<p>X : {array-like, sparse matrix} of shape (n_samples, n_features)
    Training vector, where <code>n_samples</code> is the number of samples and
    <code>n_features</code> is the number of features.</p>
<p>y : array-like of shape (n_samples,)
    Target vector relative to X.</p>
<p>sample_weight : array-like of shape (n_samples,), default=None
    Array of weights that are assigned to individual
    samples. If not provided,
    then each sample is given unit weight.</p>
<div class="codehilite"><pre><span></span><code>.. versionadded:: 0.18
</code></pre></div>

<h6 id="giants.config.ModelEstimatorConfig.LinearSVC.fit--returns">Returns<a class="headerlink" href="#giants.config.ModelEstimatorConfig.LinearSVC.fit--returns" title="Permanent link">&para;</a></h6>
<p>self : object
    An instance of the estimator.</p>

        <details class="quote">
          <summary>Source code in <code>giants/config.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Fit the model according to the given training data.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">        Training vector, where `n_samples` is the number of samples and</span>
<span class="sd">        `n_features` is the number of features.</span>

<span class="sd">    y : array-like of shape (n_samples,)</span>
<span class="sd">        Target vector relative to X.</span>

<span class="sd">    sample_weight : array-like of shape (n_samples,), default=None</span>
<span class="sd">        Array of weights that are assigned to individual</span>
<span class="sd">        samples. If not provided,</span>
<span class="sd">        then each sample is given unit weight.</span>

<span class="sd">        .. versionadded:: 0.18</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    self : object</span>
<span class="sd">        An instance of the estimator.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">C</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Penalty term must be positive; got (C=</span><span class="si">%r</span><span class="s2">)&quot;</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">C</span><span class="p">)</span>

    <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_data</span><span class="p">(</span>
        <span class="n">X</span><span class="p">,</span>
        <span class="n">y</span><span class="p">,</span>
        <span class="n">accept_sparse</span><span class="o">=</span><span class="s2">&quot;csr&quot;</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span>
        <span class="n">order</span><span class="o">=</span><span class="s2">&quot;C&quot;</span><span class="p">,</span>
        <span class="n">accept_large_sparse</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">check_classification_targets</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">intercept_</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_iter_</span> <span class="o">=</span> <span class="n">_fit_liblinear</span><span class="p">(</span>
        <span class="n">X</span><span class="p">,</span>
        <span class="n">y</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">C</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fit_intercept</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">intercept_scaling</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">class_weight</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">penalty</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dual</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_iter</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tol</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">multi_class</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">,</span>
        <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">multi_class</span> <span class="o">==</span> <span class="s2">&quot;crammer_singer&quot;</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">fit_intercept</span><span class="p">:</span>
            <span class="n">intercept</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">intercept_</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">intercept_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">intercept_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">intercept</span><span class="p">])</span>

    <span class="k">return</span> <span class="bp">self</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h3 class="doc doc-heading" id="giants.config.ModelEstimatorConfig.LinearSVR">
        <code>
LinearSVR            (<span title="sklearn.base.RegressorMixin">RegressorMixin</span>, <span title="sklearn.linear_model._base.LinearModel">LinearModel</span>)
        </code>



<a class="headerlink" href="#giants.config.ModelEstimatorConfig.LinearSVR" title="Permanent link">&para;</a></h3>

    <div class="doc doc-contents ">

      <p>Linear Support Vector Regression.</p>
<p>Similar to SVR with parameter kernel='linear', but implemented in terms of
liblinear rather than libsvm, so it has more flexibility in the choice of
penalties and loss functions and should scale better to large numbers of
samples.</p>
<p>This class supports both dense and sparse input.</p>
<p>Read more in the :ref:<code>User Guide &lt;svm_regression&gt;</code>.</p>
<p>.. versionadded:: 0.16</p>
<h5 id="giants.config.ModelEstimatorConfig.LinearSVR--parameters">Parameters<a class="headerlink" href="#giants.config.ModelEstimatorConfig.LinearSVR--parameters" title="Permanent link">&para;</a></h5>
<p>epsilon : float, default=0.0
    Epsilon parameter in the epsilon-insensitive loss function. Note
    that the value of this parameter depends on the scale of the target
    variable y. If unsure, set <code>epsilon=0</code>.</p>
<p>tol : float, default=1e-4
    Tolerance for stopping criteria.</p>
<p>C : float, default=1.0
    Regularization parameter. The strength of the regularization is
    inversely proportional to C. Must be strictly positive.</p>
<p>loss : {'epsilon_insensitive', 'squared_epsilon_insensitive'},             default='epsilon_insensitive'
    Specifies the loss function. The epsilon-insensitive loss
    (standard SVR) is the L1 loss, while the squared epsilon-insensitive
    loss ('squared_epsilon_insensitive') is the L2 loss.</p>
<p>fit_intercept : bool, default=True
    Whether to calculate the intercept for this model. If set
    to false, no intercept will be used in calculations
    (i.e. data is expected to be already centered).</p>
<p>intercept_scaling : float, default=1.0
    When self.fit_intercept is True, instance vector x becomes
    [x, self.intercept_scaling],
    i.e. a "synthetic" feature with constant value equals to
    intercept_scaling is appended to the instance vector.
    The intercept becomes intercept_scaling * synthetic feature weight
    Note! the synthetic feature weight is subject to l1/l2 regularization
    as all other features.
    To lessen the effect of regularization on synthetic feature weight
    (and therefore on the intercept) intercept_scaling has to be increased.</p>
<p>dual : bool, default=True
    Select the algorithm to either solve the dual or primal
    optimization problem. Prefer dual=False when n_samples &gt; n_features.</p>
<p>verbose : int, default=0
    Enable verbose output. Note that this setting takes advantage of a
    per-process runtime setting in liblinear that, if enabled, may not work
    properly in a multithreaded context.</p>
<p>random_state : int, RandomState instance or None, default=None
    Controls the pseudo random number generation for shuffling the data.
    Pass an int for reproducible output across multiple function calls.
    See :term:<code>Glossary &lt;random_state&gt;</code>.</p>
<p>max_iter : int, default=1000
    The maximum number of iterations to be run.</p>
<h5 id="giants.config.ModelEstimatorConfig.LinearSVR--attributes">Attributes<a class="headerlink" href="#giants.config.ModelEstimatorConfig.LinearSVR--attributes" title="Permanent link">&para;</a></h5>
<p>coef_ : ndarray of shape (n_features) if n_classes == 2             else (n_classes, n_features)
    Weights assigned to the features (coefficients in the primal
    problem).</p>
<div class="codehilite"><pre><span></span><code>`coef_` is a readonly property derived from `raw_coef_` that
follows the internal memory layout of liblinear.
</code></pre></div>

<p>intercept_ : ndarray of shape (1) if n_classes == 2 else (n_classes)
    Constants in decision function.</p>
<p>n_features_in_ : int
    Number of features seen during :term:<code>fit</code>.</p>
<div class="codehilite"><pre><span></span><code>.. versionadded:: 0.24
</code></pre></div>

<p>feature_names_in_ : ndarray of shape (<code>n_features_in_</code>,)
    Names of features seen during :term:<code>fit</code>. Defined only when <code>X</code>
    has feature names that are all strings.</p>
<div class="codehilite"><pre><span></span><code>.. versionadded:: 1.0
</code></pre></div>

<p>n_iter_ : int
    Maximum number of iterations run across all classes.</p>
<h5 id="giants.config.ModelEstimatorConfig.LinearSVR--see-also">See Also<a class="headerlink" href="#giants.config.ModelEstimatorConfig.LinearSVR--see-also" title="Permanent link">&para;</a></h5>
<p>LinearSVC : Implementation of Support Vector Machine classifier using the
    same library as this class (liblinear).</p>
<p>SVR : Implementation of Support Vector Machine regression using libsvm:
    the kernel can be non-linear but its SMO algorithm does not
    scale to large number of samples as LinearSVC does.</p>
<p>sklearn.linear_model.SGDRegressor : SGDRegressor can optimize the same cost
    function as LinearSVR
    by adjusting the penalty and loss parameters. In addition it requires
    less memory, allows incremental (online) learning, and implements
    various loss functions and regularization regimes.</p>
<h5 id="giants.config.ModelEstimatorConfig.LinearSVR--examples">Examples<a class="headerlink" href="#giants.config.ModelEstimatorConfig.LinearSVR--examples" title="Permanent link">&para;</a></h5>
<blockquote>
<blockquote>
<blockquote>
<p>from sklearn.svm import LinearSVR
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import make_regression
X, y = make_regression(n_features=4, random_state=0)
regr = make_pipeline(StandardScaler(),
...                      LinearSVR(random_state=0, tol=1e-5))
regr.fit(X, y)
Pipeline(steps=[('standardscaler', StandardScaler()),
                ('linearsvr', LinearSVR(random_state=0, tol=1e-05))])</p>
<p>print(regr.named_steps['linearsvr'].coef_)
[18.582... 27.023... 44.357... 64.522...]
print(regr.named_steps['linearsvr'].intercept_)
[-4...]
print(regr.predict([[0, 0, 0, 0]]))
[-2.384...]</p>
</blockquote>
</blockquote>
</blockquote>




  <div class="doc doc-children">










  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="giants.config.ModelEstimatorConfig.LinearSVR.fit">
<code class="codehilite language-python"><span class="n">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>


<a class="headerlink" href="#giants.config.ModelEstimatorConfig.LinearSVR.fit" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Fit the model according to the given training data.</p>
<h6 id="giants.config.ModelEstimatorConfig.LinearSVR.fit--parameters">Parameters<a class="headerlink" href="#giants.config.ModelEstimatorConfig.LinearSVR.fit--parameters" title="Permanent link">&para;</a></h6>
<p>X : {array-like, sparse matrix} of shape (n_samples, n_features)
    Training vector, where <code>n_samples</code> is the number of samples and
    <code>n_features</code> is the number of features.</p>
<p>y : array-like of shape (n_samples,)
    Target vector relative to X.</p>
<p>sample_weight : array-like of shape (n_samples,), default=None
    Array of weights that are assigned to individual
    samples. If not provided,
    then each sample is given unit weight.</p>
<div class="codehilite"><pre><span></span><code>.. versionadded:: 0.18
</code></pre></div>

<h6 id="giants.config.ModelEstimatorConfig.LinearSVR.fit--returns">Returns<a class="headerlink" href="#giants.config.ModelEstimatorConfig.LinearSVR.fit--returns" title="Permanent link">&para;</a></h6>
<p>self : object
    An instance of the estimator.</p>

        <details class="quote">
          <summary>Source code in <code>giants/config.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Fit the model according to the given training data.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">        Training vector, where `n_samples` is the number of samples and</span>
<span class="sd">        `n_features` is the number of features.</span>

<span class="sd">    y : array-like of shape (n_samples,)</span>
<span class="sd">        Target vector relative to X.</span>

<span class="sd">    sample_weight : array-like of shape (n_samples,), default=None</span>
<span class="sd">        Array of weights that are assigned to individual</span>
<span class="sd">        samples. If not provided,</span>
<span class="sd">        then each sample is given unit weight.</span>

<span class="sd">        .. versionadded:: 0.18</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    self : object</span>
<span class="sd">        An instance of the estimator.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">C</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Penalty term must be positive; got (C=</span><span class="si">%r</span><span class="s2">)&quot;</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">C</span><span class="p">)</span>

    <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_data</span><span class="p">(</span>
        <span class="n">X</span><span class="p">,</span>
        <span class="n">y</span><span class="p">,</span>
        <span class="n">accept_sparse</span><span class="o">=</span><span class="s2">&quot;csr&quot;</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span>
        <span class="n">order</span><span class="o">=</span><span class="s2">&quot;C&quot;</span><span class="p">,</span>
        <span class="n">accept_large_sparse</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">penalty</span> <span class="o">=</span> <span class="s2">&quot;l2&quot;</span>  <span class="c1"># SVR only accepts l2 penalty</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">intercept_</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_iter_</span> <span class="o">=</span> <span class="n">_fit_liblinear</span><span class="p">(</span>
        <span class="n">X</span><span class="p">,</span>
        <span class="n">y</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">C</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fit_intercept</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">intercept_scaling</span><span class="p">,</span>
        <span class="kc">None</span><span class="p">,</span>
        <span class="n">penalty</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dual</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_iter</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tol</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="p">,</span>
        <span class="n">loss</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">,</span>
        <span class="n">epsilon</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span><span class="p">,</span>
        <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>

    <span class="k">return</span> <span class="bp">self</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h3 class="doc doc-heading" id="giants.config.ModelEstimatorConfig.LogisticRegression">
        <code>
LogisticRegression            (<span title="sklearn.linear_model._base.LinearClassifierMixin">LinearClassifierMixin</span>, <span title="sklearn.linear_model._base.SparseCoefMixin">SparseCoefMixin</span>, <span title="sklearn.base.BaseEstimator">BaseEstimator</span>)
        </code>



<a class="headerlink" href="#giants.config.ModelEstimatorConfig.LogisticRegression" title="Permanent link">&para;</a></h3>

    <div class="doc doc-contents ">

      <p>Logistic Regression (aka logit, MaxEnt) classifier.</p>
<p>In the multiclass case, the training algorithm uses the one-vs-rest (OvR)
scheme if the 'multi_class' option is set to 'ovr', and uses the
cross-entropy loss if the 'multi_class' option is set to 'multinomial'.
(Currently the 'multinomial' option is supported only by the 'lbfgs',
'sag', 'saga' and 'newton-cg' solvers.)</p>
<p>This class implements regularized logistic regression using the
'liblinear' library, 'newton-cg', 'sag', 'saga' and 'lbfgs' solvers. <strong>Note
that regularization is applied by default</strong>. It can handle both dense
and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit
floats for optimal performance; any other input format will be converted
(and copied).</p>
<p>The 'newton-cg', 'sag', and 'lbfgs' solvers support only L2 regularization
with primal formulation, or no regularization. The 'liblinear' solver
supports both L1 and L2 regularization, with a dual formulation only for
the L2 penalty. The Elastic-Net regularization is only supported by the
'saga' solver.</p>
<p>Read more in the :ref:<code>User Guide &lt;logistic_regression&gt;</code>.</p>
<h5 id="giants.config.ModelEstimatorConfig.LogisticRegression--parameters">Parameters<a class="headerlink" href="#giants.config.ModelEstimatorConfig.LogisticRegression--parameters" title="Permanent link">&para;</a></h5>
<p>penalty : {'l1', 'l2', 'elasticnet', 'none'}, default='l2'
    Specify the norm of the penalty:</p>
<div class="codehilite"><pre><span></span><code>- `&#39;none&#39;`: no penalty is added;
- `&#39;l2&#39;`: add a L2 penalty term and it is the default choice;
- `&#39;l1&#39;`: add a L1 penalty term;
- `&#39;elasticnet&#39;`: both L1 and L2 penalty terms are added.

.. warning::
   Some penalties may not work with some solvers. See the parameter
   `solver` below, to know the compatibility between the penalty and
   solver.

.. versionadded:: 0.19
   l1 penalty with SAGA solver (allowing &#39;multinomial&#39; + L1)
</code></pre></div>

<p>dual : bool, default=False
    Dual or primal formulation. Dual formulation is only implemented for
    l2 penalty with liblinear solver. Prefer dual=False when
    n_samples &gt; n_features.</p>
<p>tol : float, default=1e-4
    Tolerance for stopping criteria.</p>
<p>C : float, default=1.0
    Inverse of regularization strength; must be a positive float.
    Like in support vector machines, smaller values specify stronger
    regularization.</p>
<p>fit_intercept : bool, default=True
    Specifies if a constant (a.k.a. bias or intercept) should be
    added to the decision function.</p>
<p>intercept_scaling : float, default=1
    Useful only when the solver 'liblinear' is used
    and self.fit_intercept is set to True. In this case, x becomes
    [x, self.intercept_scaling],
    i.e. a "synthetic" feature with constant value equal to
    intercept_scaling is appended to the instance vector.
    The intercept becomes <code>intercept_scaling * synthetic_feature_weight</code>.</p>
<div class="codehilite"><pre><span></span><code>Note! the synthetic feature weight is subject to l1/l2 regularization
as all other features.
To lessen the effect of regularization on synthetic feature weight
(and therefore on the intercept) intercept_scaling has to be increased.
</code></pre></div>

<p>class_weight : dict or 'balanced', default=None
    Weights associated with classes in the form <code>{class_label: weight}</code>.
    If not given, all classes are supposed to have weight one.</p>
<div class="codehilite"><pre><span></span><code>The &quot;balanced&quot; mode uses the values of y to automatically adjust
weights inversely proportional to class frequencies in the input data
as ``n_samples / (n_classes * np.bincount(y))``.

Note that these weights will be multiplied with sample_weight (passed
through the fit method) if sample_weight is specified.

.. versionadded:: 0.17
   *class_weight=&#39;balanced&#39;*
</code></pre></div>

<p>random_state : int, RandomState instance, default=None
    Used when <code>solver</code> == 'sag', 'saga' or 'liblinear' to shuffle the
    data. See :term:<code>Glossary &lt;random_state&gt;</code> for details.</p>
<p>solver : {'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'},             default='lbfgs'</p>
<div class="codehilite"><pre><span></span><code>Algorithm to use in the optimization problem. Default is &#39;lbfgs&#39;.
To choose a solver, you might want to consider the following aspects:

    - For small datasets, &#39;liblinear&#39; is a good choice, whereas &#39;sag&#39;
      and &#39;saga&#39; are faster for large ones;
    - For multiclass problems, only &#39;newton-cg&#39;, &#39;sag&#39;, &#39;saga&#39; and
      &#39;lbfgs&#39; handle multinomial loss;
    - &#39;liblinear&#39; is limited to one-versus-rest schemes.

.. warning::
   The choice of the algorithm depends on the penalty chosen:
   Supported penalties by solver:

   - &#39;newton-cg&#39;   -   [&#39;l2&#39;, &#39;none&#39;]
   - &#39;lbfgs&#39;       -   [&#39;l2&#39;, &#39;none&#39;]
   - &#39;liblinear&#39;   -   [&#39;l1&#39;, &#39;l2&#39;]
   - &#39;sag&#39;         -   [&#39;l2&#39;, &#39;none&#39;]
   - &#39;saga&#39;        -   [&#39;elasticnet&#39;, &#39;l1&#39;, &#39;l2&#39;, &#39;none&#39;]

.. note::
   &#39;sag&#39; and &#39;saga&#39; fast convergence is only guaranteed on
   features with approximately the same scale. You can
   preprocess the data with a scaler from :mod:`sklearn.preprocessing`.

.. seealso::
   Refer to the User Guide for more information regarding
   :class:`LogisticRegression` and more specifically the
   `Table &lt;https://scikit-learn.org/dev/modules/linear_model.html#logistic-regression&gt;`_
   summarazing solver/penalty supports.
   &lt;!--
   # noqa: E501
   --&gt;

.. versionadded:: 0.17
   Stochastic Average Gradient descent solver.
.. versionadded:: 0.19
   SAGA solver.
.. versionchanged:: 0.22
    The default solver changed from &#39;liblinear&#39; to &#39;lbfgs&#39; in 0.22.
</code></pre></div>

<p>max_iter : int, default=100
    Maximum number of iterations taken for the solvers to converge.</p>
<p>multi_class : {'auto', 'ovr', 'multinomial'}, default='auto'
    If the option chosen is 'ovr', then a binary problem is fit for each
    label. For 'multinomial' the loss minimised is the multinomial loss fit
    across the entire probability distribution, <em>even when the data is
    binary</em>. 'multinomial' is unavailable when solver='liblinear'.
    'auto' selects 'ovr' if the data is binary, or if solver='liblinear',
    and otherwise selects 'multinomial'.</p>
<div class="codehilite"><pre><span></span><code>.. versionadded:: 0.18
   Stochastic Average Gradient descent solver for &#39;multinomial&#39; case.
.. versionchanged:: 0.22
    Default changed from &#39;ovr&#39; to &#39;auto&#39; in 0.22.
</code></pre></div>

<p>verbose : int, default=0
    For the liblinear and lbfgs solvers set verbose to any positive
    number for verbosity.</p>
<p>warm_start : bool, default=False
    When set to True, reuse the solution of the previous call to fit as
    initialization, otherwise, just erase the previous solution.
    Useless for liblinear solver. See :term:<code>the Glossary &lt;warm_start&gt;</code>.</p>
<div class="codehilite"><pre><span></span><code>.. versionadded:: 0.17
   *warm_start* to support *lbfgs*, *newton-cg*, *sag*, *saga* solvers.
</code></pre></div>

<p>n_jobs : int, default=None
    Number of CPU cores used when parallelizing over classes if
    multi_class='ovr'". This parameter is ignored when the <code>solver</code> is
    set to 'liblinear' regardless of whether 'multi_class' is specified or
    not. <code>None</code> means 1 unless in a :obj:<code>joblib.parallel_backend</code>
    context. <code>-1</code> means using all processors.
    See :term:<code>Glossary &lt;n_jobs&gt;</code> for more details.</p>
<p>l1_ratio : float, default=None
    The Elastic-Net mixing parameter, with <code>0 &lt;= l1_ratio &lt;= 1</code>. Only
    used if <code>penalty='elasticnet'</code>. Setting <code>l1_ratio=0</code> is equivalent
    to using <code>penalty='l2'</code>, while setting <code>l1_ratio=1</code> is equivalent
    to using <code>penalty='l1'</code>. For <code>0 &lt; l1_ratio &lt;1</code>, the penalty is a
    combination of L1 and L2.</p>
<h5 id="giants.config.ModelEstimatorConfig.LogisticRegression--attributes">Attributes<a class="headerlink" href="#giants.config.ModelEstimatorConfig.LogisticRegression--attributes" title="Permanent link">&para;</a></h5>
<p>classes_ : ndarray of shape (n_classes, )
    A list of class labels known to the classifier.</p>
<p>coef_ : ndarray of shape (1, n_features) or (n_classes, n_features)
    Coefficient of the features in the decision function.</p>
<div class="codehilite"><pre><span></span><code>`coef_` is of shape (1, n_features) when the given problem is binary.
In particular, when `multi_class=&#39;multinomial&#39;`, `coef_` corresponds
to outcome 1 (True) and `-coef_` corresponds to outcome 0 (False).
</code></pre></div>

<p>intercept_ : ndarray of shape (1,) or (n_classes,)
    Intercept (a.k.a. bias) added to the decision function.</p>
<div class="codehilite"><pre><span></span><code>If `fit_intercept` is set to False, the intercept is set to zero.
`intercept_` is of shape (1,) when the given problem is binary.
In particular, when `multi_class=&#39;multinomial&#39;`, `intercept_`
corresponds to outcome 1 (True) and `-intercept_` corresponds to
outcome 0 (False).
</code></pre></div>

<p>n_features_in_ : int
    Number of features seen during :term:<code>fit</code>.</p>
<div class="codehilite"><pre><span></span><code>.. versionadded:: 0.24
</code></pre></div>

<p>feature_names_in_ : ndarray of shape (<code>n_features_in_</code>,)
    Names of features seen during :term:<code>fit</code>. Defined only when <code>X</code>
    has feature names that are all strings.</p>
<div class="codehilite"><pre><span></span><code>.. versionadded:: 1.0
</code></pre></div>

<p>n_iter_ : ndarray of shape (n_classes,) or (1, )
    Actual number of iterations for all classes. If binary or multinomial,
    it returns only 1 element. For liblinear solver, only the maximum
    number of iteration across all classes is given.</p>
<div class="codehilite"><pre><span></span><code>.. versionchanged:: 0.20

    In SciPy &lt;= 1.0.0 the number of lbfgs iterations may exceed
    ``max_iter``. ``n_iter_`` will now report at most ``max_iter``.
</code></pre></div>

<h5 id="giants.config.ModelEstimatorConfig.LogisticRegression--see-also">See Also<a class="headerlink" href="#giants.config.ModelEstimatorConfig.LogisticRegression--see-also" title="Permanent link">&para;</a></h5>
<p>SGDClassifier : Incrementally trained logistic regression (when given
    the parameter <code>loss="log"</code>).
LogisticRegressionCV : Logistic regression with built-in cross validation.</p>
<h5 id="giants.config.ModelEstimatorConfig.LogisticRegression--notes">Notes<a class="headerlink" href="#giants.config.ModelEstimatorConfig.LogisticRegression--notes" title="Permanent link">&para;</a></h5>
<p>The underlying C implementation uses a random number generator to
select features when fitting the model. It is thus not uncommon,
to have slightly different results for the same input data. If
that happens, try with a smaller tol parameter.</p>
<p>Predict output may not match that of standalone liblinear in certain
cases. See :ref:<code>differences from liblinear &lt;liblinear_differences&gt;</code>
in the narrative documentation.</p>
<h5 id="giants.config.ModelEstimatorConfig.LogisticRegression--references">References<a class="headerlink" href="#giants.config.ModelEstimatorConfig.LogisticRegression--references" title="Permanent link">&para;</a></h5>
<p>L-BFGS-B -- Software for Large-scale Bound-constrained Optimization
    Ciyou Zhu, Richard Byrd, Jorge Nocedal and Jose Luis Morales.
    <a href="http://users.iems.northwestern.edu/~nocedal/lbfgsb.html">users.iems.northwestern.edu/~nocedal/lbfgsb.html</a></p>
<p>LIBLINEAR -- A Library for Large Linear Classification
    <a href="https://www.csie.ntu.edu.tw/~cjlin/liblinear/">www.csie.ntu.edu.tw/~cjlin/liblinear/</a></p>
<p>SAG -- Mark Schmidt, Nicolas Le Roux, and Francis Bach
    Minimizing Finite Sums with the Stochastic Average Gradient
    <a href="https://hal.inria.fr/hal-00860051/document">hal.inria.fr/hal-00860051/document</a></p>
<p>SAGA -- Defazio, A., Bach F. &amp; Lacoste-Julien S. (2014).
    SAGA: A Fast Incremental Gradient Method With Support
    for Non-Strongly Convex Composite Objectives
    <a href="https://arxiv.org/abs/1407.0202">arxiv.org/abs/1407.0202</a></p>
<p>Hsiang-Fu Yu, Fang-Lan Huang, Chih-Jen Lin (2011). Dual coordinate descent
    methods for logistic regression and maximum entropy models.
    Machine Learning 85(1-2):41-75.
    <a href="https://www.csie.ntu.edu.tw/~cjlin/papers/maxent_dual.pdf">www.csie.ntu.edu.tw/~cjlin/papers/maxent_dual.pdf</a></p>
<h5 id="giants.config.ModelEstimatorConfig.LogisticRegression--examples">Examples<a class="headerlink" href="#giants.config.ModelEstimatorConfig.LogisticRegression--examples" title="Permanent link">&para;</a></h5>
<blockquote>
<blockquote>
<blockquote>
<p>from sklearn.datasets import load_iris
from sklearn.linear_model import LogisticRegression
X, y = load_iris(return_X_y=True)
clf = LogisticRegression(random_state=0).fit(X, y)
clf.predict(X[:2, :])
array([0, 0])
clf.predict_proba(X[:2, :])
array([[9.8...e-01, 1.8...e-02, 1.4...e-08],
       [9.7...e-01, 2.8...e-02, ...e-08]])
clf.score(X, y)
0.97...</p>
</blockquote>
</blockquote>
</blockquote>




  <div class="doc doc-children">










  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="giants.config.ModelEstimatorConfig.LogisticRegression.fit">
<code class="codehilite language-python"><span class="n">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>


<a class="headerlink" href="#giants.config.ModelEstimatorConfig.LogisticRegression.fit" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Fit the model according to the given training data.</p>
<h6 id="giants.config.ModelEstimatorConfig.LogisticRegression.fit--parameters">Parameters<a class="headerlink" href="#giants.config.ModelEstimatorConfig.LogisticRegression.fit--parameters" title="Permanent link">&para;</a></h6>
<p>X : {array-like, sparse matrix} of shape (n_samples, n_features)
    Training vector, where <code>n_samples</code> is the number of samples and
    <code>n_features</code> is the number of features.</p>
<p>y : array-like of shape (n_samples,)
    Target vector relative to X.</p>
<p>sample_weight : array-like of shape (n_samples,) default=None
    Array of weights that are assigned to individual samples.
    If not provided, then each sample is given unit weight.</p>
<div class="codehilite"><pre><span></span><code>.. versionadded:: 0.17
   *sample_weight* support to LogisticRegression.
</code></pre></div>

<h6 id="giants.config.ModelEstimatorConfig.LogisticRegression.fit--returns">Returns<a class="headerlink" href="#giants.config.ModelEstimatorConfig.LogisticRegression.fit--returns" title="Permanent link">&para;</a></h6>
<p>self
    Fitted estimator.</p>
<h6 id="giants.config.ModelEstimatorConfig.LogisticRegression.fit--notes">Notes<a class="headerlink" href="#giants.config.ModelEstimatorConfig.LogisticRegression.fit--notes" title="Permanent link">&para;</a></h6>
<p>The SAGA solver supports both float64 and float32 bit arrays.</p>

        <details class="quote">
          <summary>Source code in <code>giants/config.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Fit the model according to the given training data.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">        Training vector, where `n_samples` is the number of samples and</span>
<span class="sd">        `n_features` is the number of features.</span>

<span class="sd">    y : array-like of shape (n_samples,)</span>
<span class="sd">        Target vector relative to X.</span>

<span class="sd">    sample_weight : array-like of shape (n_samples,) default=None</span>
<span class="sd">        Array of weights that are assigned to individual samples.</span>
<span class="sd">        If not provided, then each sample is given unit weight.</span>

<span class="sd">        .. versionadded:: 0.17</span>
<span class="sd">           *sample_weight* support to LogisticRegression.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    self</span>
<span class="sd">        Fitted estimator.</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    The SAGA solver supports both float64 and float32 bit arrays.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">solver</span> <span class="o">=</span> <span class="n">_check_solver</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">solver</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">penalty</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dual</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">C</span><span class="p">,</span> <span class="n">numbers</span><span class="o">.</span><span class="n">Number</span><span class="p">)</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">C</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Penalty term must be positive; got (C=</span><span class="si">%r</span><span class="s2">)&quot;</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">C</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">penalty</span> <span class="o">==</span> <span class="s2">&quot;elasticnet&quot;</span><span class="p">:</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">l1_ratio</span><span class="p">,</span> <span class="n">numbers</span><span class="o">.</span><span class="n">Number</span><span class="p">)</span>
            <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">l1_ratio</span> <span class="o">&lt;</span> <span class="mi">0</span>
            <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">l1_ratio</span> <span class="o">&gt;</span> <span class="mi">1</span>
        <span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;l1_ratio must be between 0 and 1; got (l1_ratio=</span><span class="si">%r</span><span class="s2">)&quot;</span>
                <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">l1_ratio</span>
            <span class="p">)</span>
    <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">l1_ratio</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
            <span class="s2">&quot;l1_ratio parameter is only used when penalty is &quot;</span>
            <span class="s2">&quot;&#39;elasticnet&#39;. Got &quot;</span>
            <span class="s2">&quot;(penalty=</span><span class="si">{}</span><span class="s2">)&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">penalty</span><span class="p">)</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">penalty</span> <span class="o">==</span> <span class="s2">&quot;none&quot;</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">C</span> <span class="o">!=</span> <span class="mf">1.0</span><span class="p">:</span>  <span class="c1"># default values</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;Setting penalty=&#39;none&#39; will ignore the C and l1_ratio parameters&quot;</span>
            <span class="p">)</span>
            <span class="c1"># Note that check for l1_ratio is done right above</span>
        <span class="n">C_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span>
        <span class="n">penalty</span> <span class="o">=</span> <span class="s2">&quot;l2&quot;</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">C_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">C</span>
        <span class="n">penalty</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">penalty</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">max_iter</span><span class="p">,</span> <span class="n">numbers</span><span class="o">.</span><span class="n">Number</span><span class="p">)</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_iter</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;Maximum number of iteration must be positive; got (max_iter=</span><span class="si">%r</span><span class="s2">)&quot;</span>
            <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_iter</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tol</span><span class="p">,</span> <span class="n">numbers</span><span class="o">.</span><span class="n">Number</span><span class="p">)</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">tol</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;Tolerance for stopping criteria must be positive; got (tol=</span><span class="si">%r</span><span class="s2">)&quot;</span>
            <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">tol</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">solver</span> <span class="o">==</span> <span class="s2">&quot;lbfgs&quot;</span><span class="p">:</span>
        <span class="n">_dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float64</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">_dtype</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">]</span>

    <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_data</span><span class="p">(</span>
        <span class="n">X</span><span class="p">,</span>
        <span class="n">y</span><span class="p">,</span>
        <span class="n">accept_sparse</span><span class="o">=</span><span class="s2">&quot;csr&quot;</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">_dtype</span><span class="p">,</span>
        <span class="n">order</span><span class="o">=</span><span class="s2">&quot;C&quot;</span><span class="p">,</span>
        <span class="n">accept_large_sparse</span><span class="o">=</span><span class="n">solver</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;liblinear&quot;</span><span class="p">,</span> <span class="s2">&quot;sag&quot;</span><span class="p">,</span> <span class="s2">&quot;saga&quot;</span><span class="p">],</span>
    <span class="p">)</span>
    <span class="n">check_classification_targets</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

    <span class="n">multi_class</span> <span class="o">=</span> <span class="n">_check_multi_class</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">multi_class</span><span class="p">,</span> <span class="n">solver</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">solver</span> <span class="o">==</span> <span class="s2">&quot;liblinear&quot;</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">effective_n_jobs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_jobs</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;&#39;n_jobs&#39; &gt; 1 does not have any effect when&quot;</span>
                <span class="s2">&quot; &#39;solver&#39; is set to &#39;liblinear&#39;. Got &#39;n_jobs&#39;&quot;</span>
                <span class="s2">&quot; = </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">effective_n_jobs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_jobs</span><span class="p">))</span>
            <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">intercept_</span><span class="p">,</span> <span class="n">n_iter_</span> <span class="o">=</span> <span class="n">_fit_liblinear</span><span class="p">(</span>
            <span class="n">X</span><span class="p">,</span>
            <span class="n">y</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">C</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">fit_intercept</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">intercept_scaling</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">class_weight</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">penalty</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dual</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">max_iter</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">tol</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="p">,</span>
            <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_iter_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">n_iter_</span><span class="p">])</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">if</span> <span class="n">solver</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;sag&quot;</span><span class="p">,</span> <span class="s2">&quot;saga&quot;</span><span class="p">]:</span>
        <span class="n">max_squared_sum</span> <span class="o">=</span> <span class="n">row_norms</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">squared</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">max_squared_sum</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="n">n_classes</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="p">)</span>
    <span class="n">classes_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span>
    <span class="k">if</span> <span class="n">n_classes</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;This solver needs samples of at least 2 classes&quot;</span>
            <span class="s2">&quot; in the data, but the data contains only one&quot;</span>
            <span class="s2">&quot; class: </span><span class="si">%r</span><span class="s2">&quot;</span>
            <span class="o">%</span> <span class="n">classes_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="n">n_classes</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="n">classes_</span> <span class="o">=</span> <span class="n">classes_</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">warm_start</span><span class="p">:</span>
        <span class="n">warm_start_coef</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;coef_&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">warm_start_coef</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">warm_start_coef</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">fit_intercept</span><span class="p">:</span>
        <span class="n">warm_start_coef</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
            <span class="n">warm_start_coef</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">intercept_</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span>
        <span class="p">)</span>

    <span class="c1"># Hack so that we iterate only once for the multinomial case.</span>
    <span class="k">if</span> <span class="n">multi_class</span> <span class="o">==</span> <span class="s2">&quot;multinomial&quot;</span><span class="p">:</span>
        <span class="n">classes_</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span>
        <span class="n">warm_start_coef</span> <span class="o">=</span> <span class="p">[</span><span class="n">warm_start_coef</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">warm_start_coef</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">warm_start_coef</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">n_classes</span>

    <span class="n">path_func</span> <span class="o">=</span> <span class="n">delayed</span><span class="p">(</span><span class="n">_logistic_regression_path</span><span class="p">)</span>

    <span class="c1"># The SAG solver releases the GIL so it&#39;s more efficient to use</span>
    <span class="c1"># threads for this solver.</span>
    <span class="k">if</span> <span class="n">solver</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;sag&quot;</span><span class="p">,</span> <span class="s2">&quot;saga&quot;</span><span class="p">]:</span>
        <span class="n">prefer</span> <span class="o">=</span> <span class="s2">&quot;threads&quot;</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">prefer</span> <span class="o">=</span> <span class="s2">&quot;processes&quot;</span>
    <span class="n">fold_coefs_</span> <span class="o">=</span> <span class="n">Parallel</span><span class="p">(</span>
        <span class="n">n_jobs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_jobs</span><span class="p">,</span>
        <span class="n">verbose</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">,</span>
        <span class="o">**</span><span class="n">_joblib_parallel_args</span><span class="p">(</span><span class="n">prefer</span><span class="o">=</span><span class="n">prefer</span><span class="p">),</span>
    <span class="p">)(</span>
        <span class="n">path_func</span><span class="p">(</span>
            <span class="n">X</span><span class="p">,</span>
            <span class="n">y</span><span class="p">,</span>
            <span class="n">pos_class</span><span class="o">=</span><span class="n">class_</span><span class="p">,</span>
            <span class="n">Cs</span><span class="o">=</span><span class="p">[</span><span class="n">C_</span><span class="p">],</span>
            <span class="n">l1_ratio</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">l1_ratio</span><span class="p">,</span>
            <span class="n">fit_intercept</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">fit_intercept</span><span class="p">,</span>
            <span class="n">tol</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">tol</span><span class="p">,</span>
            <span class="n">verbose</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">,</span>
            <span class="n">solver</span><span class="o">=</span><span class="n">solver</span><span class="p">,</span>
            <span class="n">multi_class</span><span class="o">=</span><span class="n">multi_class</span><span class="p">,</span>
            <span class="n">max_iter</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">max_iter</span><span class="p">,</span>
            <span class="n">class_weight</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">class_weight</span><span class="p">,</span>
            <span class="n">check_input</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">random_state</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="p">,</span>
            <span class="n">coef</span><span class="o">=</span><span class="n">warm_start_coef_</span><span class="p">,</span>
            <span class="n">penalty</span><span class="o">=</span><span class="n">penalty</span><span class="p">,</span>
            <span class="n">max_squared_sum</span><span class="o">=</span><span class="n">max_squared_sum</span><span class="p">,</span>
            <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">for</span> <span class="n">class_</span><span class="p">,</span> <span class="n">warm_start_coef_</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">classes_</span><span class="p">,</span> <span class="n">warm_start_coef</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="n">fold_coefs_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">n_iter_</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">fold_coefs_</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_iter_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">n_iter_</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)[:,</span> <span class="mi">0</span><span class="p">]</span>

    <span class="n">n_features</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">multi_class</span> <span class="o">==</span> <span class="s2">&quot;multinomial&quot;</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span> <span class="o">=</span> <span class="n">fold_coefs_</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">fold_coefs_</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
            <span class="n">n_classes</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">+</span> <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fit_intercept</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">fit_intercept</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">intercept_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">intercept_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_classes</span><span class="p">)</span>

    <span class="k">return</span> <span class="bp">self</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="giants.config.ModelEstimatorConfig.LogisticRegression.predict_log_proba">
<code class="codehilite language-python"><span class="n">predict_log_proba</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span></code>


<a class="headerlink" href="#giants.config.ModelEstimatorConfig.LogisticRegression.predict_log_proba" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Predict logarithm of probability estimates.</p>
<p>The returned estimates for all classes are ordered by the
label of classes.</p>
<h6 id="giants.config.ModelEstimatorConfig.LogisticRegression.predict_log_proba--parameters">Parameters<a class="headerlink" href="#giants.config.ModelEstimatorConfig.LogisticRegression.predict_log_proba--parameters" title="Permanent link">&para;</a></h6>
<p>X : array-like of shape (n_samples, n_features)
    Vector to be scored, where <code>n_samples</code> is the number of samples and
    <code>n_features</code> is the number of features.</p>
<h6 id="giants.config.ModelEstimatorConfig.LogisticRegression.predict_log_proba--returns">Returns<a class="headerlink" href="#giants.config.ModelEstimatorConfig.LogisticRegression.predict_log_proba--returns" title="Permanent link">&para;</a></h6>
<p>T : array-like of shape (n_samples, n_classes)
    Returns the log-probability of the sample for each class in the
    model, where classes are ordered as they are in <code>self.classes_</code>.</p>

        <details class="quote">
          <summary>Source code in <code>giants/config.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">predict_log_proba</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Predict logarithm of probability estimates.</span>

<span class="sd">    The returned estimates for all classes are ordered by the</span>
<span class="sd">    label of classes.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : array-like of shape (n_samples, n_features)</span>
<span class="sd">        Vector to be scored, where `n_samples` is the number of samples and</span>
<span class="sd">        `n_features` is the number of features.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    T : array-like of shape (n_samples, n_classes)</span>
<span class="sd">        Returns the log-probability of the sample for each class in the</span>
<span class="sd">        model, where classes are ordered as they are in ``self.classes_``.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="giants.config.ModelEstimatorConfig.LogisticRegression.predict_proba">
<code class="codehilite language-python"><span class="n">predict_proba</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span></code>


<a class="headerlink" href="#giants.config.ModelEstimatorConfig.LogisticRegression.predict_proba" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Probability estimates.</p>
<p>The returned estimates for all classes are ordered by the
label of classes.</p>
<p>For a multi_class problem, if multi_class is set to be "multinomial"
the softmax function is used to find the predicted probability of
each class.
Else use a one-vs-rest approach, i.e calculate the probability
of each class assuming it to be positive using the logistic function.
and normalize these values across all the classes.</p>
<h6 id="giants.config.ModelEstimatorConfig.LogisticRegression.predict_proba--parameters">Parameters<a class="headerlink" href="#giants.config.ModelEstimatorConfig.LogisticRegression.predict_proba--parameters" title="Permanent link">&para;</a></h6>
<p>X : array-like of shape (n_samples, n_features)
    Vector to be scored, where <code>n_samples</code> is the number of samples and
    <code>n_features</code> is the number of features.</p>
<h6 id="giants.config.ModelEstimatorConfig.LogisticRegression.predict_proba--returns">Returns<a class="headerlink" href="#giants.config.ModelEstimatorConfig.LogisticRegression.predict_proba--returns" title="Permanent link">&para;</a></h6>
<p>T : array-like of shape (n_samples, n_classes)
    Returns the probability of the sample for each class in the model,
    where classes are ordered as they are in <code>self.classes_</code>.</p>

        <details class="quote">
          <summary>Source code in <code>giants/config.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">predict_proba</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Probability estimates.</span>

<span class="sd">    The returned estimates for all classes are ordered by the</span>
<span class="sd">    label of classes.</span>

<span class="sd">    For a multi_class problem, if multi_class is set to be &quot;multinomial&quot;</span>
<span class="sd">    the softmax function is used to find the predicted probability of</span>
<span class="sd">    each class.</span>
<span class="sd">    Else use a one-vs-rest approach, i.e calculate the probability</span>
<span class="sd">    of each class assuming it to be positive using the logistic function.</span>
<span class="sd">    and normalize these values across all the classes.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : array-like of shape (n_samples, n_features)</span>
<span class="sd">        Vector to be scored, where `n_samples` is the number of samples and</span>
<span class="sd">        `n_features` is the number of features.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    T : array-like of shape (n_samples, n_classes)</span>
<span class="sd">        Returns the probability of the sample for each class in the model,</span>
<span class="sd">        where classes are ordered as they are in ``self.classes_``.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

    <span class="n">ovr</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">multi_class</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;ovr&quot;</span><span class="p">,</span> <span class="s2">&quot;warn&quot;</span><span class="p">]</span> <span class="ow">or</span> <span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">multi_class</span> <span class="o">==</span> <span class="s2">&quot;auto&quot;</span>
        <span class="ow">and</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="o">.</span><span class="n">size</span> <span class="o">&lt;=</span> <span class="mi">2</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">solver</span> <span class="o">==</span> <span class="s2">&quot;liblinear&quot;</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="n">ovr</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_predict_proba_lr</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">decision</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decision_function</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">decision</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="c1"># Workaround for multi_class=&quot;multinomial&quot; and binary outcomes</span>
            <span class="c1"># which requires softmax prediction with only a 1D decision.</span>
            <span class="n">decision_2d</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="o">-</span><span class="n">decision</span><span class="p">,</span> <span class="n">decision</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">decision_2d</span> <span class="o">=</span> <span class="n">decision</span>
        <span class="k">return</span> <span class="n">softmax</span><span class="p">(</span><span class="n">decision_2d</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h3 class="doc doc-heading" id="giants.config.ModelEstimatorConfig.RandomForestClassifier">
        <code>
RandomForestClassifier            (<span title="sklearn.ensemble._forest.ForestClassifier">ForestClassifier</span>)
        </code>



<a class="headerlink" href="#giants.config.ModelEstimatorConfig.RandomForestClassifier" title="Permanent link">&para;</a></h3>

    <div class="doc doc-contents ">

      <p>A random forest classifier.</p>
<p>A random forest is a meta estimator that fits a number of decision tree
classifiers on various sub-samples of the dataset and uses averaging to
improve the predictive accuracy and control over-fitting.
The sub-sample size is controlled with the <code>max_samples</code> parameter if
<code>bootstrap=True</code> (default), otherwise the whole dataset is used to build
each tree.</p>
<p>Read more in the :ref:<code>User Guide &lt;forest&gt;</code>.</p>
<h5 id="giants.config.ModelEstimatorConfig.RandomForestClassifier--parameters">Parameters<a class="headerlink" href="#giants.config.ModelEstimatorConfig.RandomForestClassifier--parameters" title="Permanent link">&para;</a></h5>
<p>n_estimators : int, default=100
    The number of trees in the forest.</p>
<div class="codehilite"><pre><span></span><code>.. versionchanged:: 0.22
   The default value of ``n_estimators`` changed from 10 to 100
   in 0.22.
</code></pre></div>

<p>criterion : {"gini", "entropy"}, default="gini"
    The function to measure the quality of a split. Supported criteria are
    "gini" for the Gini impurity and "entropy" for the information gain.
    Note: this parameter is tree-specific.</p>
<p>max_depth : int, default=None
    The maximum depth of the tree. If None, then nodes are expanded until
    all leaves are pure or until all leaves contain less than
    min_samples_split samples.</p>
<p>min_samples_split : int or float, default=2
    The minimum number of samples required to split an internal node:</p>
<div class="codehilite"><pre><span></span><code>- If int, then consider `min_samples_split` as the minimum number.
- If float, then `min_samples_split` is a fraction and
  `ceil(min_samples_split * n_samples)` are the minimum
  number of samples for each split.

.. versionchanged:: 0.18
   Added float values for fractions.
</code></pre></div>

<p>min_samples_leaf : int or float, default=1
    The minimum number of samples required to be at a leaf node.
    A split point at any depth will only be considered if it leaves at
    least <code>min_samples_leaf</code> training samples in each of the left and
    right branches.  This may have the effect of smoothing the model,
    especially in regression.</p>
<div class="codehilite"><pre><span></span><code>- If int, then consider `min_samples_leaf` as the minimum number.
- If float, then `min_samples_leaf` is a fraction and
  `ceil(min_samples_leaf * n_samples)` are the minimum
  number of samples for each node.

.. versionchanged:: 0.18
   Added float values for fractions.
</code></pre></div>

<p>min_weight_fraction_leaf : float, default=0.0
    The minimum weighted fraction of the sum total of weights (of all
    the input samples) required to be at a leaf node. Samples have
    equal weight when sample_weight is not provided.</p>
<p>max_features : {"auto", "sqrt", "log2"}, int or float, default="auto"
    The number of features to consider when looking for the best split:</p>
<div class="codehilite"><pre><span></span><code>- If int, then consider `max_features` features at each split.
- If float, then `max_features` is a fraction and
  `round(max_features * n_features)` features are considered at each
  split.
- If &quot;auto&quot;, then `max_features=sqrt(n_features)`.
- If &quot;sqrt&quot;, then `max_features=sqrt(n_features)` (same as &quot;auto&quot;).
- If &quot;log2&quot;, then `max_features=log2(n_features)`.
- If None, then `max_features=n_features`.

Note: the search for a split does not stop until at least one
valid partition of the node samples is found, even if it requires to
effectively inspect more than ``max_features`` features.
</code></pre></div>

<p>max_leaf_nodes : int, default=None
    Grow trees with <code>max_leaf_nodes</code> in best-first fashion.
    Best nodes are defined as relative reduction in impurity.
    If None then unlimited number of leaf nodes.</p>
<p>min_impurity_decrease : float, default=0.0
    A node will be split if this split induces a decrease of the impurity
    greater than or equal to this value.</p>
<div class="codehilite"><pre><span></span><code>The weighted impurity decrease equation is the following::

    N_t / N * (impurity - N_t_R / N_t * right_impurity
                        - N_t_L / N_t * left_impurity)

where ``N`` is the total number of samples, ``N_t`` is the number of
samples at the current node, ``N_t_L`` is the number of samples in the
left child, and ``N_t_R`` is the number of samples in the right child.

``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,
if ``sample_weight`` is passed.

.. versionadded:: 0.19
</code></pre></div>

<p>bootstrap : bool, default=True
    Whether bootstrap samples are used when building trees. If False, the
    whole dataset is used to build each tree.</p>
<p>oob_score : bool, default=False
    Whether to use out-of-bag samples to estimate the generalization score.
    Only available if bootstrap=True.</p>
<p>n_jobs : int, default=None
    The number of jobs to run in parallel. :meth:<code>fit</code>, :meth:<code>predict</code>,
    :meth:<code>decision_path</code> and :meth:<code>apply</code> are all parallelized over the
    trees. <code>None</code> means 1 unless in a :obj:<code>joblib.parallel_backend</code>
    context. <code>-1</code> means using all processors. See :term:<code>Glossary
    &lt;n_jobs&gt;</code> for more details.</p>
<p>random_state : int, RandomState instance or None, default=None
    Controls both the randomness of the bootstrapping of the samples used
    when building trees (if <code>bootstrap=True</code>) and the sampling of the
    features to consider when looking for the best split at each node
    (if <code>max_features &lt; n_features</code>).
    See :term:<code>Glossary &lt;random_state&gt;</code> for details.</p>
<p>verbose : int, default=0
    Controls the verbosity when fitting and predicting.</p>
<p>warm_start : bool, default=False
    When set to <code>True</code>, reuse the solution of the previous call to fit
    and add more estimators to the ensemble, otherwise, just fit a whole
    new forest. See :term:<code>the Glossary &lt;warm_start&gt;</code>.</p>
<p>class_weight : {"balanced", "balanced_subsample"}, dict or list of dicts,             default=None
    Weights associated with classes in the form <code>{class_label: weight}</code>.
    If not given, all classes are supposed to have weight one. For
    multi-output problems, a list of dicts can be provided in the same
    order as the columns of y.</p>
<div class="codehilite"><pre><span></span><code>Note that for multioutput (including multilabel) weights should be
defined for each class of every column in its own dict. For example,
for four-class multilabel classification weights should be
[{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of
[{1:1}, {2:5}, {3:1}, {4:1}].

The &quot;balanced&quot; mode uses the values of y to automatically adjust
weights inversely proportional to class frequencies in the input data
as ``n_samples / (n_classes * np.bincount(y))``

The &quot;balanced_subsample&quot; mode is the same as &quot;balanced&quot; except that
weights are computed based on the bootstrap sample for every tree
grown.

For multi-output, the weights of each column of y will be multiplied.

Note that these weights will be multiplied with sample_weight (passed
through the fit method) if sample_weight is specified.
</code></pre></div>

<p>ccp_alpha : non-negative float, default=0.0
    Complexity parameter used for Minimal Cost-Complexity Pruning. The
    subtree with the largest cost complexity that is smaller than
    <code>ccp_alpha</code> will be chosen. By default, no pruning is performed. See
    :ref:<code>minimal_cost_complexity_pruning</code> for details.</p>
<div class="codehilite"><pre><span></span><code>.. versionadded:: 0.22
</code></pre></div>

<p>max_samples : int or float, default=None
    If bootstrap is True, the number of samples to draw from X
    to train each base estimator.</p>
<div class="codehilite"><pre><span></span><code>- If None (default), then draw `X.shape[0]` samples.
- If int, then draw `max_samples` samples.
- If float, then draw `max_samples * X.shape[0]` samples. Thus,
  `max_samples` should be in the interval `(0.0, 1.0]`.

.. versionadded:: 0.22
</code></pre></div>

<h5 id="giants.config.ModelEstimatorConfig.RandomForestClassifier--attributes">Attributes<a class="headerlink" href="#giants.config.ModelEstimatorConfig.RandomForestClassifier--attributes" title="Permanent link">&para;</a></h5>
<p>base_estimator_ : DecisionTreeClassifier
    The child estimator template used to create the collection of fitted
    sub-estimators.</p>
<p>estimators_ : list of DecisionTreeClassifier
    The collection of fitted sub-estimators.</p>
<p>classes_ : ndarray of shape (n_classes,) or a list of such arrays
    The classes labels (single output problem), or a list of arrays of
    class labels (multi-output problem).</p>
<p>n_classes_ : int or list
    The number of classes (single output problem), or a list containing the
    number of classes for each output (multi-output problem).</p>
<p>n_features_ : int
    The number of features when <code>fit</code> is performed.</p>
<div class="codehilite"><pre><span></span><code>.. deprecated:: 1.0
    Attribute `n_features_` was deprecated in version 1.0 and will be
    removed in 1.2. Use `n_features_in_` instead.
</code></pre></div>

<p>n_features_in_ : int
    Number of features seen during :term:<code>fit</code>.</p>
<div class="codehilite"><pre><span></span><code>.. versionadded:: 0.24
</code></pre></div>

<p>feature_names_in_ : ndarray of shape (<code>n_features_in_</code>,)
    Names of features seen during :term:<code>fit</code>. Defined only when <code>X</code>
    has feature names that are all strings.
    .. versionadded:: 1.0</p>
<p>n_outputs_ : int
    The number of outputs when <code>fit</code> is performed.</p>
<p>feature_importances_ : ndarray of shape (n_features,)
    The impurity-based feature importances.
    The higher, the more important the feature.
    The importance of a feature is computed as the (normalized)
    total reduction of the criterion brought by that feature.  It is also
    known as the Gini importance.</p>
<div class="codehilite"><pre><span></span><code>Warning: impurity-based feature importances can be misleading for
high cardinality features (many unique values). See
:func:`sklearn.inspection.permutation_importance` as an alternative.
</code></pre></div>

<p>oob_score_ : float
    Score of the training dataset obtained using an out-of-bag estimate.
    This attribute exists only when <code>oob_score</code> is True.</p>
<p>oob_decision_function_ : ndarray of shape (n_samples, n_classes) or             (n_samples, n_classes, n_outputs)
    Decision function computed with out-of-bag estimate on the training
    set. If n_estimators is small it might be possible that a data point
    was never left out during the bootstrap. In this case,
    <code>oob_decision_function_</code> might contain NaN. This attribute exists
    only when <code>oob_score</code> is True.</p>
<h5 id="giants.config.ModelEstimatorConfig.RandomForestClassifier--see-also">See Also<a class="headerlink" href="#giants.config.ModelEstimatorConfig.RandomForestClassifier--see-also" title="Permanent link">&para;</a></h5>
<p>sklearn.tree.DecisionTreeClassifier : A decision tree classifier.
sklearn.ensemble.ExtraTreesClassifier : Ensemble of extremely randomized
    tree classifiers.</p>
<h5 id="giants.config.ModelEstimatorConfig.RandomForestClassifier--notes">Notes<a class="headerlink" href="#giants.config.ModelEstimatorConfig.RandomForestClassifier--notes" title="Permanent link">&para;</a></h5>
<p>The default values for the parameters controlling the size of the trees
(e.g. <code>max_depth</code>, <code>min_samples_leaf</code>, etc.) lead to fully grown and
unpruned trees which can potentially be very large on some data sets. To
reduce memory consumption, the complexity and size of the trees should be
controlled by setting those parameter values.</p>
<p>The features are always randomly permuted at each split. Therefore,
the best found split may vary, even with the same training data,
<code>max_features=n_features</code> and <code>bootstrap=False</code>, if the improvement
of the criterion is identical for several splits enumerated during the
search of the best split. To obtain a deterministic behaviour during
fitting, <code>random_state</code> has to be fixed.</p>
<h5 id="giants.config.ModelEstimatorConfig.RandomForestClassifier--references">References<a class="headerlink" href="#giants.config.ModelEstimatorConfig.RandomForestClassifier--references" title="Permanent link">&para;</a></h5>
<p>.. [1] L. Breiman, "Random Forests", Machine Learning, 45(1), 5-32, 2001.</p>
<h5 id="giants.config.ModelEstimatorConfig.RandomForestClassifier--examples">Examples<a class="headerlink" href="#giants.config.ModelEstimatorConfig.RandomForestClassifier--examples" title="Permanent link">&para;</a></h5>
<blockquote>
<blockquote>
<blockquote>
<p>from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import make_classification
X, y = make_classification(n_samples=1000, n_features=4,
...                            n_informative=2, n_redundant=0,
...                            random_state=0, shuffle=False)
clf = RandomForestClassifier(max_depth=2, random_state=0)
clf.fit(X, y)
RandomForestClassifier(...)
print(clf.predict([[0, 0, 0, 0]]))
[1]</p>
</blockquote>
</blockquote>
</blockquote>




  <div class="doc doc-children">












  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h3 class="doc doc-heading" id="giants.config.ModelEstimatorConfig.RandomForestRegressor">
        <code>
RandomForestRegressor            (<span title="sklearn.ensemble._forest.ForestRegressor">ForestRegressor</span>)
        </code>



<a class="headerlink" href="#giants.config.ModelEstimatorConfig.RandomForestRegressor" title="Permanent link">&para;</a></h3>

    <div class="doc doc-contents ">

      <p>A random forest regressor.</p>
<p>A random forest is a meta estimator that fits a number of classifying
decision trees on various sub-samples of the dataset and uses averaging
to improve the predictive accuracy and control over-fitting.
The sub-sample size is controlled with the <code>max_samples</code> parameter if
<code>bootstrap=True</code> (default), otherwise the whole dataset is used to build
each tree.</p>
<p>Read more in the :ref:<code>User Guide &lt;forest&gt;</code>.</p>
<h5 id="giants.config.ModelEstimatorConfig.RandomForestRegressor--parameters">Parameters<a class="headerlink" href="#giants.config.ModelEstimatorConfig.RandomForestRegressor--parameters" title="Permanent link">&para;</a></h5>
<p>n_estimators : int, default=100
    The number of trees in the forest.</p>
<div class="codehilite"><pre><span></span><code>.. versionchanged:: 0.22
   The default value of ``n_estimators`` changed from 10 to 100
   in 0.22.
</code></pre></div>

<p>criterion : {"squared_error", "absolute_error", "poisson"},             default="squared_error"
    The function to measure the quality of a split. Supported criteria
    are "squared_error" for the mean squared error, which is equal to
    variance reduction as feature selection criterion, "absolute_error"
    for the mean absolute error, and "poisson" which uses reduction in
    Poisson deviance to find splits.
    Training using "absolute_error" is significantly slower
    than when using "squared_error".</p>
<div class="codehilite"><pre><span></span><code>.. versionadded:: 0.18
   Mean Absolute Error (MAE) criterion.

.. versionadded:: 1.0
   Poisson criterion.

.. deprecated:: 1.0
    Criterion &quot;mse&quot; was deprecated in v1.0 and will be removed in
    version 1.2. Use `criterion=&quot;squared_error&quot;` which is equivalent.

.. deprecated:: 1.0
    Criterion &quot;mae&quot; was deprecated in v1.0 and will be removed in
    version 1.2. Use `criterion=&quot;absolute_error&quot;` which is equivalent.
</code></pre></div>

<p>max_depth : int, default=None
    The maximum depth of the tree. If None, then nodes are expanded until
    all leaves are pure or until all leaves contain less than
    min_samples_split samples.</p>
<p>min_samples_split : int or float, default=2
    The minimum number of samples required to split an internal node:</p>
<div class="codehilite"><pre><span></span><code>- If int, then consider `min_samples_split` as the minimum number.
- If float, then `min_samples_split` is a fraction and
  `ceil(min_samples_split * n_samples)` are the minimum
  number of samples for each split.

.. versionchanged:: 0.18
   Added float values for fractions.
</code></pre></div>

<p>min_samples_leaf : int or float, default=1
    The minimum number of samples required to be at a leaf node.
    A split point at any depth will only be considered if it leaves at
    least <code>min_samples_leaf</code> training samples in each of the left and
    right branches.  This may have the effect of smoothing the model,
    especially in regression.</p>
<div class="codehilite"><pre><span></span><code>- If int, then consider `min_samples_leaf` as the minimum number.
- If float, then `min_samples_leaf` is a fraction and
  `ceil(min_samples_leaf * n_samples)` are the minimum
  number of samples for each node.

.. versionchanged:: 0.18
   Added float values for fractions.
</code></pre></div>

<p>min_weight_fraction_leaf : float, default=0.0
    The minimum weighted fraction of the sum total of weights (of all
    the input samples) required to be at a leaf node. Samples have
    equal weight when sample_weight is not provided.</p>
<p>max_features : {"auto", "sqrt", "log2"}, int or float, default="auto"
    The number of features to consider when looking for the best split:</p>
<div class="codehilite"><pre><span></span><code>- If int, then consider `max_features` features at each split.
- If float, then `max_features` is a fraction and
  `round(max_features * n_features)` features are considered at each
  split.
- If &quot;auto&quot;, then `max_features=n_features`.
- If &quot;sqrt&quot;, then `max_features=sqrt(n_features)`.
- If &quot;log2&quot;, then `max_features=log2(n_features)`.
- If None, then `max_features=n_features`.

Note: the search for a split does not stop until at least one
valid partition of the node samples is found, even if it requires to
effectively inspect more than ``max_features`` features.
</code></pre></div>

<p>max_leaf_nodes : int, default=None
    Grow trees with <code>max_leaf_nodes</code> in best-first fashion.
    Best nodes are defined as relative reduction in impurity.
    If None then unlimited number of leaf nodes.</p>
<p>min_impurity_decrease : float, default=0.0
    A node will be split if this split induces a decrease of the impurity
    greater than or equal to this value.</p>
<div class="codehilite"><pre><span></span><code>The weighted impurity decrease equation is the following::

    N_t / N * (impurity - N_t_R / N_t * right_impurity
                        - N_t_L / N_t * left_impurity)

where ``N`` is the total number of samples, ``N_t`` is the number of
samples at the current node, ``N_t_L`` is the number of samples in the
left child, and ``N_t_R`` is the number of samples in the right child.

``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,
if ``sample_weight`` is passed.

.. versionadded:: 0.19
</code></pre></div>

<p>bootstrap : bool, default=True
    Whether bootstrap samples are used when building trees. If False, the
    whole dataset is used to build each tree.</p>
<p>oob_score : bool, default=False
    Whether to use out-of-bag samples to estimate the generalization score.
    Only available if bootstrap=True.</p>
<p>n_jobs : int, default=None
    The number of jobs to run in parallel. :meth:<code>fit</code>, :meth:<code>predict</code>,
    :meth:<code>decision_path</code> and :meth:<code>apply</code> are all parallelized over the
    trees. <code>None</code> means 1 unless in a :obj:<code>joblib.parallel_backend</code>
    context. <code>-1</code> means using all processors. See :term:<code>Glossary
    &lt;n_jobs&gt;</code> for more details.</p>
<p>random_state : int, RandomState instance or None, default=None
    Controls both the randomness of the bootstrapping of the samples used
    when building trees (if <code>bootstrap=True</code>) and the sampling of the
    features to consider when looking for the best split at each node
    (if <code>max_features &lt; n_features</code>).
    See :term:<code>Glossary &lt;random_state&gt;</code> for details.</p>
<p>verbose : int, default=0
    Controls the verbosity when fitting and predicting.</p>
<p>warm_start : bool, default=False
    When set to <code>True</code>, reuse the solution of the previous call to fit
    and add more estimators to the ensemble, otherwise, just fit a whole
    new forest. See :term:<code>the Glossary &lt;warm_start&gt;</code>.</p>
<p>ccp_alpha : non-negative float, default=0.0
    Complexity parameter used for Minimal Cost-Complexity Pruning. The
    subtree with the largest cost complexity that is smaller than
    <code>ccp_alpha</code> will be chosen. By default, no pruning is performed. See
    :ref:<code>minimal_cost_complexity_pruning</code> for details.</p>
<div class="codehilite"><pre><span></span><code>.. versionadded:: 0.22
</code></pre></div>

<p>max_samples : int or float, default=None
    If bootstrap is True, the number of samples to draw from X
    to train each base estimator.</p>
<div class="codehilite"><pre><span></span><code>- If None (default), then draw `X.shape[0]` samples.
- If int, then draw `max_samples` samples.
- If float, then draw `max_samples * X.shape[0]` samples. Thus,
  `max_samples` should be in the interval `(0.0, 1.0]`.

.. versionadded:: 0.22
</code></pre></div>

<h5 id="giants.config.ModelEstimatorConfig.RandomForestRegressor--attributes">Attributes<a class="headerlink" href="#giants.config.ModelEstimatorConfig.RandomForestRegressor--attributes" title="Permanent link">&para;</a></h5>
<p>base_estimator_ : DecisionTreeRegressor
    The child estimator template used to create the collection of fitted
    sub-estimators.</p>
<p>estimators_ : list of DecisionTreeRegressor
    The collection of fitted sub-estimators.</p>
<p>feature_importances_ : ndarray of shape (n_features,)
    The impurity-based feature importances.
    The higher, the more important the feature.
    The importance of a feature is computed as the (normalized)
    total reduction of the criterion brought by that feature.  It is also
    known as the Gini importance.</p>
<div class="codehilite"><pre><span></span><code>Warning: impurity-based feature importances can be misleading for
high cardinality features (many unique values). See
:func:`sklearn.inspection.permutation_importance` as an alternative.
</code></pre></div>

<p>n_features_ : int
    The number of features when <code>fit</code> is performed.</p>
<div class="codehilite"><pre><span></span><code>.. deprecated:: 1.0
    Attribute `n_features_` was deprecated in version 1.0 and will be
    removed in 1.2. Use `n_features_in_` instead.
</code></pre></div>

<p>n_features_in_ : int
    Number of features seen during :term:<code>fit</code>.</p>
<div class="codehilite"><pre><span></span><code>.. versionadded:: 0.24
</code></pre></div>

<p>feature_names_in_ : ndarray of shape (<code>n_features_in_</code>,)
    Names of features seen during :term:<code>fit</code>. Defined only when <code>X</code>
    has feature names that are all strings.
    .. versionadded:: 1.0</p>
<p>n_outputs_ : int
    The number of outputs when <code>fit</code> is performed.</p>
<p>oob_score_ : float
    Score of the training dataset obtained using an out-of-bag estimate.
    This attribute exists only when <code>oob_score</code> is True.</p>
<p>oob_prediction_ : ndarray of shape (n_samples,) or (n_samples, n_outputs)
    Prediction computed with out-of-bag estimate on the training set.
    This attribute exists only when <code>oob_score</code> is True.</p>
<h5 id="giants.config.ModelEstimatorConfig.RandomForestRegressor--see-also">See Also<a class="headerlink" href="#giants.config.ModelEstimatorConfig.RandomForestRegressor--see-also" title="Permanent link">&para;</a></h5>
<p>sklearn.tree.DecisionTreeRegressor : A decision tree regressor.
sklearn.ensemble.ExtraTreesRegressor : Ensemble of extremely randomized
    tree regressors.</p>
<h5 id="giants.config.ModelEstimatorConfig.RandomForestRegressor--notes">Notes<a class="headerlink" href="#giants.config.ModelEstimatorConfig.RandomForestRegressor--notes" title="Permanent link">&para;</a></h5>
<p>The default values for the parameters controlling the size of the trees
(e.g. <code>max_depth</code>, <code>min_samples_leaf</code>, etc.) lead to fully grown and
unpruned trees which can potentially be very large on some data sets. To
reduce memory consumption, the complexity and size of the trees should be
controlled by setting those parameter values.</p>
<p>The features are always randomly permuted at each split. Therefore,
the best found split may vary, even with the same training data,
<code>max_features=n_features</code> and <code>bootstrap=False</code>, if the improvement
of the criterion is identical for several splits enumerated during the
search of the best split. To obtain a deterministic behaviour during
fitting, <code>random_state</code> has to be fixed.</p>
<p>The default value <code>max_features="auto"</code> uses <code>n_features</code>
rather than <code>n_features / 3</code>. The latter was originally suggested in
[1], whereas the former was more recently justified empirically in [2].</p>
<h5 id="giants.config.ModelEstimatorConfig.RandomForestRegressor--references">References<a class="headerlink" href="#giants.config.ModelEstimatorConfig.RandomForestRegressor--references" title="Permanent link">&para;</a></h5>
<p>.. [1] L. Breiman, "Random Forests", Machine Learning, 45(1), 5-32, 2001.</p>
<p>.. [2] P. Geurts, D. Ernst., and L. Wehenkel, "Extremely randomized
       trees", Machine Learning, 63(1), 3-42, 2006.</p>
<h5 id="giants.config.ModelEstimatorConfig.RandomForestRegressor--examples">Examples<a class="headerlink" href="#giants.config.ModelEstimatorConfig.RandomForestRegressor--examples" title="Permanent link">&para;</a></h5>
<blockquote>
<blockquote>
<blockquote>
<p>from sklearn.ensemble import RandomForestRegressor
from sklearn.datasets import make_regression
X, y = make_regression(n_features=4, n_informative=2,
...                        random_state=0, shuffle=False)
regr = RandomForestRegressor(max_depth=2, random_state=0)
regr.fit(X, y)
RandomForestRegressor(...)
print(regr.predict([[0, 0, 0, 0]]))
[-8.32987858]</p>
</blockquote>
</blockquote>
</blockquote>




  <div class="doc doc-children">












  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h3 class="doc doc-heading" id="giants.config.ModelEstimatorConfig.SVC">
        <code>
SVC            (<span title="sklearn.svm._base.BaseSVC">BaseSVC</span>)
        </code>



<a class="headerlink" href="#giants.config.ModelEstimatorConfig.SVC" title="Permanent link">&para;</a></h3>

    <div class="doc doc-contents ">

      <p>C-Support Vector Classification.</p>
<p>The implementation is based on libsvm. The fit time scales at least
quadratically with the number of samples and may be impractical
beyond tens of thousands of samples. For large datasets
consider using :class:<code>~sklearn.svm.LinearSVC</code> or
:class:<code>~sklearn.linear_model.SGDClassifier</code> instead, possibly after a
:class:<code>~sklearn.kernel_approximation.Nystroem</code> transformer.</p>
<p>The multiclass support is handled according to a one-vs-one scheme.</p>
<p>For details on the precise mathematical formulation of the provided
kernel functions and how <code>gamma</code>, <code>coef0</code> and <code>degree</code> affect each
other, see the corresponding section in the narrative documentation:
:ref:<code>svm_kernels</code>.</p>
<p>Read more in the :ref:<code>User Guide &lt;svm_classification&gt;</code>.</p>
<h5 id="giants.config.ModelEstimatorConfig.SVC--parameters">Parameters<a class="headerlink" href="#giants.config.ModelEstimatorConfig.SVC--parameters" title="Permanent link">&para;</a></h5>
<p>C : float, default=1.0
    Regularization parameter. The strength of the regularization is
    inversely proportional to C. Must be strictly positive. The penalty
    is a squared l2 penalty.</p>
<p>kernel : {'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'}, default='rbf'
    Specifies the kernel type to be used in the algorithm.
    It must be one of 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed' or
    a callable.
    If none is given, 'rbf' will be used. If a callable is given it is
    used to pre-compute the kernel matrix from data matrices; that matrix
    should be an array of shape <code>(n_samples, n_samples)</code>.</p>
<p>degree : int, default=3
    Degree of the polynomial kernel function ('poly').
    Ignored by all other kernels.</p>
<p>gamma : {'scale', 'auto'} or float, default='scale'
    Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.</p>
<div class="codehilite"><pre><span></span><code>- if ``gamma=&#39;scale&#39;`` (default) is passed then it uses
  1 / (n_features * X.var()) as value of gamma,
- if &#39;auto&#39;, uses 1 / n_features.

.. versionchanged:: 0.22
   The default value of ``gamma`` changed from &#39;auto&#39; to &#39;scale&#39;.
</code></pre></div>

<p>coef0 : float, default=0.0
    Independent term in kernel function.
    It is only significant in 'poly' and 'sigmoid'.</p>
<p>shrinking : bool, default=True
    Whether to use the shrinking heuristic.
    See the :ref:<code>User Guide &lt;shrinking_svm&gt;</code>.</p>
<p>probability : bool, default=False
    Whether to enable probability estimates. This must be enabled prior
    to calling <code>fit</code>, will slow down that method as it internally uses
    5-fold cross-validation, and <code>predict_proba</code> may be inconsistent with
    <code>predict</code>. Read more in the :ref:<code>User Guide &lt;scores_probabilities&gt;</code>.</p>
<p>tol : float, default=1e-3
    Tolerance for stopping criterion.</p>
<p>cache_size : float, default=200
    Specify the size of the kernel cache (in MB).</p>
<p>class_weight : dict or 'balanced', default=None
    Set the parameter C of class i to class_weight[i]*C for
    SVC. If not given, all classes are supposed to have
    weight one.
    The "balanced" mode uses the values of y to automatically adjust
    weights inversely proportional to class frequencies in the input data
    as <code>n_samples / (n_classes * np.bincount(y))</code>.</p>
<p>verbose : bool, default=False
    Enable verbose output. Note that this setting takes advantage of a
    per-process runtime setting in libsvm that, if enabled, may not work
    properly in a multithreaded context.</p>
<p>max_iter : int, default=-1
    Hard limit on iterations within solver, or -1 for no limit.</p>
<p>decision_function_shape : {'ovo', 'ovr'}, default='ovr'
    Whether to return a one-vs-rest ('ovr') decision function of shape
    (n_samples, n_classes) as all other classifiers, or the original
    one-vs-one ('ovo') decision function of libsvm which has shape
    (n_samples, n_classes * (n_classes - 1) / 2). However, one-vs-one
    ('ovo') is always used as multi-class strategy. The parameter is
    ignored for binary classification.</p>
<div class="codehilite"><pre><span></span><code>.. versionchanged:: 0.19
    decision_function_shape is &#39;ovr&#39; by default.

.. versionadded:: 0.17
   *decision_function_shape=&#39;ovr&#39;* is recommended.

.. versionchanged:: 0.17
   Deprecated *decision_function_shape=&#39;ovo&#39; and None*.
</code></pre></div>

<p>break_ties : bool, default=False
    If true, <code>decision_function_shape='ovr'</code>, and number of classes &gt; 2,
    :term:<code>predict</code> will break ties according to the confidence values of
    :term:<code>decision_function</code>; otherwise the first class among the tied
    classes is returned. Please note that breaking ties comes at a
    relatively high computational cost compared to a simple predict.</p>
<div class="codehilite"><pre><span></span><code>.. versionadded:: 0.22
</code></pre></div>

<p>random_state : int, RandomState instance or None, default=None
    Controls the pseudo random number generation for shuffling the data for
    probability estimates. Ignored when <code>probability</code> is False.
    Pass an int for reproducible output across multiple function calls.
    See :term:<code>Glossary &lt;random_state&gt;</code>.</p>
<h5 id="giants.config.ModelEstimatorConfig.SVC--attributes">Attributes<a class="headerlink" href="#giants.config.ModelEstimatorConfig.SVC--attributes" title="Permanent link">&para;</a></h5>
<p>class_weight_ : ndarray of shape (n_classes,)
    Multipliers of parameter C for each class.
    Computed based on the <code>class_weight</code> parameter.</p>
<p>classes_ : ndarray of shape (n_classes,)
    The classes labels.</p>
<p>coef_ : ndarray of shape (n_classes * (n_classes - 1) / 2, n_features)
    Weights assigned to the features (coefficients in the primal
    problem). This is only available in the case of a linear kernel.</p>
<div class="codehilite"><pre><span></span><code>`coef_` is a readonly property derived from `dual_coef_` and
`support_vectors_`.
</code></pre></div>

<p>dual_coef_ : ndarray of shape (n_classes -1, n_SV)
    Dual coefficients of the support vector in the decision
    function (see :ref:<code>sgd_mathematical_formulation</code>), multiplied by
    their targets.
    For multiclass, coefficient for all 1-vs-1 classifiers.
    The layout of the coefficients in the multiclass case is somewhat
    non-trivial. See the :ref:<code>multi-class section of the User Guide
    &lt;svm_multi_class&gt;</code> for details.</p>
<p>fit_status_ : int
    0 if correctly fitted, 1 otherwise (will raise warning)</p>
<p>intercept_ : ndarray of shape (n_classes * (n_classes - 1) / 2,)
    Constants in decision function.</p>
<p>n_features_in_ : int
    Number of features seen during :term:<code>fit</code>.</p>
<div class="codehilite"><pre><span></span><code>.. versionadded:: 0.24
</code></pre></div>

<p>feature_names_in_ : ndarray of shape (<code>n_features_in_</code>,)
    Names of features seen during :term:<code>fit</code>. Defined only when <code>X</code>
    has feature names that are all strings.</p>
<div class="codehilite"><pre><span></span><code>.. versionadded:: 1.0
</code></pre></div>

<p>support_ : ndarray of shape (n_SV)
    Indices of support vectors.</p>
<p>support_vectors_ : ndarray of shape (n_SV, n_features)
    Support vectors.</p>
<p>n_support_ : ndarray of shape (n_classes,), dtype=int32
    Number of support vectors for each class.</p>
<p>probA_ : ndarray of shape (n_classes * (n_classes - 1) / 2)
probB_ : ndarray of shape (n_classes * (n_classes - 1) / 2)
    If <code>probability=True</code>, it corresponds to the parameters learned in
    Platt scaling to produce probability estimates from decision values.
    If <code>probability=False</code>, it's an empty array. Platt scaling uses the
    logistic function
    <code>1 / (1 + exp(decision_value * probA_ + probB_))</code>
    where <code>probA_</code> and <code>probB_</code> are learned from the dataset [2]<em>. For
    more information on the multiclass case and training procedure see
    section 8 of [1]</em>.</p>
<p>shape_fit_ : tuple of int of shape (n_dimensions_of_X,)
    Array dimensions of training vector <code>X</code>.</p>
<h5 id="giants.config.ModelEstimatorConfig.SVC--see-also">See Also<a class="headerlink" href="#giants.config.ModelEstimatorConfig.SVC--see-also" title="Permanent link">&para;</a></h5>
<p>SVR : Support Vector Machine for Regression implemented using libsvm.</p>
<p>LinearSVC : Scalable Linear Support Vector Machine for classification
    implemented using liblinear. Check the See Also section of
    LinearSVC for more comparison element.</p>
<h5 id="giants.config.ModelEstimatorConfig.SVC--references">References<a class="headerlink" href="#giants.config.ModelEstimatorConfig.SVC--references" title="Permanent link">&para;</a></h5>
<p>.. [1] <code>LIBSVM: A Library for Support Vector Machines
    &lt;http://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf&gt;</code>_</p>
<p>.. [2] <code>Platt, John (1999). "Probabilistic outputs for support vector
    machines and comparison to regularizedlikelihood methods."
    &lt;http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.41.1639&gt;</code>_</p>
<h5 id="giants.config.ModelEstimatorConfig.SVC--examples">Examples<a class="headerlink" href="#giants.config.ModelEstimatorConfig.SVC--examples" title="Permanent link">&para;</a></h5>
<blockquote>
<blockquote>
<blockquote>
<p>import numpy as np
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])
y = np.array([1, 1, 2, 2])
from sklearn.svm import SVC
clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))
clf.fit(X, y)
Pipeline(steps=[('standardscaler', StandardScaler()),
                ('svc', SVC(gamma='auto'))])</p>
<p>print(clf.predict([[-0.8, -1]]))
[1]</p>
</blockquote>
</blockquote>
</blockquote>




  <div class="doc doc-children">












  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h3 class="doc doc-heading" id="giants.config.ModelEstimatorConfig.SVR">
        <code>
SVR            (<span title="sklearn.base.RegressorMixin">RegressorMixin</span>, <span title="sklearn.svm._base.BaseLibSVM">BaseLibSVM</span>)
        </code>



<a class="headerlink" href="#giants.config.ModelEstimatorConfig.SVR" title="Permanent link">&para;</a></h3>

    <div class="doc doc-contents ">

      <p>Epsilon-Support Vector Regression.</p>
<p>The free parameters in the model are C and epsilon.</p>
<p>The implementation is based on libsvm. The fit time complexity
is more than quadratic with the number of samples which makes it hard
to scale to datasets with more than a couple of 10000 samples. For large
datasets consider using :class:<code>~sklearn.svm.LinearSVR</code> or
:class:<code>~sklearn.linear_model.SGDRegressor</code> instead, possibly after a
:class:<code>~sklearn.kernel_approximation.Nystroem</code> transformer.</p>
<p>Read more in the :ref:<code>User Guide &lt;svm_regression&gt;</code>.</p>
<h5 id="giants.config.ModelEstimatorConfig.SVR--parameters">Parameters<a class="headerlink" href="#giants.config.ModelEstimatorConfig.SVR--parameters" title="Permanent link">&para;</a></h5>
<p>kernel : {'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'}, default='rbf'
     Specifies the kernel type to be used in the algorithm.
     It must be one of 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed' or
     a callable.
     If none is given, 'rbf' will be used. If a callable is given it is
     used to precompute the kernel matrix.</p>
<p>degree : int, default=3
    Degree of the polynomial kernel function ('poly').
    Ignored by all other kernels.</p>
<p>gamma : {'scale', 'auto'} or float, default='scale'
    Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.</p>
<div class="codehilite"><pre><span></span><code>- if ``gamma=&#39;scale&#39;`` (default) is passed then it uses
  1 / (n_features * X.var()) as value of gamma,
- if &#39;auto&#39;, uses 1 / n_features.

.. versionchanged:: 0.22
   The default value of ``gamma`` changed from &#39;auto&#39; to &#39;scale&#39;.
</code></pre></div>

<p>coef0 : float, default=0.0
    Independent term in kernel function.
    It is only significant in 'poly' and 'sigmoid'.</p>
<p>tol : float, default=1e-3
    Tolerance for stopping criterion.</p>
<p>C : float, default=1.0
    Regularization parameter. The strength of the regularization is
    inversely proportional to C. Must be strictly positive.
    The penalty is a squared l2 penalty.</p>
<p>epsilon : float, default=0.1
     Epsilon in the epsilon-SVR model. It specifies the epsilon-tube
     within which no penalty is associated in the training loss function
     with points predicted within a distance epsilon from the actual
     value.</p>
<p>shrinking : bool, default=True
    Whether to use the shrinking heuristic.
    See the :ref:<code>User Guide &lt;shrinking_svm&gt;</code>.</p>
<p>cache_size : float, default=200
    Specify the size of the kernel cache (in MB).</p>
<p>verbose : bool, default=False
    Enable verbose output. Note that this setting takes advantage of a
    per-process runtime setting in libsvm that, if enabled, may not work
    properly in a multithreaded context.</p>
<p>max_iter : int, default=-1
    Hard limit on iterations within solver, or -1 for no limit.</p>
<h5 id="giants.config.ModelEstimatorConfig.SVR--attributes">Attributes<a class="headerlink" href="#giants.config.ModelEstimatorConfig.SVR--attributes" title="Permanent link">&para;</a></h5>
<p>class_weight_ : ndarray of shape (n_classes,)
    Multipliers of parameter C for each class.
    Computed based on the <code>class_weight</code> parameter.</p>
<p>coef_ : ndarray of shape (1, n_features)
    Weights assigned to the features (coefficients in the primal
    problem). This is only available in the case of a linear kernel.</p>
<div class="codehilite"><pre><span></span><code>`coef_` is readonly property derived from `dual_coef_` and
`support_vectors_`.
</code></pre></div>

<p>dual_coef_ : ndarray of shape (1, n_SV)
    Coefficients of the support vector in the decision function.</p>
<p>fit_status_ : int
    0 if correctly fitted, 1 otherwise (will raise warning)</p>
<p>intercept_ : ndarray of shape (1,)
    Constants in decision function.</p>
<p>n_features_in_ : int
    Number of features seen during :term:<code>fit</code>.</p>
<div class="codehilite"><pre><span></span><code>.. versionadded:: 0.24
</code></pre></div>

<p>feature_names_in_ : ndarray of shape (<code>n_features_in_</code>,)
    Names of features seen during :term:<code>fit</code>. Defined only when <code>X</code>
    has feature names that are all strings.</p>
<div class="codehilite"><pre><span></span><code>.. versionadded:: 1.0
</code></pre></div>

<p>n_support_ : ndarray of shape (n_classes,), dtype=int32
    Number of support vectors for each class.</p>
<p>shape_fit_ : tuple of int of shape (n_dimensions_of_X,)
    Array dimensions of training vector <code>X</code>.</p>
<p>support_ : ndarray of shape (n_SV,)
    Indices of support vectors.</p>
<p>support_vectors_ : ndarray of shape (n_SV, n_features)
    Support vectors.</p>
<h5 id="giants.config.ModelEstimatorConfig.SVR--see-also">See Also<a class="headerlink" href="#giants.config.ModelEstimatorConfig.SVR--see-also" title="Permanent link">&para;</a></h5>
<p>NuSVR : Support Vector Machine for regression implemented using libsvm
    using a parameter to control the number of support vectors.</p>
<p>LinearSVR : Scalable Linear Support Vector Machine for regression
    implemented using liblinear.</p>
<h5 id="giants.config.ModelEstimatorConfig.SVR--references">References<a class="headerlink" href="#giants.config.ModelEstimatorConfig.SVR--references" title="Permanent link">&para;</a></h5>
<p>.. [1] <code>LIBSVM: A Library for Support Vector Machines
    &lt;http://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf&gt;</code>_</p>
<p>.. [2] <code>Platt, John (1999). "Probabilistic outputs for support vector
    machines and comparison to regularizedlikelihood methods."
    &lt;http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.41.1639&gt;</code>_</p>
<h5 id="giants.config.ModelEstimatorConfig.SVR--examples">Examples<a class="headerlink" href="#giants.config.ModelEstimatorConfig.SVR--examples" title="Permanent link">&para;</a></h5>
<blockquote>
<blockquote>
<blockquote>
<p>from sklearn.svm import SVR
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
import numpy as np
n_samples, n_features = 10, 5
rng = np.random.RandomState(0)
y = rng.randn(n_samples)
X = rng.randn(n_samples, n_features)
regr = make_pipeline(StandardScaler(), SVR(C=1.0, epsilon=0.2))
regr.fit(X, y)
Pipeline(steps=[('standardscaler', StandardScaler()),
                ('svr', SVR(epsilon=0.2))])</p>
</blockquote>
</blockquote>
</blockquote>




  <div class="doc doc-children">












  </div>

    </div>

  </div>







  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 class="doc doc-heading" id="giants.config.ParamGridConfig">
        <code>
ParamGridConfig        </code>



<a class="headerlink" href="#giants.config.ParamGridConfig" title="Permanent link">&para;</a></h2>

    <div class="doc doc-contents ">

      <p>Stores the default grid search parameters to explore for each model.</p>




  <div class="doc doc-children">
























  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 class="doc doc-heading" id="giants.config.TypeConfig">
        <code>
TypeConfig        </code>



<a class="headerlink" href="#giants.config.TypeConfig" title="Permanent link">&para;</a></h2>

    <div class="doc doc-contents ">

      <p>Stores a series of python type hints for model-specific keywords.</p>




  <div class="doc doc-children">











  <div class="doc doc-object doc-class">



<h3 class="doc doc-heading" id="giants.config.TypeConfig.Array">
        <code>
Array        </code>



<a class="headerlink" href="#giants.config.TypeConfig.Array" title="Permanent link">&para;</a></h3>

    <div class="doc doc-contents ">

      <p>ndarray(shape, dtype=float, buffer=None, offset=0,
        strides=None, order=None)</p>
<p>An array object represents a multidimensional, homogeneous array
of fixed-size items.  An associated data-type object describes the
format of each element in the array (its byte-order, how many bytes it
occupies in memory, whether it is an integer, a floating point number,
or something else, etc.)</p>
<p>Arrays should be constructed using <code>array</code>, <code>zeros</code> or <code>empty</code> (refer
to the See Also section below).  The parameters given here refer to
a low-level method (<code>ndarray(...)</code>) for instantiating an array.</p>
<p>For more information, refer to the <code>numpy</code> module and examine the
methods and attributes of an array.</p>
<h5 id="giants.config.TypeConfig.Array--parameters">Parameters<a class="headerlink" href="#giants.config.TypeConfig.Array--parameters" title="Permanent link">&para;</a></h5>
<p>(for the <strong>new</strong> method; see Notes below)</p>
<p>shape : tuple of ints
    Shape of created array.
dtype : data-type, optional
    Any object that can be interpreted as a numpy data type.
buffer : object exposing buffer interface, optional
    Used to fill the array with data.
offset : int, optional
    Offset of array data in buffer.
strides : tuple of ints, optional
    Strides of data in memory.
order : {'C', 'F'}, optional
    Row-major (C-style) or column-major (Fortran-style) order.</p>
<h5 id="giants.config.TypeConfig.Array--attributes">Attributes<a class="headerlink" href="#giants.config.TypeConfig.Array--attributes" title="Permanent link">&para;</a></h5>
<p>T : ndarray
    Transpose of the array.
data : buffer
    The array's elements, in memory.
dtype : dtype object
    Describes the format of the elements in the array.
flags : dict
    Dictionary containing information related to memory use, e.g.,
    'C_CONTIGUOUS', 'OWNDATA', 'WRITEABLE', etc.
flat : numpy.flatiter object
    Flattened version of the array as an iterator.  The iterator
    allows assignments, e.g., <code>x.flat = 3</code> (See <code>ndarray.flat</code> for
    assignment examples; TODO).
imag : ndarray
    Imaginary part of the array.
real : ndarray
    Real part of the array.
size : int
    Number of elements in the array.
itemsize : int
    The memory use of each array element in bytes.
nbytes : int
    The total number of bytes required to store the array data,
    i.e., <code>itemsize * size</code>.
ndim : int
    The array's number of dimensions.
shape : tuple of ints
    Shape of the array.
strides : tuple of ints
    The step-size required to move from one element to the next in
    memory. For example, a contiguous <code>(3, 4)</code> array of type
    <code>int16</code> in C-order has strides <code>(8, 2)</code>.  This implies that
    to move from element to element in memory requires jumps of 2 bytes.
    To move from row-to-row, one needs to jump 8 bytes at a time
    (<code>2 * 4</code>).
ctypes : ctypes object
    Class containing properties of the array needed for interaction
    with ctypes.
base : ndarray
    If the array is a view into another array, that array is its <code>base</code>
    (unless that array is also a view).  The <code>base</code> array is where the
    array data is actually stored.</p>
<h5 id="giants.config.TypeConfig.Array--see-also">See Also<a class="headerlink" href="#giants.config.TypeConfig.Array--see-also" title="Permanent link">&para;</a></h5>
<p>array : Construct an array.
zeros : Create an array, each element of which is zero.
empty : Create an array, but leave its allocated memory unchanged (i.e.,
        it contains "garbage").
dtype : Create a data-type.
numpy.typing.NDArray : A :term:<code>generic &lt;generic type&gt;</code> version
                       of ndarray.</p>
<h5 id="giants.config.TypeConfig.Array--notes">Notes<a class="headerlink" href="#giants.config.TypeConfig.Array--notes" title="Permanent link">&para;</a></h5>
<p>There are two modes of creating an array using <code>__new__</code>:</p>
<ol>
<li>If <code>buffer</code> is None, then only <code>shape</code>, <code>dtype</code>, and <code>order</code>
   are used.</li>
<li>If <code>buffer</code> is an object exposing the buffer interface, then
   all keywords are interpreted.</li>
</ol>
<p>No <code>__init__</code> method is needed because the array is fully initialized
after the <code>__new__</code> method.</p>
<h5 id="giants.config.TypeConfig.Array--examples">Examples<a class="headerlink" href="#giants.config.TypeConfig.Array--examples" title="Permanent link">&para;</a></h5>
<p>These examples illustrate the low-level <code>ndarray</code> constructor.  Refer
to the <code>See Also</code> section above for easier ways of constructing an
ndarray.</p>
<p>First mode, <code>buffer</code> is None:</p>
<blockquote>
<blockquote>
<blockquote>
<p>np.ndarray(shape=(2,2), dtype=float, order='F')
array([[0.0e+000, 0.0e+000], # random
       [     nan, 2.5e-323]])</p>
</blockquote>
</blockquote>
</blockquote>
<p>Second mode:</p>
<blockquote>
<blockquote>
<blockquote>
<p>np.ndarray((2,), buffer=np.array([1,2,3]),
...            offset=np.int_().itemsize,
...            dtype=int) # offset = 1*itemsize, i.e. skip first element
array([2, 3])</p>
</blockquote>
</blockquote>
</blockquote>



    </div>

  </div>



  <div class="doc doc-object doc-class">



<h3 class="doc doc-heading" id="giants.config.TypeConfig.BaseSearchCV">
        <code>
BaseSearchCV            (<span title="sklearn.base.MetaEstimatorMixin">MetaEstimatorMixin</span>, <span title="sklearn.base.BaseEstimator">BaseEstimator</span>)
        </code>



<a class="headerlink" href="#giants.config.TypeConfig.BaseSearchCV" title="Permanent link">&para;</a></h3>

    <div class="doc doc-contents ">

      <p>Abstract base class for hyper parameter search with cross-validation.</p>




  <div class="doc doc-children">






  <div class="doc doc-object doc-attribute">



<h4 class="doc doc-heading" id="giants.config.TypeConfig.BaseSearchCV.classes_">
<code class="codehilite language-python"><span class="n">classes_</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-readonly"><code>readonly</code></small>
  </span>

<a class="headerlink" href="#giants.config.TypeConfig.BaseSearchCV.classes_" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Class labels.</p>
<p>Only available when <code>refit=True</code> and the estimator is a classifier.</p>
    </div>

  </div>



  <div class="doc doc-object doc-attribute">



<h4 class="doc doc-heading" id="giants.config.TypeConfig.BaseSearchCV.n_features_in_">
<code class="codehilite language-python"><span class="n">n_features_in_</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-readonly"><code>readonly</code></small>
  </span>

<a class="headerlink" href="#giants.config.TypeConfig.BaseSearchCV.n_features_in_" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Number of features seen during :term:<code>fit</code>.</p>
<p>Only available when <code>refit=True</code>.</p>
    </div>

  </div>







  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="giants.config.TypeConfig.BaseSearchCV.decision_function">
<code class="codehilite language-python"><span class="n">decision_function</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span></code>


<a class="headerlink" href="#giants.config.TypeConfig.BaseSearchCV.decision_function" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Call decision_function on the estimator with the best found parameters.</p>
<p>Only available if <code>refit=True</code> and the underlying estimator supports
<code>decision_function</code>.</p>
<h6 id="giants.config.TypeConfig.BaseSearchCV.decision_function--parameters">Parameters<a class="headerlink" href="#giants.config.TypeConfig.BaseSearchCV.decision_function--parameters" title="Permanent link">&para;</a></h6>
<p>X : indexable, length n_samples
    Must fulfill the input assumptions of the
    underlying estimator.</p>
<h6 id="giants.config.TypeConfig.BaseSearchCV.decision_function--returns">Returns<a class="headerlink" href="#giants.config.TypeConfig.BaseSearchCV.decision_function--returns" title="Permanent link">&para;</a></h6>
<p>y_score : ndarray of shape (n_samples,) or (n_samples, n_classes)                 or (n_samples, n_classes * (n_classes-1) / 2)
    Result of the decision function for <code>X</code> based on the estimator with
    the best found parameters.</p>

        <details class="quote">
          <summary>Source code in <code>giants/config.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="nd">@available_if</span><span class="p">(</span><span class="n">_estimator_has</span><span class="p">(</span><span class="s2">&quot;decision_function&quot;</span><span class="p">))</span>
<span class="k">def</span> <span class="nf">decision_function</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Call decision_function on the estimator with the best found parameters.</span>

<span class="sd">    Only available if ``refit=True`` and the underlying estimator supports</span>
<span class="sd">    ``decision_function``.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : indexable, length n_samples</span>
<span class="sd">        Must fulfill the input assumptions of the</span>
<span class="sd">        underlying estimator.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    y_score : ndarray of shape (n_samples,) or (n_samples, n_classes) \</span>
<span class="sd">            or (n_samples, n_classes * (n_classes-1) / 2)</span>
<span class="sd">        Result of the decision function for `X` based on the estimator with</span>
<span class="sd">        the best found parameters.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">best_estimator_</span><span class="o">.</span><span class="n">decision_function</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="giants.config.TypeConfig.BaseSearchCV.fit">
<code class="codehilite language-python"><span class="n">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">fit_params</span><span class="p">)</span></code>


<a class="headerlink" href="#giants.config.TypeConfig.BaseSearchCV.fit" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Run fit with all sets of parameters.</p>
<h6 id="giants.config.TypeConfig.BaseSearchCV.fit--parameters">Parameters<a class="headerlink" href="#giants.config.TypeConfig.BaseSearchCV.fit--parameters" title="Permanent link">&para;</a></h6>
<p>X : array-like of shape (n_samples, n_features)
    Training vector, where <code>n_samples</code> is the number of samples and
    <code>n_features</code> is the number of features.</p>
<p>y : array-like of shape (n_samples, n_output)             or (n_samples,), default=None
    Target relative to X for classification or regression;
    None for unsupervised learning.</p>
<p>groups : array-like of shape (n_samples,), default=None
    Group labels for the samples used while splitting the dataset into
    train/test set. Only used in conjunction with a "Group" :term:<code>cv</code>
    instance (e.g., :class:<code>~sklearn.model_selection.GroupKFold</code>).</p>
<p>**fit_params : dict of str -&gt; object
    Parameters passed to the <code>fit</code> method of the estimator.</p>
<h6 id="giants.config.TypeConfig.BaseSearchCV.fit--returns">Returns<a class="headerlink" href="#giants.config.TypeConfig.BaseSearchCV.fit--returns" title="Permanent link">&para;</a></h6>
<p>self : object
    Instance of fitted estimator.</p>

        <details class="quote">
          <summary>Source code in <code>giants/config.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">fit_params</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Run fit with all sets of parameters.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>

<span class="sd">    X : array-like of shape (n_samples, n_features)</span>
<span class="sd">        Training vector, where `n_samples` is the number of samples and</span>
<span class="sd">        `n_features` is the number of features.</span>

<span class="sd">    y : array-like of shape (n_samples, n_output) \</span>
<span class="sd">        or (n_samples,), default=None</span>
<span class="sd">        Target relative to X for classification or regression;</span>
<span class="sd">        None for unsupervised learning.</span>

<span class="sd">    groups : array-like of shape (n_samples,), default=None</span>
<span class="sd">        Group labels for the samples used while splitting the dataset into</span>
<span class="sd">        train/test set. Only used in conjunction with a &quot;Group&quot; :term:`cv`</span>
<span class="sd">        instance (e.g., :class:`~sklearn.model_selection.GroupKFold`).</span>

<span class="sd">    **fit_params : dict of str -&gt; object</span>
<span class="sd">        Parameters passed to the ``fit`` method of the estimator.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    self : object</span>
<span class="sd">        Instance of fitted estimator.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">estimator</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimator</span>
    <span class="n">refit_metric</span> <span class="o">=</span> <span class="s2">&quot;score&quot;</span>

    <span class="k">if</span> <span class="n">callable</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">scoring</span><span class="p">):</span>
        <span class="n">scorers</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">scoring</span>
    <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">scoring</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">scoring</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
        <span class="n">scorers</span> <span class="o">=</span> <span class="n">check_scoring</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimator</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">scoring</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">scorers</span> <span class="o">=</span> <span class="n">_check_multimetric_scoring</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimator</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">scoring</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_refit_for_multimetric</span><span class="p">(</span><span class="n">scorers</span><span class="p">)</span>
        <span class="n">refit_metric</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">refit</span>

    <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">groups</span> <span class="o">=</span> <span class="n">indexable</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">groups</span><span class="p">)</span>
    <span class="n">fit_params</span> <span class="o">=</span> <span class="n">_check_fit_params</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">fit_params</span><span class="p">)</span>

    <span class="n">cv_orig</span> <span class="o">=</span> <span class="n">check_cv</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cv</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">classifier</span><span class="o">=</span><span class="n">is_classifier</span><span class="p">(</span><span class="n">estimator</span><span class="p">))</span>
    <span class="n">n_splits</span> <span class="o">=</span> <span class="n">cv_orig</span><span class="o">.</span><span class="n">get_n_splits</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">groups</span><span class="p">)</span>

    <span class="n">base_estimator</span> <span class="o">=</span> <span class="n">clone</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimator</span><span class="p">)</span>

    <span class="n">parallel</span> <span class="o">=</span> <span class="n">Parallel</span><span class="p">(</span><span class="n">n_jobs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_jobs</span><span class="p">,</span> <span class="n">pre_dispatch</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">pre_dispatch</span><span class="p">)</span>

    <span class="n">fit_and_score_kwargs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
        <span class="n">scorer</span><span class="o">=</span><span class="n">scorers</span><span class="p">,</span>
        <span class="n">fit_params</span><span class="o">=</span><span class="n">fit_params</span><span class="p">,</span>
        <span class="n">return_train_score</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">return_train_score</span><span class="p">,</span>
        <span class="n">return_n_test_samples</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">return_times</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">return_parameters</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">error_score</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">error_score</span><span class="p">,</span>
        <span class="n">verbose</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">results</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">with</span> <span class="n">parallel</span><span class="p">:</span>
        <span class="n">all_candidate_params</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">all_out</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">all_more_results</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">list</span><span class="p">)</span>

        <span class="k">def</span> <span class="nf">evaluate_candidates</span><span class="p">(</span><span class="n">candidate_params</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">more_results</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
            <span class="n">cv</span> <span class="o">=</span> <span class="n">cv</span> <span class="ow">or</span> <span class="n">cv_orig</span>
            <span class="n">candidate_params</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">candidate_params</span><span class="p">)</span>
            <span class="n">n_candidates</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">candidate_params</span><span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span>
                    <span class="s2">&quot;Fitting </span><span class="si">{0}</span><span class="s2"> folds for each of </span><span class="si">{1}</span><span class="s2"> candidates,&quot;</span>
                    <span class="s2">&quot; totalling </span><span class="si">{2}</span><span class="s2"> fits&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                        <span class="n">n_splits</span><span class="p">,</span> <span class="n">n_candidates</span><span class="p">,</span> <span class="n">n_candidates</span> <span class="o">*</span> <span class="n">n_splits</span>
                    <span class="p">)</span>
                <span class="p">)</span>

            <span class="n">out</span> <span class="o">=</span> <span class="n">parallel</span><span class="p">(</span>
                <span class="n">delayed</span><span class="p">(</span><span class="n">_fit_and_score</span><span class="p">)(</span>
                    <span class="n">clone</span><span class="p">(</span><span class="n">base_estimator</span><span class="p">),</span>
                    <span class="n">X</span><span class="p">,</span>
                    <span class="n">y</span><span class="p">,</span>
                    <span class="n">train</span><span class="o">=</span><span class="n">train</span><span class="p">,</span>
                    <span class="n">test</span><span class="o">=</span><span class="n">test</span><span class="p">,</span>
                    <span class="n">parameters</span><span class="o">=</span><span class="n">parameters</span><span class="p">,</span>
                    <span class="n">split_progress</span><span class="o">=</span><span class="p">(</span><span class="n">split_idx</span><span class="p">,</span> <span class="n">n_splits</span><span class="p">),</span>
                    <span class="n">candidate_progress</span><span class="o">=</span><span class="p">(</span><span class="n">cand_idx</span><span class="p">,</span> <span class="n">n_candidates</span><span class="p">),</span>
                    <span class="o">**</span><span class="n">fit_and_score_kwargs</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="k">for</span> <span class="p">(</span><span class="n">cand_idx</span><span class="p">,</span> <span class="n">parameters</span><span class="p">),</span> <span class="p">(</span><span class="n">split_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">train</span><span class="p">,</span> <span class="n">test</span><span class="p">))</span> <span class="ow">in</span> <span class="n">product</span><span class="p">(</span>
                    <span class="nb">enumerate</span><span class="p">(</span><span class="n">candidate_params</span><span class="p">),</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">cv</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">groups</span><span class="p">))</span>
                <span class="p">)</span>
            <span class="p">)</span>

            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">out</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;No fits were performed. &quot;</span>
                    <span class="s2">&quot;Was the CV iterator empty? &quot;</span>
                    <span class="s2">&quot;Were there no candidates?&quot;</span>
                <span class="p">)</span>
            <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">out</span><span class="p">)</span> <span class="o">!=</span> <span class="n">n_candidates</span> <span class="o">*</span> <span class="n">n_splits</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;cv.split and cv.get_n_splits returned &quot;</span>
                    <span class="s2">&quot;inconsistent results. Expected </span><span class="si">{}</span><span class="s2"> &quot;</span>
                    <span class="s2">&quot;splits, got </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">n_splits</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">out</span><span class="p">)</span> <span class="o">//</span> <span class="n">n_candidates</span><span class="p">)</span>
                <span class="p">)</span>

            <span class="n">_warn_about_fit_failures</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">error_score</span><span class="p">)</span>

            <span class="c1"># For callable self.scoring, the return type is only know after</span>
            <span class="c1"># calling. If the return type is a dictionary, the error scores</span>
            <span class="c1"># can now be inserted with the correct key. The type checking</span>
            <span class="c1"># of out will be done in `_insert_error_scores`.</span>
            <span class="k">if</span> <span class="n">callable</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">scoring</span><span class="p">):</span>
                <span class="n">_insert_error_scores</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">error_score</span><span class="p">)</span>

            <span class="n">all_candidate_params</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">candidate_params</span><span class="p">)</span>
            <span class="n">all_out</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">more_results</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">more_results</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                    <span class="n">all_more_results</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>

            <span class="k">nonlocal</span> <span class="n">results</span>
            <span class="n">results</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_format_results</span><span class="p">(</span>
                <span class="n">all_candidate_params</span><span class="p">,</span> <span class="n">n_splits</span><span class="p">,</span> <span class="n">all_out</span><span class="p">,</span> <span class="n">all_more_results</span>
            <span class="p">)</span>

            <span class="k">return</span> <span class="n">results</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_run_search</span><span class="p">(</span><span class="n">evaluate_candidates</span><span class="p">)</span>

        <span class="c1"># multimetric is determined here because in the case of a callable</span>
        <span class="c1"># self.scoring the return type is only known after calling</span>
        <span class="n">first_test_score</span> <span class="o">=</span> <span class="n">all_out</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;test_scores&quot;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">multimetric_</span> <span class="o">=</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">first_test_score</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span>

        <span class="c1"># check refit_metric now for a callabe scorer that is multimetric</span>
        <span class="k">if</span> <span class="n">callable</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">scoring</span><span class="p">)</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">multimetric_</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_check_refit_for_multimetric</span><span class="p">(</span><span class="n">first_test_score</span><span class="p">)</span>
            <span class="n">refit_metric</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">refit</span>

    <span class="c1"># For multi-metric evaluation, store the best_index_, best_params_ and</span>
    <span class="c1"># best_score_ iff refit is one of the scorer names</span>
    <span class="c1"># In single metric evaluation, refit_metric is &quot;score&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">refit</span> <span class="ow">or</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">multimetric_</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">best_index_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_select_best_index</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">refit</span><span class="p">,</span> <span class="n">refit_metric</span><span class="p">,</span> <span class="n">results</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">callable</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">refit</span><span class="p">):</span>
            <span class="c1"># With a non-custom callable, we can select the best score</span>
            <span class="c1"># based on the best index</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">best_score_</span> <span class="o">=</span> <span class="n">results</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;mean_test_</span><span class="si">{</span><span class="n">refit_metric</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">][</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">best_index_</span>
            <span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">best_params_</span> <span class="o">=</span> <span class="n">results</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">][</span><span class="bp">self</span><span class="o">.</span><span class="n">best_index_</span><span class="p">]</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">refit</span><span class="p">:</span>
        <span class="c1"># we clone again after setting params in case some</span>
        <span class="c1"># of the params are estimators as well.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">best_estimator_</span> <span class="o">=</span> <span class="n">clone</span><span class="p">(</span>
            <span class="n">clone</span><span class="p">(</span><span class="n">base_estimator</span><span class="p">)</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">best_params_</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">refit_start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">y</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">best_estimator_</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="o">**</span><span class="n">fit_params</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">best_estimator_</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="o">**</span><span class="n">fit_params</span><span class="p">)</span>
        <span class="n">refit_end_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">refit_time_</span> <span class="o">=</span> <span class="n">refit_end_time</span> <span class="o">-</span> <span class="n">refit_start_time</span>

        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">best_estimator_</span><span class="p">,</span> <span class="s2">&quot;feature_names_in_&quot;</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">feature_names_in_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">best_estimator_</span><span class="o">.</span><span class="n">feature_names_in_</span>

    <span class="c1"># Store the only scorer not as a dict for single metric evaluation</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">scorer_</span> <span class="o">=</span> <span class="n">scorers</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">cv_results_</span> <span class="o">=</span> <span class="n">results</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_splits_</span> <span class="o">=</span> <span class="n">n_splits</span>

    <span class="k">return</span> <span class="bp">self</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="giants.config.TypeConfig.BaseSearchCV.inverse_transform">
<code class="codehilite language-python"><span class="n">inverse_transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Xt</span><span class="p">)</span></code>


<a class="headerlink" href="#giants.config.TypeConfig.BaseSearchCV.inverse_transform" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Call inverse_transform on the estimator with the best found params.</p>
<p>Only available if the underlying estimator implements
<code>inverse_transform</code> and <code>refit=True</code>.</p>
<h6 id="giants.config.TypeConfig.BaseSearchCV.inverse_transform--parameters">Parameters<a class="headerlink" href="#giants.config.TypeConfig.BaseSearchCV.inverse_transform--parameters" title="Permanent link">&para;</a></h6>
<p>Xt : indexable, length n_samples
    Must fulfill the input assumptions of the
    underlying estimator.</p>
<h6 id="giants.config.TypeConfig.BaseSearchCV.inverse_transform--returns">Returns<a class="headerlink" href="#giants.config.TypeConfig.BaseSearchCV.inverse_transform--returns" title="Permanent link">&para;</a></h6>
<p>X : {ndarray, sparse matrix} of shape (n_samples, n_features)
    Result of the <code>inverse_transform</code> function for <code>Xt</code> based on the
    estimator with the best found parameters.</p>

        <details class="quote">
          <summary>Source code in <code>giants/config.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="nd">@available_if</span><span class="p">(</span><span class="n">_estimator_has</span><span class="p">(</span><span class="s2">&quot;inverse_transform&quot;</span><span class="p">))</span>
<span class="k">def</span> <span class="nf">inverse_transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Xt</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Call inverse_transform on the estimator with the best found params.</span>

<span class="sd">    Only available if the underlying estimator implements</span>
<span class="sd">    ``inverse_transform`` and ``refit=True``.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    Xt : indexable, length n_samples</span>
<span class="sd">        Must fulfill the input assumptions of the</span>
<span class="sd">        underlying estimator.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    X : {ndarray, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">        Result of the `inverse_transform` function for `Xt` based on the</span>
<span class="sd">        estimator with the best found parameters.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">best_estimator_</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">Xt</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="giants.config.TypeConfig.BaseSearchCV.predict">
<code class="codehilite language-python"><span class="n">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span></code>


<a class="headerlink" href="#giants.config.TypeConfig.BaseSearchCV.predict" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Call predict on the estimator with the best found parameters.</p>
<p>Only available if <code>refit=True</code> and the underlying estimator supports
<code>predict</code>.</p>
<h6 id="giants.config.TypeConfig.BaseSearchCV.predict--parameters">Parameters<a class="headerlink" href="#giants.config.TypeConfig.BaseSearchCV.predict--parameters" title="Permanent link">&para;</a></h6>
<p>X : indexable, length n_samples
    Must fulfill the input assumptions of the
    underlying estimator.</p>
<h6 id="giants.config.TypeConfig.BaseSearchCV.predict--returns">Returns<a class="headerlink" href="#giants.config.TypeConfig.BaseSearchCV.predict--returns" title="Permanent link">&para;</a></h6>
<p>y_pred : ndarray of shape (n_samples,)
    The predicted labels or values for <code>X</code> based on the estimator with
    the best found parameters.</p>

        <details class="quote">
          <summary>Source code in <code>giants/config.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="nd">@available_if</span><span class="p">(</span><span class="n">_estimator_has</span><span class="p">(</span><span class="s2">&quot;predict&quot;</span><span class="p">))</span>
<span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Call predict on the estimator with the best found parameters.</span>

<span class="sd">    Only available if ``refit=True`` and the underlying estimator supports</span>
<span class="sd">    ``predict``.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : indexable, length n_samples</span>
<span class="sd">        Must fulfill the input assumptions of the</span>
<span class="sd">        underlying estimator.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    y_pred : ndarray of shape (n_samples,)</span>
<span class="sd">        The predicted labels or values for `X` based on the estimator with</span>
<span class="sd">        the best found parameters.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">best_estimator_</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="giants.config.TypeConfig.BaseSearchCV.predict_log_proba">
<code class="codehilite language-python"><span class="n">predict_log_proba</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span></code>


<a class="headerlink" href="#giants.config.TypeConfig.BaseSearchCV.predict_log_proba" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Call predict_log_proba on the estimator with the best found parameters.</p>
<p>Only available if <code>refit=True</code> and the underlying estimator supports
<code>predict_log_proba</code>.</p>
<h6 id="giants.config.TypeConfig.BaseSearchCV.predict_log_proba--parameters">Parameters<a class="headerlink" href="#giants.config.TypeConfig.BaseSearchCV.predict_log_proba--parameters" title="Permanent link">&para;</a></h6>
<p>X : indexable, length n_samples
    Must fulfill the input assumptions of the
    underlying estimator.</p>
<h6 id="giants.config.TypeConfig.BaseSearchCV.predict_log_proba--returns">Returns<a class="headerlink" href="#giants.config.TypeConfig.BaseSearchCV.predict_log_proba--returns" title="Permanent link">&para;</a></h6>
<p>y_pred : ndarray of shape (n_samples,) or (n_samples, n_classes)
    Predicted class log-probabilities for <code>X</code> based on the estimator
    with the best found parameters. The order of the classes
    corresponds to that in the fitted attribute :term:<code>classes_</code>.</p>

        <details class="quote">
          <summary>Source code in <code>giants/config.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="nd">@available_if</span><span class="p">(</span><span class="n">_estimator_has</span><span class="p">(</span><span class="s2">&quot;predict_log_proba&quot;</span><span class="p">))</span>
<span class="k">def</span> <span class="nf">predict_log_proba</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Call predict_log_proba on the estimator with the best found parameters.</span>

<span class="sd">    Only available if ``refit=True`` and the underlying estimator supports</span>
<span class="sd">    ``predict_log_proba``.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : indexable, length n_samples</span>
<span class="sd">        Must fulfill the input assumptions of the</span>
<span class="sd">        underlying estimator.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    y_pred : ndarray of shape (n_samples,) or (n_samples, n_classes)</span>
<span class="sd">        Predicted class log-probabilities for `X` based on the estimator</span>
<span class="sd">        with the best found parameters. The order of the classes</span>
<span class="sd">        corresponds to that in the fitted attribute :term:`classes_`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">best_estimator_</span><span class="o">.</span><span class="n">predict_log_proba</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="giants.config.TypeConfig.BaseSearchCV.predict_proba">
<code class="codehilite language-python"><span class="n">predict_proba</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span></code>


<a class="headerlink" href="#giants.config.TypeConfig.BaseSearchCV.predict_proba" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Call predict_proba on the estimator with the best found parameters.</p>
<p>Only available if <code>refit=True</code> and the underlying estimator supports
<code>predict_proba</code>.</p>
<h6 id="giants.config.TypeConfig.BaseSearchCV.predict_proba--parameters">Parameters<a class="headerlink" href="#giants.config.TypeConfig.BaseSearchCV.predict_proba--parameters" title="Permanent link">&para;</a></h6>
<p>X : indexable, length n_samples
    Must fulfill the input assumptions of the
    underlying estimator.</p>
<h6 id="giants.config.TypeConfig.BaseSearchCV.predict_proba--returns">Returns<a class="headerlink" href="#giants.config.TypeConfig.BaseSearchCV.predict_proba--returns" title="Permanent link">&para;</a></h6>
<p>y_pred : ndarray of shape (n_samples,) or (n_samples, n_classes)
    Predicted class probabilities for <code>X</code> based on the estimator with
    the best found parameters. The order of the classes corresponds
    to that in the fitted attribute :term:<code>classes_</code>.</p>

        <details class="quote">
          <summary>Source code in <code>giants/config.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="nd">@available_if</span><span class="p">(</span><span class="n">_estimator_has</span><span class="p">(</span><span class="s2">&quot;predict_proba&quot;</span><span class="p">))</span>
<span class="k">def</span> <span class="nf">predict_proba</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Call predict_proba on the estimator with the best found parameters.</span>

<span class="sd">    Only available if ``refit=True`` and the underlying estimator supports</span>
<span class="sd">    ``predict_proba``.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : indexable, length n_samples</span>
<span class="sd">        Must fulfill the input assumptions of the</span>
<span class="sd">        underlying estimator.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    y_pred : ndarray of shape (n_samples,) or (n_samples, n_classes)</span>
<span class="sd">        Predicted class probabilities for `X` based on the estimator with</span>
<span class="sd">        the best found parameters. The order of the classes corresponds</span>
<span class="sd">        to that in the fitted attribute :term:`classes_`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">best_estimator_</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="giants.config.TypeConfig.BaseSearchCV.score">
<code class="codehilite language-python"><span class="n">score</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>


<a class="headerlink" href="#giants.config.TypeConfig.BaseSearchCV.score" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Return the score on the given data, if the estimator has been refit.</p>
<p>This uses the score defined by <code>scoring</code> where provided, and the
<code>best_estimator_.score</code> method otherwise.</p>
<h6 id="giants.config.TypeConfig.BaseSearchCV.score--parameters">Parameters<a class="headerlink" href="#giants.config.TypeConfig.BaseSearchCV.score--parameters" title="Permanent link">&para;</a></h6>
<p>X : array-like of shape (n_samples, n_features)
    Input data, where <code>n_samples</code> is the number of samples and
    <code>n_features</code> is the number of features.</p>
<p>y : array-like of shape (n_samples, n_output)             or (n_samples,), default=None
    Target relative to X for classification or regression;
    None for unsupervised learning.</p>
<h6 id="giants.config.TypeConfig.BaseSearchCV.score--returns">Returns<a class="headerlink" href="#giants.config.TypeConfig.BaseSearchCV.score--returns" title="Permanent link">&para;</a></h6>
<p>score : float
    The score defined by <code>scoring</code> if provided, and the
    <code>best_estimator_.score</code> method otherwise.</p>

        <details class="quote">
          <summary>Source code in <code>giants/config.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">score</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Return the score on the given data, if the estimator has been refit.</span>

<span class="sd">    This uses the score defined by ``scoring`` where provided, and the</span>
<span class="sd">    ``best_estimator_.score`` method otherwise.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : array-like of shape (n_samples, n_features)</span>
<span class="sd">        Input data, where `n_samples` is the number of samples and</span>
<span class="sd">        `n_features` is the number of features.</span>

<span class="sd">    y : array-like of shape (n_samples, n_output) \</span>
<span class="sd">        or (n_samples,), default=None</span>
<span class="sd">        Target relative to X for classification or regression;</span>
<span class="sd">        None for unsupervised learning.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    score : float</span>
<span class="sd">        The score defined by ``scoring`` if provided, and the</span>
<span class="sd">        ``best_estimator_.score`` method otherwise.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_refit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;score&quot;</span><span class="p">)</span>
    <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">scorer_</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;No score function explicitly defined, &quot;</span>
            <span class="s2">&quot;and the estimator doesn&#39;t provide one </span><span class="si">%s</span><span class="s2">&quot;</span>
            <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">best_estimator_</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">scorer_</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">multimetric_</span><span class="p">:</span>
            <span class="n">scorer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">scorer_</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">refit</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">scorer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">scorer_</span>
        <span class="k">return</span> <span class="n">scorer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">best_estimator_</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="c1"># callable</span>
    <span class="n">score</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">scorer_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">best_estimator_</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">multimetric_</span><span class="p">:</span>
        <span class="n">score</span> <span class="o">=</span> <span class="n">score</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">refit</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">score</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="giants.config.TypeConfig.BaseSearchCV.score_samples">
<code class="codehilite language-python"><span class="n">score_samples</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span></code>


<a class="headerlink" href="#giants.config.TypeConfig.BaseSearchCV.score_samples" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Call score_samples on the estimator with the best found parameters.</p>
<p>Only available if <code>refit=True</code> and the underlying estimator supports
<code>score_samples</code>.</p>
<p>.. versionadded:: 0.24</p>
<h6 id="giants.config.TypeConfig.BaseSearchCV.score_samples--parameters">Parameters<a class="headerlink" href="#giants.config.TypeConfig.BaseSearchCV.score_samples--parameters" title="Permanent link">&para;</a></h6>
<p>X : iterable
    Data to predict on. Must fulfill input requirements
    of the underlying estimator.</p>
<h6 id="giants.config.TypeConfig.BaseSearchCV.score_samples--returns">Returns<a class="headerlink" href="#giants.config.TypeConfig.BaseSearchCV.score_samples--returns" title="Permanent link">&para;</a></h6>
<p>y_score : ndarray of shape (n_samples,)
    The <code>best_estimator_.score_samples</code> method.</p>

        <details class="quote">
          <summary>Source code in <code>giants/config.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="nd">@available_if</span><span class="p">(</span><span class="n">_estimator_has</span><span class="p">(</span><span class="s2">&quot;score_samples&quot;</span><span class="p">))</span>
<span class="k">def</span> <span class="nf">score_samples</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Call score_samples on the estimator with the best found parameters.</span>

<span class="sd">    Only available if ``refit=True`` and the underlying estimator supports</span>
<span class="sd">    ``score_samples``.</span>

<span class="sd">    .. versionadded:: 0.24</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : iterable</span>
<span class="sd">        Data to predict on. Must fulfill input requirements</span>
<span class="sd">        of the underlying estimator.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    y_score : ndarray of shape (n_samples,)</span>
<span class="sd">        The ``best_estimator_.score_samples`` method.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">best_estimator_</span><span class="o">.</span><span class="n">score_samples</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h4 class="doc doc-heading" id="giants.config.TypeConfig.BaseSearchCV.transform">
<code class="codehilite language-python"><span class="n">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span></code>


<a class="headerlink" href="#giants.config.TypeConfig.BaseSearchCV.transform" title="Permanent link">&para;</a></h4>

    <div class="doc doc-contents ">

      <p>Call transform on the estimator with the best found parameters.</p>
<p>Only available if the underlying estimator supports <code>transform</code> and
<code>refit=True</code>.</p>
<h6 id="giants.config.TypeConfig.BaseSearchCV.transform--parameters">Parameters<a class="headerlink" href="#giants.config.TypeConfig.BaseSearchCV.transform--parameters" title="Permanent link">&para;</a></h6>
<p>X : indexable, length n_samples
    Must fulfill the input assumptions of the
    underlying estimator.</p>
<h6 id="giants.config.TypeConfig.BaseSearchCV.transform--returns">Returns<a class="headerlink" href="#giants.config.TypeConfig.BaseSearchCV.transform--returns" title="Permanent link">&para;</a></h6>
<p>Xt : {ndarray, sparse matrix} of shape (n_samples, n_features)
    <code>X</code> transformed in the new space based on the estimator with
    the best found parameters.</p>

        <details class="quote">
          <summary>Source code in <code>giants/config.py</code></summary>
          <div class="codehilite"><pre><span></span><code><span class="nd">@available_if</span><span class="p">(</span><span class="n">_estimator_has</span><span class="p">(</span><span class="s2">&quot;transform&quot;</span><span class="p">))</span>
<span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Call transform on the estimator with the best found parameters.</span>

<span class="sd">    Only available if the underlying estimator supports ``transform`` and</span>
<span class="sd">    ``refit=True``.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : indexable, length n_samples</span>
<span class="sd">        Must fulfill the input assumptions of the</span>
<span class="sd">        underlying estimator.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    Xt : {ndarray, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">        `X` transformed in the new space based on the estimator with</span>
<span class="sd">        the best found parameters.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">best_estimator_</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h3 class="doc doc-heading" id="giants.config.TypeConfig.Number">
        <code>
Number        </code>



<a class="headerlink" href="#giants.config.TypeConfig.Number" title="Permanent link">&para;</a></h3>

    <div class="doc doc-contents ">

      <p>All numbers inherit from this class.</p>
<p>If you just want to check if an argument x is a number, without
caring what kind, use isinstance(x, Number).</p>




  <div class="doc doc-children">













  </div>

    </div>

  </div>







  </div>

    </div>

  </div>







  </div>

    </div>

  </div>

              
            </article>
          </div>
        </div>
        
          <a href="#" class="md-top md-icon" data-md-component="top" data-md-state="hidden">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"/></svg>
            Back to top
          </a>
        
      </main>
      
        <footer class="md-footer">
  
    <nav class="md-footer__inner md-grid" aria-label="Footer">
      
        
        <a href="../model/" class="md-footer__link md-footer__link--prev" aria-label="Previous: giants.model" rel="prev">
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
          </div>
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Previous
              </span>
              giants.model
            </div>
          </div>
        </a>
      
      
    </nav>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        <div class="md-social">
  
    
    
      
      
    
    <a href="https://github.com/forestobservatory" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
    </a>
  
    
    
      
      
    
    <a href="https://twitter.com/forestobs" target="_blank" rel="noopener" title="twitter.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg>
    </a>
  
    
    
      
      
    
    <a href="https://the.forestobservatory.com/" target="_blank" rel="noopener" title="the.forestobservatory.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17.9 17.39c-.26-.8-1.01-1.39-1.9-1.39h-1v-3a1 1 0 0 0-1-1H8v-2h2a1 1 0 0 0 1-1V7h2a2 2 0 0 0 2-2v-.41a7.984 7.984 0 0 1 2.9 12.8M11 19.93c-3.95-.49-7-3.85-7-7.93 0-.62.08-1.22.21-1.79L9 15v1a2 2 0 0 0 2 2m1-16A10 10 0 0 0 2 12a10 10 0 0 0 10 10 10 10 0 0 0 10-10A10 10 0 0 0 12 2z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.top"], "translations": {"clipboard.copy": "Copy to clipboard", "clipboard.copied": "Copied to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.placeholder": "Type to start searching", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.term.missing": "Missing", "select.version.title": "Select version"}, "search": "../../assets/javascripts/workers/search.0bbba5b5.min.js"}</script>
    
    
      <script src="../../assets/javascripts/bundle.e1a181d9.min.js"></script>
      
    
  </body>
</html>